{"posts":[{"title":"当代孔乙己-Attention机制的四种写法","text":"大语言模型（LLM）发展方兴未艾，从ChatGPT横空出世横扫英文互联网，到DeepSeek惊讶世人宣告低成本LLM可行性，2025年短短一年便造就了人工智能领域如此震撼的估值繁荣（or bubble），不得不让人感慨《三体》中 “技术爆炸” 一词提出的先见性。 一切从《Attention is All You Need》开始。这篇论文彻底抛弃了长序列预测的LSTM和RNN模型，在学术界有着划时代意义；更在传播学上展示了一个好的名字到底有多重要。 以下内容大量参考chaofa用代码打点酱油的博客 在AI相关的面试中，经常会要求对Transformer架构有足够的理解，通常通过手写self-attention机制来验证。这里给出了Attention机制从简到繁的四种写法 什么是$Self-Attention$？$Attention$ 机制通常在已实现 $Embedding$ + 位置编码后进行。引入 $Attention$ 机制主要是由于 $Embedding$ 和位置编码依然不够表示所有的信息。 举个例子 “你不要打人” 和 “你快去打水” 中 “打” 字含义截然不同，但仅靠 $Embedding$ 和位置编码两者并不能区别，这就需要结合上下文来增加更多信息，$Attention$ 就是这种结合的方式。 $$SelfAttention(X)=softmax(\\frac{Q\\cdot K}{\\sqrt{d}})\\cdot V$$ $Q = K = V = W_{Q/K/V} \\cdot X$，其中$Q，K，V$分别对应不同（形状也不同）的权重矩阵W matmul和@符号是一样的作用，矩阵乘法 为什么要除以$\\sqrt{d}$：一是防止梯度消失（参考Kaiming Initialization参数初始化）；二是为了让QK的内积分布保持和输入一样 爱因斯坦方程表达式用法：torch.einsum(“bqd,bkd-&gt;bqk”,X,X).shape 为什么使用softmax?注意力机制，本质上是想找与查询向量最相关的隐向量序列，实际上是想找，onehot(argmax)，比如的任务是中视野中找苹果，直接依据苹果的特征查找苹果。但是这个求解是无法计算梯度，所以才用onehot(argmax)的光滑版本softmax，以获得良好的梯度特性。是一种数学上的妥协 X.repeat(1,1,3)表示在不同维度进行repeat操作，也可以用tensor.expand操作 12345import mathimport warningsimport torchimport torch.nn as nnwarnings.filterwarnings(action='ignore') 第一种写法：简洁123456789101112131415161718192021222324class SelfAttention_1(nn.Module): # 父类是pytorch中的核心Module def __init__(self, hidden_dim): super(SelfAttention_1,self).__init__()# 调用父类的构造方法，并传递所有参数 self.hidden_dim = hidden_dim self.Q = nn.Linear(hidden_dim, hidden_dim) self.K = nn.Linear(hidden_dim, hidden_dim) self.V = nn.Linear(hidden_dim, hidden_dim) def forward(self,X): # X.shape = [batch_size, seq_len, hidden_dim] # 根据输入X的形状创建Q,K,V Q = self.Q(X) K = self.K(X) V = self.V(X) # K需要改为shape = [batch_size, seq_len, hidden_dim] QK = Q @ K.transpose(-1,-2)# K.transpose(1,2) softmax_QK = torch.softmax( QK / math.sqrt(self.hidden_dim), dim = -1) # 在最后一维上计算softmax output = softmax_QK @ V return output X = torch.rand(3,2,4)net = SelfAttention_1(4)net(X) 第二种写法：效率优化12345678910111213141516171819class SelfAttention_2(nn.Module): def __init__(self,hidden_dim): super().__init__() self.hidden_dim = hidden_dim self.proj = nn.Linear(hidden_dim, hidden_dim*3) #直接将Q，K，V网络合并 self.output_proj = nn.Linear(hidden_dim,hidden_dim) def forward(self,X): # X.shape = [batch_size, seq_len, hidden_dim] QKV = self.proj(X) # [batch_size, seq_len, dim*3] # torch.split(x, chunks_size, dim) Q,K,V = torch.split(QKV , self.hidden_dim , dim =-1) QK = torch.softmax(Q @ K.transpose(-1,-2) / math.sqrt(self.hidden_dim), dim = -1) output = QK @ V return self.output_proj(output) X = torch.rand(3,2,4)net = SelfAttention_2(4)net(X) 第三种写法：加上细节attention_mask，更完整 Attention计算时有dropout层，设置的位置很有趣 Attention计算时会加入attention_mask，因为样本会进行padding操作（不同长度的序列输入，需要相同的维度Attention_map，这要求在QKV以外进行mask处理） MultiHeadAttention过程中，除QKV三个矩阵外，还有output对应的投影矩阵 12345678910111213141516171819202122232425262728293031323334353637383940414243class SelfAttention_3(nn.Module): def __init__(self, hidden_dim): super().__init__() self.hidden_dim = hidden_dim self.proj = nn.Linear(hidden_dim, hidden_dim*3) self.att_drop = nn.Dropout(0.1) self.output_proj = nn.Linear(hidden_dim, hidden_dim) def forward(self, X,attention_mask = None): # attention_mask = [batch_size, seq_len] # X.shape = [batch_size, seq_len, hidden_dim] QKV = self.proj(X) Q, K, V = torch.split(QKV,self.hidden_dim, dim=-1) QK = Q @ K.transpose(-1,-2) / math.sqrt(self.hidden_dim) if attention_mask is not None: # mask的目的：屏蔽掉不该参与注意力计算的位置（无效padding、未来token屏蔽、无效区域） # masked_fill(condition, value) PyTorch 的方法：将 condition 为 True 的位置替换成 value # value = float(&quot;-1e20&quot;)是因为在softmax后值接近于0 QK = QK.masked_fill(attention_mask == 0, float(&quot;-1e20&quot;)) QK = torch.softmax(QK,dim = -1) # 这里在 BERT中的官方代码也说很奇怪，但是原文中这么用了，所以继承了下来 # （用于 output 后面会更符合直觉？） QK= self.att_drop(QK) output = QK @ V return self.output_proj(output) X = torch.rand(3,4,2)b = torch.tensor( [ [1, 1, 1, 0], [1, 1, 0, 0], [1, 0, 0, 0], ])print(f&quot;mask的初始形状{b.shape}&quot;)# unsqueeze(dim)表示在指定位置增加维度mask = b.unsqueeze(dim=1).repeat(1, 4, 1)print(f&quot;mask：{mask}&quot;)net = SelfAttention_3(2)net(X,mask) 第四种写法：从单头到多头一般在实际上的训练过程中都会使用 Multi Head, 多头注意力的实现核心是torch.permute()函数（支持高维转置操作）, 而且其实也仅仅是每个 Head 做完 Self-Attention 得到结果之后，进行拼接，然后过一个 output 投影矩阵。本质上是一种集成学习，与券商研报里的因子打分相比还是更高明一点。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667class MultiHeadAttention(nn.Module): def __init__(self,hidden_dim, n_head): super().__init__() self.n_head = n_head self.hidden_dim = hidden_dim self.head_dim = hidden_dim // n_head # 注意hidden_dim= head_dim * n_head # 将最后一个维度进行拆分，一个诸葛亮拆成n_head个臭皮匠 self.Q = nn.Linear(hidden_dim,hidden_dim) self.K = nn.Linear(hidden_dim,hidden_dim) self.V = nn.Linear(hidden_dim,hidden_dim) self.att_dropout = nn.Dropout(0.1) self.proj = nn.Linear(hidden_dim, hidden_dim) def forward(self,X,attention_mask=None): # attention_mask = [batch_size, seq_len] # X.shape = [batch_size, seq_len, hidden_dim] batch_size, seq_len, _ = X.size() Q = self.Q(X) K = self.Q(X) V = self.Q(X) # 下面reshape改变Q，K，V的shape为[batch_size,seq_len, n_head, head_dim] # 多头注意力训练建立在认知：训练多个小模型难度低于训练一个大模型 Q_state = Q.reshape(batch_size, seq_len, self.n_head, self.head_dim).permute(0,2,1,3) K_state = K.reshape(batch_size, seq_len, self.n_head, self.head_dim).permute(0,2,1,3) V_state = V.reshape(batch_size, seq_len, self.n_head, self.head_dim).permute(0,2,1,3) QK = Q_state @ K_state.transpose(-1,-2) / math.sqrt(self.head_dim) if attention_mask is not None: QK = QK.masked_fill(attention_mask == 0, float(&quot;-1e20&quot;)) # 或者有些实现是 attention_mask == 0 时 mask，看具体代码 # 对最后一个维度head_dim进行softmax平滑化 QK = torch.softmax(QK, dim = 3) QK = self.att_dropout(QK) print(QK.shape) print(V_state.shape) output = QK @ V_state print(output.shape) output = output.reshape(batch_size,seq_len, -1) return self.proj(output) #在 PyTorch 的 MultiHeadAttention（或自己实现的）里，attention_mask 的形状通常是：(batch_size, num_heads, query_len, key_len) # 所以我们要把这个 (batch_size, seq_len) 的 mask 扩展成 (batch_size, num_heads, seq_len, seq_len)，并且要广播到每个 head 上#最终得到的 attention_mask[batch, head, i, j] 表示：#第 batch 个样本的第 head 个头中，第 i 个 query 能不能看到第 j 个 keyattention_mask = ( torch.tensor( [ [0, 1], [0, 0], [1, 0], ] )# shape: (3, 2) 原始mask，0=可见，1=不可见（或反过来，取决于实现） .unsqueeze(1)# → (3, 1, 2) .unsqueeze(2)# → (3, 1, 1, 2) .expand(3, 8, 2, 2)# → (3, 8, 2, 2))x = torch.rand(3, 2, 128)# 八头注意力net = MultiHeadAttention(128, 8)net(x, attention_mask) 这里再次解释一下，为什么现在现在的代码实现都是 q k v 的投影矩阵都是分开写的，这是因为现在的模型很大，本身可能会做 张量并行，流水线并行等方式，所以分开写问题也不大（分开写很清晰），可能是加速效果并不明显。","link":"/2025/12/10/Attention/"},{"title":"你要高卢还是那个女孩","text":"喝了酒，人就会想说话，某位名片上挂着总经理衔的师兄就是这样。那天夜里大雨扫荡广州的长街小巷，下雨天恰好是听故事的天，于是师兄开始讲他的爱情。 师兄是从南方不知名的小城考到这的，行事比一般的南方人还要精明，大事上也很放得开，很诚恳踏实，因此在同学间很受好评。我们都说他是天生的生意人，师兄则说他刚上大学不是这样的。因为那时家里穷，每月的生活费都捉襟见肘，成绩也谈不上好，人黑黑矮矮的，总觉得低人一等。但是，他暗恋的女孩却有一米七！ “是那种白皙清丽的类型”，这是师兄的原话，因为他甚至没有保存一张那个女孩的正面照，只好指着一张模模糊糊的背影偷拍照跟我们说。不过翻了半天找到了毕业照，有点模糊，但也看着蛮漂亮的。 除了白皙靓丽，打动他的还有那个女孩会拉小提琴。师兄有一次路过校园里的小湖泊，看见音乐社在湖边排练，女孩是其中一位小提琴手。 我们大学时候都管那片叫情人湖，每到晚上就是亲亲啃啃的地方。师兄看到那个女孩就走不动道了。女孩在人群边，穿着白色短裙和海蓝色的宽松衬衫，长发是用白色带彩点的丝带系着。他讲的太仔细了，以至于我们都笑他是不是在旁边坐了一天。“像只蝴蝶一样”，讲这话的时候，师兄眼睛亮亮的。 于是一个男孩爱上了一个女孩。师兄每次食堂遇到了那个女孩，想靠近又不敢搭话，只好坐远点望着人家背影，有时恨不得拿个光滑的铁勺，用反光来看人家。 但是人家是有男朋友的，而且非常出名，某院篮球队长，毕业肯定是要拿全奖出国念博士的，而且据说家里有能力，必然是麻省、耶鲁这样的顶级名校，计划毕业后结婚，然后女孩也一起拿卡出国。当时的女生们听了眼睛都直冒星星，对师兄来说却是五雷轰顶，不亚于101听到塔山丢了。 他虽然为那女孩思念得神魂颠倒乃至于夜不能寐，但并不觉得自己有机会能和这位超级finance竞争，老家的父母还盼着他毕业找个体面的工作出人头地，他也觉得自己配不上人家。于是只好怀揣这份感情，又像偷了什么珍宝一样惭愧心虚。毕业的时候赠送了女孩一本小提琴谱，谎称自己随手挑的，分别之际做个礼物留念。其实是专门去乐行买的，动了可能连毕业论文都比不上的心思挑，里面还夹着自己做的书签……我们都笑了，有人说还以为师兄勇敢表白了，师兄说真他妈的美好啊！ 继续听师兄讲故事。师兄后来工作了几年，又去了香港做金融，天天跨洋去美国出差。赶上计算机红火发展的几年，拉了在华尔街用PPT骗投资人钱的同学入伙，创业成功之后卖股份实现财富自由，如今在国内做资管。他讲话很有逻辑很真诚务实，认识他的人不管合不合作都觉得他可信赖，于是他渐渐成了闻达之人，酒席上大家都会讲他的故事。如此折腾二十载，如今已是中年。 这时他听朋友说起那个女孩在上海的近况，竟也忍不住，心生一念就要去探访。他找个出差的机会，但其实并不需要他亲自去，可以想象他一脸严肃地交代助理，说这件事情很重要他必须亲自过问。于是飞去上海，穿着全新的衬衣，心中忐忑，从表盒里选了不那么夸张但有身份的表戴着，他还洗了牙。我问他说师兄你是准备和暗恋对象旧情复燃吗，他说不不，完全没有什么旧情，我只是打算认认真真地去见一面，想跟她说当年暗恋你的时候是很认真的。 约在一个很高级的餐厅见面。原谅我没听清餐厅的名字，但大概是那种放着舒缓音乐，大理石地板光可鉴人，甜点餐车在桌子间缓缓来去的高端场所吧。师兄说我从来没有这么紧张地等待一个人，总觉得衬衫颜色选的不对，手表有点张扬，而且他妈的怎么空调开那么冷，手有点抖。 有人想当课代表了，说师兄我知道结局了，女孩来了，跟你简简单单地吃完饭，她也老了，身材也走样了，和你谈话的主题也完全和预想的不同，然后就结束了，对不对？ 师兄说不，她没你说的那么老！好吧，其他的都跟你说的差不多，她来的时候还带着闺蜜，可以看出两个人都是居家的那种。两个人聊孩子话题根本插不进去，于是只好把共同认识的每个同学回忆了一遍，然后就轻轻散了。 我听出师兄话里的遗憾，问他，那你期望的故事结局是什么样的呢？师兄摘下眼镜，用眼睛布擦着说，我好歹希望，她能在一个有音乐的地方安安静静地听我说一句，我当年很喜欢你。 我说师兄你知道尤利乌斯·恺撒吗？他曾有过好几个女人，其中还包括埃及艳后，生下了后来的埃及法老托勒密十五世。你能够想象凯撒那种男人对吧，他跳上战马说我要征服高卢于是就出发了，就算心里无比惦记某个女孩，但他还是坚忍卓越，攻占高卢，掌握罗马，牛逼到死。 师兄说我会是凯撒吗？ 我说是啊，你虽然没见到那个女孩的丈夫，但你恐怕也不难想象他很有可能是读完博士入了站，但是做的成果没办法在美国留下，于是只好回国找教职，也许现在会是个头发稀疏的副教授吧。周末有空会在家待着，晚上吃过饭带孩子和老婆出门溜达，再看会儿学生的论文准备睡觉。而你呢？周末的晚上你绝对会在酒席上冲锋陷阵，潇洒倜傥，满嘴笑话，大家都喜欢你。你就是凯撒，酒席就是你的高卢，你攻占了它，你得到了光荣，想想你现有的东西，你会和他交换吗？ 这时候师兄已经戴上了眼镜，喝了酒而微微眯着朦胧的眼睛里重新又透出那股子南方人的狡猾来，他说，肯定不愿意。 是呀，所以我们都知道，师兄永远也得不到那个女孩。","link":"/2025/12/15/article1/"},{"title":"手写Transformer-Decoder","text":"今年是2025年，参加组会的你一定会遇到某位师兄或师姐说，上周读了《Transformer is All You Need》论文作汇报。然后就会展示出这张经典的图片并说，大语言模型由Encoder和Decoder组成，巴拉巴拉。你的导师似懂非懂点了点头，你也点了点头，原来LLM就是这样，开始玩起了手机。 事实上Transformer结构确实主要由Encoder、Decoder 两个部分组成，两个部分分别具有不一样的结构和输入输出。 但针对 Encoder、Decoder的特点，面对不同的人物和微调需求，出现了不同的、对 Transformer 进行优化的思路。例如，Google 仅选择了 Encoder 层，通过将 Encoder 层进行堆叠，再提出不同的预训练任务-掩码语言模型（Masked Language Model，MLM），打造了一统自然语言理解（Natural Language Understanding，NLU）任务的代表模型——BERT。而 OpenAI 则选择了 Decoder 层，使用原有的语言模型（Language Model，LM）任务，通过不断增加模型参数和预训练语料，打造了在 NLG（Natural Language Generation，自然语言生成）任务上优势明显的 GPT 系列模型，也是现今大火的LLM的基座模型。当然，还有一种思路是同时保留 Encoder 与 Decoder，打造预训练的 Transformer 模型，例如由 Google 发布的 T5模型。 总的来说，短短十年不到的时间，就按顺序出现了Encoder-Only、Encoder-Decoder、Decoder-Only三种主流预训练模型。 具体来说: BERT 采用了 Encoder-Only 结构，只包含编码器部分；而 GPT 采用了 Decoder-Only 结构，只包含解码器部分。T5 则采用了 Encoder-Decoder 结构， 其中最火热的当属业界主流技术Decoder-Only，Decoder-Only作为GPT的模型，真正让LLM为世人所知。这种结构只需要接受序列数据，输出序列数据，就可以完成翻译，问答对话等多任务。其本质是自回归模型。 以下内容大量参考chaofa用代码打点酱油的博客，模型技术以当前（2025.12）最好的开源模型——Llama开源文档为准 面试过程中让写 transformers Decoder 一定要沟通清楚是写一个 CausalLM decoder 还是原版的，原版的比较复杂，一般也不会让写。这里的 Decoder 一般指的是 CausalLM，具体变化是少了 encoder 部分的输入，所以也就没有了 encoder and decoder cross attention。因为重点希望写 CausalLM，所以没有 Cross attention 和 也省略了 token embedding 这一步。 知识点这里有很多细节可以多说 关于归一化方法的说明： 按照归一化方法来分，主要分为LayerNorm，BatchNorm，RMSNorm以及DeepNorm。如果按照归一化位置来分类，包括 postNorm 和 preNorm，但具体来说用postNorm比较多。 BatchNorm对数据的一定维度在batch数据中归一化，一般应用于图像。这种方法很难适用于序列数据，对于序列数据而言，在batch维度做归一意义不大，而且一个batch内的序列长度不同。LayerNorm是针对序列数据提出的一种归一化方法，主要在layer维度进行归一化，即对整个序列进行归一化。layerNorm会计算一个layer的所有activation的均值和方差，利用均值和方差进行归一化。RMSNorm的提出是为了提升layerNorm的训练速度提出的。RMSNorm也是一种layerNorm，只是归一化的方法不同。相比layerNorm中利用均值和方差进行归一化，RMSNorm 利用均方根进行归一化。 $$LayerNorm: \\dfrac{x-Ex}{\\sqrt{Dx+\\epsilon}}*\\gamma + \\beta$$ transformers decoder 的流程是：input -&gt; self-attention -&gt; cross-attention -&gt; FFN causalLM decoder 的流程是 input -&gt; self-attention -&gt; FFN其他 [self-attention, FFN] 是一个 block，一般会有很多的 block FFN 矩阵有两次变化，一次升维度，一次降维度。其中 LLaMA 对于 GPT 的改进还有把 GeLU 变成了 SwishGLU，多了一个矩阵。所以一般升维会从 4h -&gt; 4h * 2 / 3 FFN是常见的前馈神经网络，主要经历一个升维再降维的过程（生成器基本都这样），为什么常见都是升4维呢？从工程角度来说，这是权衡训练成本和实际效果后的选择。从理论角度来说，常见的激活函数都会导致近50%的信息损失（参考ReLU），两层的结构说明最后会导致只有50% * 50% = 25%的信息保留，为了控制前后信息量不大变化，选择升4维。 原版的 transformers 用 post-norm, 后面 gpt2, llama 系列用的是 pre-norm。其中 llama 系列一般用 RMSNorm 代替 GPT and transformers decoder 中的 LayerNorm。 具体来说用postNorm比较多，具体的原因可以看之前这一篇 为什么大模型结构设计中往往使用postNorm而不用preNorm？ Decoder部分的代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108import mathimport torchimport torch.nn as nnimport warningswarnings.filterwarnings(action=&quot;ignore&quot;)# 写一个Blockclass SimpleDecoder(nn.Module): def __init__(self, hidden_dim, n_heads, dropout = 0.1): super().__init__() self.n_heads = n_heads self.head_dim = hidden_dim // n_heads self.dropout = dropout # 这里按照 transformers 中的 decoder 来写，用 post_norm 的方式实现，主要有 残差链接 # eps 是为了防止溢出；其中 llama 系列的模型一般用的是 RMSnorm 以及 pre-norm（为了稳定性） # RMSnorm 没有一个 recenter 的操作，而 layernorm 是让模型重新变成 均值为 0，方差为 1 # RMS 使用 w平方根均值进行归一化 $\\sqrt{\\frac{1}{n} \\sum_{1}^{n}{a_i^2} }$ self.layernorm_att = nn.LayerNorm(hidden_dim, eps =0.00001) self.q_proj = nn.Linear(hidden_dim, hidden_dim) self.k_proj = nn.Linear(hidden_dim, hidden_dim) self.v_proj = nn.Linear(hidden_dim, hidden_dim) self.o_proj = nn.Linear(hidden_dim, hidden_dim) self.drop_att = nn.Dropout(self.dropout) #准备FFN前馈神经网络 self.up_proj = nn.Linear(hidden_dim, hidden_dim * 4) self.down_proj = nn.Linear(hidden_dim * 4, hidden_dim) self.layernorm_ffn = nn.LayerNorm(hidden_dim, eps = 0.00001) self.act_fn = nn.ReLU() self.drop_ffn = nn.Dropout(self.dropout) def attention_output(self, Q, K , V, attention_mask = None): #计算两者相关性 K = K.transpose(2,3) # [batch_size, n_heads, head_dim, seq_len] # 还没有softmax的att_weight att_weight = (Q @ K) / math.sqrt(self.head_dim) #attention mask进行依次调整：变成causal_attention if attention_mask is not None: #变成下三角矩阵 attention_mask = attention_mask.tril() #提取下三角部分单向注意力 att_weight = att_weight.masked_fill(attention_mask == 0, float(&quot;-1e20&quot;)) #对0部分参与一个极小值，在softmax后为0 else: #人工构造下三角Attention mask attention_mask = torch.ones_like(att_weight).tril() att_weight = att_weight.masked_fill(attention_mask == 0, float(&quot;-1e20&quot;)) att_weight = torch.softmax(att_weight, dim = -1) # print(att_weight) # Dropout论文最早提出的时候被拒稿，认为只有工程而无理论。再看看今天Dropout无处不在，果然是“入关后自有大儒为我辩证” att_weight = self.drop_att(att_weight) mid_output = att_weight @ V # shape [batch_size, n_heads, seq_len, head_dim] #contiguous()函数在PyTorch中用于确保张量（tensor）在内存中的数据是连续存储的。当执行transpose()、permute()等改变维度顺序的操作后，张量可能会变得“非连续”（不按行优先或列优先存储）。contiguous()的作用是返回一个新的、内存连续的张量副本，这样view()（重塑形状）等需要连续内存的操作才能正确执行，提升计算效率。 mid_output = mid_output.transpose(1,2).contiguous() batch_size, seq_len, _, _ = mid_output.size() mid_output = mid_output.view(batch_size, seq_len, -1) # O矩阵只是权重矩阵，多头注意力机制中，对拼接后的矩阵处理 output = self.o_proj(mid_output) return output def attention_block(self, X, attention_mask = None): batch_size, seq_len, _ = X.size() Q = self.q_proj(X).view(batch_size, seq_len, self.n_heads, -1).transpose(1,2) K = self.k_proj(X).view(batch_size, seq_len, self.n_heads, -1).transpose(1,2) V = self.v_proj(X).view(batch_size, seq_len, self.n_heads, -1).transpose(1,2) output = self.attention_output(Q,K,V,attention_mask = attention_mask) return self.layernorm_att(X + output) def ffn_block(self, X): # act_fn = ReLU up = self.act_fn(self.up_proj(X)) down = self.down_proj(up) #dropout down = self.drop_ffn(down) # norm return self.layernorm_ffn(X + down) def forward(self, X, attention_mask = None): # X 一般假设是已经经过embedding的输入， （batch_size, seq_len, hidden_dim） # attention_mask 一般指的是 tokenizer 后返回的 mask 结果，表示哪些样本需要忽略 # shape 一般是： (batch_size, n_heads, seq_len) att_output = self.attention_block(X, attention_mask = attention_mask) ffn_output = self.ffn_block(att_output) return ffn_output# 测试x = torch.rand(3, 4, 64) # [batch_size, seq_len, hidden_dim]net = SimpleDecoder(64, 8) # hidden_dim = 64,n_heads = 8mask = ( torch.tensor([[1, 1, 1, 1], [1, 1, 0, 0], [1, 1, 1, 0]]) .unsqueeze(1) .unsqueeze(2) .repeat(1, 8, 4, 1))net(x, mask).shape 关于推理过程如果你认真看了上面的代码部分，不知道你会不会产生这样的疑问：大语言模型不是根据当前的字预测下一个字是什么吗？请问一下推理时prefill阶段为什么需要计算所有token的q。预测next token的话不是只需要计算上一个token的q就好了么？因此只需要算prefill最后一个token的q就行了？ 这是一个非常非常非常好的问题👍 核心原因是 Decoder 中有很多层，假设只有一层，你的思考是没有问题。但是 decoder 有很多 block，上一层（假设第2层）的输入是下一层的输出，下一层（第一层）的输出依赖于 qkv 的计算，那么计算第二层的 q k v的时候，如果不知道第一层的 q k v（或者output），那么第二层的 k v 就无法计算出来了。因为 prefill 之前没有缓存好每一层的 k v 是什么。 那么换句话说，如果能缓存好每一层的K，V矩阵，推理速度将会大大增加！ 唯一需要考虑的问题是：如何高效的实现KV矩阵缓存。我们将来到KV Cache ——百倍推理加速技术","link":"/2025/12/11/TransformerDecoder/"},{"title":"LLM中的LoRA实现","text":"AI时代，诸如OpenAI、Google、阿里巴巴、Meta等大公司可以依靠恐怖的计算资源来训练基座大模型，但对于对资源有限的企业或课题组来说，如何高效、快速对模型进行领域或任务的微调，以低成本地使用 LLM 完成目标任务，是非常重要的。 涌现能力区分 LLM （Large Language Model）与传统 PLM（Pre-trained Language Model） 最显著的特征即是 LLM 具备涌现能力 。涌现能力是指同样的模型架构与预训练任务下，某些能力在小型模型中不明显，但在大型模型中特别突出。可以类比到物理学中的相变现象，涌现能力的显现就像是模型性能随着规模增大而迅速提升，超过了随机水平，也就是我们常说的量变引起了质变。 一般来说，LLM 指包含数百亿（或更多）参数的语言模型，它们往往在数 T token 语料上通过多卡分布式集群进行预训练，具备远超出传统预训练模型的文本理解与生成能力。不过，随着 LLM 研究的不断深入，多种参数尺寸的 LLM 逐渐丰富，广义的 LLM 一般覆盖了从十亿参数（如 Qwen-1.5B）到千亿参数（如 Grok-314B）的所有大型语言模型。只要模型展现出涌现能力，即在一系列复杂任务上表现出远超传统预训练模型（如 BERT、T5）的能力与潜力，都可以称之为 LLM。 在这种恐怖的参数规模下，调教大语言模型（更新模型参数）所需的电力资源，显卡资源都是恐怖的。 首先需要回顾一下，对 LLM 进行预训练和进行有监督微调的核心差异在于什么。 目前成型的 LLM 一般通过 Pretrain-SFT-RLHF 三阶段来训练，Pretrain 阶段，会对海量无监督文本进行自监督建模，来学习文本语义规则和文本中的世界知识；SFT 阶段，一般通过对 Pretrain 好的模型进行指令微调，即训练模型根据用户指令完成对应任务，从而使模型能够遵循用户指令，根据用户指令进行规划、行动和输出。因此，Pretrain 和 SFT 均使用 CLM （Causal Language Model）建模，其核心差异在于，Pretrain 使用海量无监督文本进行训练，模型直接对文本执行“预测下一个 token”的任务；而 SFT 使用构建成对的指令对数据，模型根据输入的指令，建模后续的输出。反映到具体的训练实现上，Pretrain 会对全部 text 进行 loss 计算，要求模型对整个文本实现建模预测；而 SFT 仅对输出进行 loss 计算，不计算指令部分的 loss。。但是，由于 LLM 参数量大，训练数据多，通过上述方式对模型进行训练（主要指 SFT 及 RLHF）需要调整模型全部参数，资源压力非常大。 这时候，微调技术应运而生。 微调（Fine Tuning）对所有参数进行训练调整的全量微调太过昂贵，需要大量的GPU显存，个人的显卡上很难实现，因此各种参数高效（Parameter-Efficient）的方法层出不穷，最受大家欢迎的就是 LoRA方法《LoRA: Low-Rank Adaptation of Large Language Models》 LoRA 有很多的优点，节约显存，训练快，效果损失较小（相对于全参数微调），推理的时候不增加耗时，可以做一个插入式组件使用。缺点当然也有，那就是还是会有一些效果的损失（笑）。 与其他高效微调方法相比，LoRA 存在以下优势： 以针对不同的下游任务构建小型 LoRA 模块，从而在共享预训练模型参数基础上有效地切换下游任务。 LoRA 使用自适应优化器（Adaptive Optimizer），不需要计算梯度或维护大多数参数的优化器状态，训练更有效、硬件门槛更低。 LoRA 使用简单的线性设计，在部署时将可训练矩阵与冻结权重合并，不存在推理延迟。 LoRA 与其他方法正交，可以组合。 因此，LoRA 成为目前高效微调 LLM 的主流方法，尤其是对于资源受限、有监督训练数据受限的情况下，LoRA 微调往往会成为 LLM 微调的首选方法。 LoRA核心原理LoRA代码Pytorch实现","link":"/2025/12/10/LoRA/"},{"title":"我是谁","text":"很久以前看B站《我的音乐你听吗?》，算是这么些年罕见的高质量原创音乐舞台，也记住了很多优秀作品。印象最深的是《我是谁》这首歌。那时还是无忧无虑的大二时光，没有意识到无论是自己的人生还是国家都在一个关键点。但我还是如歌词里唱着一样，被张佩服用石头砸了一下头，完事醒了之后，忘了我是谁。 时代变化如此之快，身处其中的时候总是感觉不到，等停下来才后知后觉，怎么没买英伟达的股票，还有黄金。数据的爆炸已经让人脑无法荷载，这个时代的数据已经成为了“房间里的大象”，愈来愈烈的舆论争吵与人群对立不过是反反复复在说明一件事：所有人都是盲人摸象，只能看到自己眼中的世界，然后通过“触摸大象”来加深这种刻板。","link":"/2025/12/06/%E6%88%91%E6%98%AF%E8%B0%81/"},{"title":"LLM的前世今生","text":"时代的发展速度在变的越来越快。1975年，半导体芯片领域的著名论断————摩尔定律被提出（集成电路上可容纳的晶体管数目，每隔约两年便会增加一倍）。半导体行业大致按照摩尔定律发展了半个多世纪，带来了电脑，互联网，手机等事物。如今，从2017年开始到2025年即将结束，从NLP（自然语言处理）到LLM（通用大语言模型），深度学习技术的发展，并行计算GPU的进步，智能时代还会需要半个世纪的时间才能到达吗？ 不是，身处技术爆炸的时刻，时代发展速度只会越来越快，直到遇到人本困境。本文旨在介绍LLM大语言模型技术是如何从曾经的NLP任务，到通用预训练模型（Pre-Trained Model），再到未来展望的自学习模型（或者是AI Agent）。 自然语言处理：NLP（Natural Language Process）任务很好理解，就是试图让机器掌握人类使用的自然语言，诸如语音识别，外语翻译，舆情监控等任务都是其中的一部分。NLP有2个核心的任务：分别是【自然语言理解—NL Understand】和【自然语言生成—NL Generate】。 NLU是所有支持机器理解文本内容的方法模型或任务的总称，即能够进行常见的文本分类、序列标注、信息抽取等任务。简单来说，是指自然语言理解的运用希望机器人能够像人一样，具备正常的语言理解能力。例如，意图识别和实体提取的关键技能。 NLG （自然语言生成）是NLP的另一项核心任务，主要目的是降低人类和机器之间的沟通鸿沟，将非语言格式的数据转换成人类可以理解的语言格式。","link":"/2025/12/14/Summary/"},{"title":"一些有意思的算法题","text":"不想刷LeetCode啊！！！ 言归正传，写代码还是很有意思的事情。我大学读的是数学专业，一开始只觉得写代码是计算机专业该干的事，后来惊奇地发现，有些算法莫名与那些恐怖的数学知识相关，而这些算法都是为了解决一个具体的有趣问题。这里整理一些LeetCode中有趣的题目，更多还是结合数学的内容。 Q1: 爬楼梯这个问题我应该是最早在电影《少年班》里看到的，那个神童摇着龟壳，根据掉落铜钱来回答的场景太神棍，印象很深。 假设你正在爬楼梯。需要 n 阶你才能到达楼顶。每次你可以爬 1 或 2 个台阶。你有多少种不同的方法可以爬到楼顶呢？比如2级台阶有两种方法：1+1、2；3级台阶有三种方法：1+1+1、1+2、2+1； 从递归的角度容易考虑到，记上$n$级台阶的方法数量为$f(n)$，那么前一步一定在$n-1$级台阶或者$n-2$级台阶，有$f(n)=f(n-1)+f(n-2)$等式成立。这就是斐波那契数列的通项$1,2,3,5,8,13,21….$，自然很容易的写出这样的算法。 12345678910def climbStairs(self, n: int) -&gt; int: if n &lt;= 2: return n # Fibonacci-like sequence first, second = 1, 2 for i in range(3, n + 1): first, second = second, first + second return second 事实上对于$f(n)=f(n-1)+f(n-2)$的齐次线性递推式，除了用高中在数列中用到的特征方程方法来求出斐波那契数列的通项公式外，还可以将齐次线性递推式写为矩阵，使用矩阵快速幂方法来实现。后者的优势在于，对于某些特殊的非齐次线性递推式，也可以处理。 Q2: 只出现一次的数 给你一个 非空 整数数组 nums ，除了某个元素只出现一次以外，其余每个元素均出现两次。找出那个只出现了一次的元素。你必须设计并实现线性时间复杂度的算法来解决此问题，且该算法只使用常量额外空间。 这道题难度easy，但是其解法中的位运算方法实在太优雅（依赖于其余数只出现偶数次），让我想起很多年前见到过一个位运算求N皇后问题的牛逼解法。这正好借这个机会展示二进制的魅力。 1234567891011def singleNumber(self, nums: List[int]) -&gt; int: &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; n = len(nums) index = 0 for i in range(n): # 异或有结合律和交换律 index ^= nums[i] return index","link":"/2025/12/11/%E4%B8%80%E4%BA%9B%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E7%AE%97%E6%B3%95%E9%A2%98/"}],"tags":[{"name":"大语言模型","slug":"大语言模型","link":"/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"name":"每个人都是作家","slug":"每个人都是作家","link":"/tags/%E6%AF%8F%E4%B8%AA%E4%BA%BA%E9%83%BD%E6%98%AF%E4%BD%9C%E5%AE%B6/"},{"name":"生活","slug":"生活","link":"/tags/%E7%94%9F%E6%B4%BB/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"}],"categories":[{"name":"LLM学习系列","slug":"LLM学习系列","link":"/categories/LLM%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97/"},{"name":"个人拙作","slug":"个人拙作","link":"/categories/%E4%B8%AA%E4%BA%BA%E6%8B%99%E4%BD%9C/"}],"pages":[]}