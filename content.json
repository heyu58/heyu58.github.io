{"posts":[{"title":"Distillation","text":"","link":"/2025/12/17/Distillation/"},{"title":"当代孔乙己-Attention机制的四种写法","text":"大语言模型（LLM）发展方兴未艾，从ChatGPT横空出世横扫英文互联网，到DeepSeek惊讶世人宣告低成本LLM可行性，2025年短短一年便造就了人工智能领域如此震撼的估值繁荣（or bubble），不得不让人感慨《三体》中 “技术爆炸” 一词提出的先见性。 一切从《Attention is All You Need》开始。这篇论文彻底抛弃了长序列预测的LSTM和RNN模型，在学术界有着划时代意义；更在传播学上展示了一个好的名字到底有多重要。 以下内容大量参考chaofa用代码打点酱油的博客 在AI相关的面试中，经常会要求对Transformer架构有足够的理解，通常通过手写self-attention机制来验证。这里给出了Attention机制从简到繁的四种写法 什么是$Self-Attention$？$Attention$ 机制通常在已实现 $Embedding$ + 位置编码后进行。引入 $Attention$ 机制主要是由于 $Embedding$ 和位置编码依然不够表示所有的信息。 举个例子 “你不要打人” 和 “你快去打水” 中 “打” 字含义截然不同，但仅靠 $Embedding$ 和位置编码两者并不能区别，这就需要结合上下文来增加更多信息，$Attention$ 就是这种结合的方式。 $$SelfAttention(X)=softmax(\\frac{Q\\cdot K}{\\sqrt{d}})\\cdot V$$ $Q = K = V = W_{Q/K/V} \\cdot X$，其中$Q，K，V$分别对应不同（形状也不同）的权重矩阵W matmul和@符号是一样的作用，矩阵乘法 为什么要除以$\\sqrt{d}$：一是防止梯度消失（参考Kaiming Initialization参数初始化）；二是为了让QK的内积分布保持和输入一样 爱因斯坦方程表达式用法：torch.einsum(“bqd,bkd-&gt;bqk”,X,X).shape 为什么使用softmax?注意力机制，本质上是想找与查询向量最相关的隐向量序列，实际上是想找，onehot(argmax)，比如的任务是中视野中找苹果，直接依据苹果的特征查找苹果。但是这个求解是无法计算梯度，所以才用onehot(argmax)的光滑版本softmax，以获得良好的梯度特性。是一种数学上的妥协 X.repeat(1,1,3)表示在不同维度进行repeat操作，也可以用tensor.expand操作 12345import mathimport warningsimport torchimport torch.nn as nnwarnings.filterwarnings(action='ignore') 第一种写法：简洁123456789101112131415161718192021222324class SelfAttention_1(nn.Module): # 父类是pytorch中的核心Module def __init__(self, hidden_dim): super(SelfAttention_1,self).__init__()# 调用父类的构造方法，并传递所有参数 self.hidden_dim = hidden_dim self.Q = nn.Linear(hidden_dim, hidden_dim) self.K = nn.Linear(hidden_dim, hidden_dim) self.V = nn.Linear(hidden_dim, hidden_dim) def forward(self,X): # X.shape = [batch_size, seq_len, hidden_dim] # 根据输入X的形状创建Q,K,V Q = self.Q(X) K = self.K(X) V = self.V(X) # K需要改为shape = [batch_size, seq_len, hidden_dim] QK = Q @ K.transpose(-1,-2)# K.transpose(1,2) softmax_QK = torch.softmax( QK / math.sqrt(self.hidden_dim), dim = -1) # 在最后一维上计算softmax output = softmax_QK @ V return output X = torch.rand(3,2,4)net = SelfAttention_1(4)net(X) 第二种写法：效率优化12345678910111213141516171819class SelfAttention_2(nn.Module): def __init__(self,hidden_dim): super().__init__() self.hidden_dim = hidden_dim self.proj = nn.Linear(hidden_dim, hidden_dim*3) #直接将Q，K，V网络合并 self.output_proj = nn.Linear(hidden_dim,hidden_dim) def forward(self,X): # X.shape = [batch_size, seq_len, hidden_dim] QKV = self.proj(X) # [batch_size, seq_len, dim*3] # torch.split(x, chunks_size, dim) Q,K,V = torch.split(QKV , self.hidden_dim , dim =-1) QK = torch.softmax(Q @ K.transpose(-1,-2) / math.sqrt(self.hidden_dim), dim = -1) output = QK @ V return self.output_proj(output) X = torch.rand(3,2,4)net = SelfAttention_2(4)net(X) 第三种写法：加上细节attention_mask，更完整 Attention计算时有dropout层，设置的位置很有趣 Attention计算时会加入attention_mask，因为样本会进行padding操作（不同长度的序列输入，需要相同的维度Attention_map，这要求在QKV以外进行mask处理） MultiHeadAttention过程中，除QKV三个矩阵外，还有output对应的投影矩阵 12345678910111213141516171819202122232425262728293031323334353637383940414243class SelfAttention_3(nn.Module): def __init__(self, hidden_dim): super().__init__() self.hidden_dim = hidden_dim self.proj = nn.Linear(hidden_dim, hidden_dim*3) self.att_drop = nn.Dropout(0.1) self.output_proj = nn.Linear(hidden_dim, hidden_dim) def forward(self, X,attention_mask = None): # attention_mask = [batch_size, seq_len] # X.shape = [batch_size, seq_len, hidden_dim] QKV = self.proj(X) Q, K, V = torch.split(QKV,self.hidden_dim, dim=-1) QK = Q @ K.transpose(-1,-2) / math.sqrt(self.hidden_dim) if attention_mask is not None: # mask的目的：屏蔽掉不该参与注意力计算的位置（无效padding、未来token屏蔽、无效区域） # masked_fill(condition, value) PyTorch 的方法：将 condition 为 True 的位置替换成 value # value = float(&quot;-1e20&quot;)是因为在softmax后值接近于0 QK = QK.masked_fill(attention_mask == 0, float(&quot;-1e20&quot;)) QK = torch.softmax(QK,dim = -1) # 这里在 BERT中的官方代码也说很奇怪，但是原文中这么用了，所以继承了下来 # （用于 output 后面会更符合直觉？） QK= self.att_drop(QK) output = QK @ V return self.output_proj(output) X = torch.rand(3,4,2)b = torch.tensor( [ [1, 1, 1, 0], [1, 1, 0, 0], [1, 0, 0, 0], ])print(f&quot;mask的初始形状{b.shape}&quot;)# unsqueeze(dim)表示在指定位置增加维度mask = b.unsqueeze(dim=1).repeat(1, 4, 1)print(f&quot;mask：{mask}&quot;)net = SelfAttention_3(2)net(X,mask) 第四种写法：从单头到多头一般在实际上的训练过程中都会使用 Multi Head, 多头注意力的实现核心是torch.permute()函数（支持高维转置操作）, 而且其实也仅仅是每个 Head 做完 Self-Attention 得到结果之后，进行拼接，然后过一个 output 投影矩阵。本质上是一种集成学习，与券商研报里的因子打分相比还是更高明一点。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667class MultiHeadAttention(nn.Module): def __init__(self,hidden_dim, n_head): super().__init__() self.n_head = n_head self.hidden_dim = hidden_dim self.head_dim = hidden_dim // n_head # 注意hidden_dim= head_dim * n_head # 将最后一个维度进行拆分，一个诸葛亮拆成n_head个臭皮匠 self.Q = nn.Linear(hidden_dim,hidden_dim) self.K = nn.Linear(hidden_dim,hidden_dim) self.V = nn.Linear(hidden_dim,hidden_dim) self.att_dropout = nn.Dropout(0.1) self.proj = nn.Linear(hidden_dim, hidden_dim) def forward(self,X,attention_mask=None): # attention_mask = [batch_size, seq_len] # X.shape = [batch_size, seq_len, hidden_dim] batch_size, seq_len, _ = X.size() Q = self.Q(X) K = self.Q(X) V = self.Q(X) # 下面reshape改变Q，K，V的shape为[batch_size,seq_len, n_head, head_dim] # 多头注意力训练建立在认知：训练多个小模型难度低于训练一个大模型 Q_state = Q.reshape(batch_size, seq_len, self.n_head, self.head_dim).permute(0,2,1,3) K_state = K.reshape(batch_size, seq_len, self.n_head, self.head_dim).permute(0,2,1,3) V_state = V.reshape(batch_size, seq_len, self.n_head, self.head_dim).permute(0,2,1,3) QK = Q_state @ K_state.transpose(-1,-2) / math.sqrt(self.head_dim) if attention_mask is not None: QK = QK.masked_fill(attention_mask == 0, float(&quot;-1e20&quot;)) # 或者有些实现是 attention_mask == 0 时 mask，看具体代码 # 对最后一个维度head_dim进行softmax平滑化 QK = torch.softmax(QK, dim = 3) QK = self.att_dropout(QK) print(QK.shape) print(V_state.shape) output = QK @ V_state print(output.shape) output = output.reshape(batch_size,seq_len, -1) return self.proj(output) #在 PyTorch 的 MultiHeadAttention（或自己实现的）里，attention_mask 的形状通常是：(batch_size, num_heads, query_len, key_len) # 所以我们要把这个 (batch_size, seq_len) 的 mask 扩展成 (batch_size, num_heads, seq_len, seq_len)，并且要广播到每个 head 上#最终得到的 attention_mask[batch, head, i, j] 表示：#第 batch 个样本的第 head 个头中，第 i 个 query 能不能看到第 j 个 keyattention_mask = ( torch.tensor( [ [0, 1], [0, 0], [1, 0], ] )# shape: (3, 2) 原始mask，0=可见，1=不可见（或反过来，取决于实现） .unsqueeze(1)# → (3, 1, 2) .unsqueeze(2)# → (3, 1, 1, 2) .expand(3, 8, 2, 2)# → (3, 8, 2, 2))x = torch.rand(3, 2, 128)# 八头注意力net = MultiHeadAttention(128, 8)net(x, attention_mask) 这里再次解释一下，为什么现在现在的代码实现都是 q k v 的投影矩阵都是分开写的，这是因为现在的模型很大，本身可能会做 张量并行，流水线并行等方式，所以分开写问题也不大（分开写很清晰），可能是加速效果并不明显。","link":"/2025/12/10/Attention/"},{"title":"LLM中的LoRA实现","text":"AI时代，诸如OpenAI、Google、阿里巴巴、Meta等大公司可以依靠恐怖的计算资源来训练基座大模型，但对于对资源有限的企业或课题组来说，如何高效、快速对模型进行领域或任务的微调，以低成本地使用 LLM 完成目标任务，是非常重要的。 涌现能力区分 LLM （Large Language Model）与传统 PLM（Pre-trained Language Model） 最显著的特征即是 LLM 具备涌现能力 。涌现能力是指同样的模型架构与预训练任务下，某些能力在小型模型中不明显，但在大型模型中特别突出。可以类比到物理学中的相变现象，涌现能力的显现就像是模型性能随着规模增大而迅速提升，超过了随机水平，也就是我们常说的量变引起了质变。 一般来说，LLM 指包含数百亿（或更多）参数的语言模型，它们往往在数 T token 语料上通过多卡分布式集群进行预训练，具备远超出传统预训练模型的文本理解与生成能力。不过，随着 LLM 研究的不断深入，多种参数尺寸的 LLM 逐渐丰富，广义的 LLM 一般覆盖了从十亿参数（如 Qwen-1.5B）到千亿参数（如 Grok-314B）的所有大型语言模型。只要模型展现出涌现能力，即在一系列复杂任务上表现出远超传统预训练模型（如 BERT、T5）的能力与潜力，都可以称之为 LLM。 在这种恐怖的参数规模下，调教大语言模型（更新模型参数）所需的电力资源，显卡资源都是恐怖的。 首先需要回顾一下，对 LLM 进行预训练和进行有监督微调的核心差异在于什么。 目前成型的 LLM 一般通过 Pretrain-SFT-RLHF 三阶段来训练，Pretrain 阶段，会对海量无监督文本进行自监督建模，来学习文本语义规则和文本中的世界知识；SFT 阶段，一般通过对 Pretrain 好的模型进行指令微调，即训练模型根据用户指令完成对应任务，从而使模型能够遵循用户指令，根据用户指令进行规划、行动和输出。因此，Pretrain 和 SFT 均使用 CLM （Causal Language Model）建模，其核心差异在于，Pretrain 使用海量无监督文本进行训练，模型直接对文本执行“预测下一个 token”的任务；而 SFT 使用构建成对的指令对数据，模型根据输入的指令，建模后续的输出。反映到具体的训练实现上，Pretrain 会对全部 text 进行 loss 计算，要求模型对整个文本实现建模预测；而 SFT 仅对输出进行 loss 计算，不计算指令部分的 loss。。但是，由于 LLM 参数量大，训练数据多，通过上述方式对模型进行训练（主要指 SFT 及 RLHF）需要调整模型全部参数，资源压力非常大。 这时候，微调技术应运而生。 微调（Fine Tuning）对所有参数进行训练调整的全量微调太过昂贵，需要大量的GPU显存，个人的显卡上很难实现，因此各种参数高效（Parameter-Efficient）的方法层出不穷，最受大家欢迎的就是 LoRA方法《LoRA: Low-Rank Adaptation of Large Language Models》 LoRA 有很多的优点，节约显存，训练快，效果损失较小（相对于全参数微调），推理的时候不增加耗时，可以做一个插入式组件使用。缺点当然也有，那就是还是会有一些效果的损失（笑）。 与其他高效微调方法相比，LoRA 存在以下优势： 以针对不同的下游任务构建小型 LoRA 模块，从而在共享预训练模型参数基础上有效地切换下游任务。 LoRA 使用自适应优化器（Adaptive Optimizer），不需要计算梯度或维护大多数参数的优化器状态，训练更有效、硬件门槛更低。 LoRA 使用简单的线性设计，在部署时将可训练矩阵与冻结权重合并，不存在推理延迟。 LoRA 与其他方法正交，可以组合。 因此，LoRA 成为目前高效微调 LLM 的主流方法，尤其是对于资源受限、有监督训练数据受限的情况下，LoRA 微调往往会成为 LLM 微调的首选方法。 LoRA核心原理LoRA代码Pytorch实现","link":"/2025/12/10/LoRA/"},{"title":"RAG检索增强生成模型","text":"RAG技术的诞生源于生成模型无法避免的幻觉问题。具体来说纯生成模型，尤其是基于大语言模型的生成模型，虽然在生成连贯、流畅的文本方面表现出色，但它们有时会产生与事实不符的信息。这种现象被称为幻觉（hallucination）。幻觉的产生通常是由于模型在训练过程中学习到的信息是不完整的，或者模型在尝试生成看似合理但实际上并非基于真实信息的内容时过度自信。 什么是RAG？RAG全称为检索增强生成，(Retrieval Augmented Generation，RAG)。从名字来看，这种技术结合了检索 (Retrieval) 和生成 (Generation) 两个过程，旨在提高机器生成文本的相关性、准确性和多样性。RAG技术使用外部数据源的信息辅助文本生成，通过访问外部数据库检索得到有关的信息（通常以chunk形式返回所需信息）。把检索得到的信息与用户源问题合并为提示，让大语言模型从包含外部信息的提示中学习知识（in-context learning）并生成正确答案。从而缓解了幻觉问题。（缓解幻觉问题还有提示词约束，CoT推理链，RLHF反馈等等方法） RAG的核心部件可以参考流程图 向量数据库（Vector Database）：一种专门设计用来高效存储和检索向量数据的数据库系统。数据经常以高维向量的形式存在，比如文本、图片或其他类型的数据经过嵌入模型转换成的向量。这些向量代表了原始数据的特征和语义信息，可以用于各种相似性搜索和数据分析任务。向量数据库通过优化这类向量数据的存储结构和检索算法，提供了一种高效率的方式来处理大规模向量集合。检索增强生成模型的一个关键步骤是从大规模数据集中检索相关信息，这些信息随后用于辅助生成模型产生回答。向量数据库能够快速检索到与查询向量最相似的数据向量，从而大大加快了这一过程，提高了信息检索的效率和准确性。 查询检索（Retriever）：检索器可以从一个大规模的文档集合或知识库中检索出与给定查询最相关的信息，这个过程是通过比较查询的表示（通常是一个向量）和文档集合中每个文档的表示来完成的，通过检索器，检索增强生成系统能够访问到更广泛的、实时更新的信息，从而扩展了模型处理问题时的知识范围。目标是找到最能帮助生成系统回答查询的信息。 重新排序（Re-ranker）：我们可以在许多文本文档中执行语义搜索，相关文档可能有数万到数百亿个。但由于大语言模型对于传递文本量有限制，我们需要对文档质量进行排序，然后返回top-k文档用于下一步检索生成。在重排器中，给定查询和文档对，将输出相似性得分。我们使用这个分数根据与我们的查询的相关性对文档进行重新排序。 生成回答（Generator）在整个检索增强生成系统中，生成器是负责将检索到的信息生成最终文本输出的组件，它利用这些检索到的信息来构建回答或完成特定的文本生成任务。生成器将检索到的多个文档或信息片段综合考虑，融合它们的内容来构建一个连贯、逻辑一致的输出。这个过程涉及到语言生成的各个方面，包括词汇选择、语法结构和内容连贯性等。 因为检索增强生成不仅用于事实推理，还可以被用于生成代码、图像等，因此面对不同下游任务，生成器模型已经衍生出多种变体。Transformer模型经常应用于文本生成任务；VisualGPT常用于从图像生成文本描述；Stable Diffusion主要用于根据文本提示生成图像；Codex则专注于从文本描述生成代码。 外部数据：知识文档的准备文件类型多样，知识源可能有多种形式，如Word，TXT文件，CSV数据表，Excel表格，PDF文件，HTML网页，图片，视频等等（这些内容已经涉及多模态） 所以第一步就是进行文档加载器（PDF提取）或多模态技术（OCR，矢量图检索），将知识源（私有，专业，时效）转化为大语言模型能够理解的文本序列信息。 此外，文档可能会过长，所以另一项关键步骤文档切片，英文名chunk。关于文档分块方法的选择，有以下参考： 固定大小分块：nothing to talk。通常会保持块之间的一些重叠，以确保语义的连贯性。 内容分块：根据标点符号或段落分割，或直接用更高级的NLTK或spaCy库提供的句子分割功能。 递归分块：多数情况下推荐方法。具体来说，在Langchain中会先通过段落换行符分割。然后检查这些块的大小，如果不超过一定阈值，则该块被保留。对于大小超过标准的块，使用单换行符再次分割。以此类推，不断根据块大小更新更小的分块规则（如空格，句号）。这种方法显然更加灵活，挑战也在此：需要制定精细规则来决定何时和如何分割文本。 特殊结构分块：主要针对Markdown、Latex、Code等特殊文件。 然而还是要决定chunk的大小，如何选择分块大小呢？ 不同的嵌入模型有最佳的输入大小，如Openai的text-embedding-ada-002的模型在256或512大小的chunk上效果更好 长篇文章或文档等，较大分块。对社媒评论或帖子，小分块。 尝试从128开始。 向量检索准备：嵌入模型与向量数据库嵌入模型的核心任务是将文本转换为向量形式，存储到专用的向量数据库中。如果选择Word2Vec模型，尽管功能强大，但其生成的词向量是静态的。一旦模型训练完成，每个词的向量就固定不变，这在处理一词多义情况时可能存在问题。而如果选择自注意力机制的模型，如BERT，能够提供动态的词义理解。可以调整不同语义下的词向量。 有些项目为了让模型对特定垂直领域的词汇有更好的理解，会嵌入模型进行微调。但在这里我们并不推荐这种方法，一方面其对训练数据的质量有较高要求，另一方面也需要较多的人力物力投入，且效果未必理想，最终得不偿失。在这种情况下，对于具体应该如何选择嵌入模型，可以参考Hugging Face推出的嵌入模型排行榜，这个排行榜提供了多种模型的性能比较，能帮助我们做出更明智的选择。同时，要注意并非所有嵌入模型都支持中文，因此在选择时应查阅模型说明。 关于向量数据库也有一些需要注意的地方。只使用向量搜索可能不够，最好使用一些原元数据以支持关键词查询的方法。 当在向量数据库中存储向量数据时，某些数据库支持将向量与元数据（即非向量化的数据）一同存储。为向量添加元数据标注是一种提高检索效率的有效策略，它在处理搜索结果时发挥着重要作用。 例如，日期就是一种常见的元数据标签。它能够帮助我们根据时间顺序进行筛选。设想一下，如果我们正在开发一款允许用户查询他们电子邮件历史记录的应用程序。在这种情况下，日期最近的电子邮件可能与用户的查询更相关。然而，从嵌入的角度来看，我们无法直接判断这些邮件与用户查询的相似度。通过将每封电子邮件的日期作为元数据附加到其嵌入中，我们可以在检索过程中优先考虑最近日期的邮件，从而提高搜索结果的相关性。 此外，我们还可以添加诸如章节或小节的引用，文本的关键信息、小节标题或关键词等作为元数据。这些元数据不仅有助于改进知识检索的准确性，还能为最终用户提供更加丰富和精确的搜索体验。 查询检索在RAG系统中，用户的查询问题被转化为向量，然后在向量数据库中进行匹配。不难想象，查询的措辞会直接影响搜索结果。如果搜索结果不理想，可以尝试以下几种方法对问题进行重写，以提升召回效果： 结合历史对话的重新表述：在向量空间中，对人类来说看似相同的两个问题其向量大小并不一定很相似。我们可以直接利用LLM 重新表述问题来进行尝试。此外，在进行多轮对话时，用户的提问中的某个词可能会指代上文中的部分信息，因此可以将历史信息和用户提问一并交给LLM重新表述。 假设文档嵌入（HyDE）：HyDE的核心思想是接收用户提问后，先让LLM在没有外部知识的情况下生成一个假设性的回复。然后，将这个假设性回复和原始查询一起用于向量检索。假设回复可能包含虚假信息，但蕴含着LLM认为相关的信息和文档模式，有助于在知识库中寻找类似的文档。 退后提示（Step Back Prompting）：如果果原始查询太复杂或返回的信息太广泛，我们可以选择生成一个抽象层次更高的“退后”问题，与原始问题一起用于检索，以增加返回结果的数量。例如，原问题是“桌子君在特定时期去了哪所学校”，而退后问题可能是关于他的“教育历史”。这种更高层次的问题可能更容易找到答案。 多查询检索/多路召回（Multi Query Retrieval）：使用LLM生成多个搜索查询，特别适用于一个问题可能需要依赖多个子问题的情况。 通过这些方法，RAG系统能够更精准地处理和响应复杂的用户查询，从而提升整体的搜索效率和准确性。 终于我们把查询问题准备好了，可以进入向量数据库进行检索。在具体的检索过程中，我们可以根据向量数据库的特定设置来优化一些检索参数，以下是一些常见的可设定参数： 稀疏和稠密搜索权重：稠密搜索即通过向量进行搜索。然而，在某些场景下可能存在限制，此时可以尝试使用原始字符串进行关键字匹配的稀疏搜索。一种有效的稀疏搜索算法是最佳匹配25（BM25），它基于统计输入短语中的单词频率，频繁出现的单词得分较低，而稀有的词被视为关键词，得分会较高。我们可以结合稀疏和稠密搜索得出最终结果。向量数据库通常允许设定两者对最终结果评分的权重比例，如0.6表示40%的得分来自稀疏搜索，60%来自稠密搜索。 结果数量（topK）：检索结果的数量是另一个关键因素。足够的检索结果可以确保系统覆盖到用户查询的各个方面。在回答多方面或复杂问题时，更多的结果提供了丰富的语境，有助于RAG系统更好地理解问题的上下文和隐含细节。但需注意，结果数量过多可能导致信息过载，降低回答准确性并增加系统的时间和资源成 相似度度量方法：计算两个向量相似度的方法也是一个可选参数。这包括使用欧式距离和Jaccard距离计算两个向量的差异，以及利用余弦相似度衡量夹角的相似性。通常，余弦相似度更受青睐，因为它不受向量长度的影响，只反映方向上的相似度。这使得模型能够忽略文本长度差异，专注于内容的语义相似性。需要注意的是，并非所有嵌入模型都支持所有度量方法，具体可参考所用嵌入模型的说明。 生成回答终于我们来到最后一步——LLM生成回答。LLM是生成响应的核心组件。与嵌入模型类似，可以根据自己的需求选择LLM，例如开放模型与专有模型、推理成本、上下文长度等。此外，可以使用一些LLM开发框架来搭建RAG系统，比如，LlamaIndex或LangChain。这两个框架都拥有比较好用的debugging工具，可以让我们定义回调函数，查看使用了哪些上下文，检查检索结果来自哪个文档等等。 如何评价RAG项目效果？针对检索环节评估： MRR平均倒数排名（Mean Reciprocal Rank）：用于衡量搜索引擎，推荐系统等根据查询返回多个结果的相关性。结果列表中，第i个结果匹配，分数为$\\dfrac{1}{i}$ Hits Rate命中率 NDCG 针对生成环节评估： 非量化：完整性、正确性、相关性（主观打分） 量化：Rouge-L方法需要准备人工的标准摘要集，将生成内容与标准摘要集对比，计算最长公共子序列来衡量生成文本和参考文本的相似性。计算精确率与召回率，最终得到F1-score作为Rouge-L最终分数。 由于Rouge-L注重最长公共子序列，这意味它比Rouge-1或Rouge-2更能衡量文本生成的结构和顺序是否与参考文本接近。因此，它在衡量文段的连贯性和句子顺序上具有优势。 RAG项目常见的优化策略（总结）RAG项目各个环节均有着极大的优化空间，下面我们穿插12个具体的优化策略来一次讲解。下面介绍的方法在AI开发框架langchain和LLamaindex中有具体实现。 1、知识文档准备阶段：（1）数据清洗（2）分块处理 2、嵌入模型：（1）优选嵌入模型 3、向量数据库：（1）元数据（支持关键词查询） 4、查询索引阶段：（1）多级索引（2）索引/查询算法（3）查询转换（4）检索参数（5）高级检索策略（6）重排模型 5、生成回答阶段：（1）提示词（2）大语言模型","link":"/2025/12/17/RAG/"},{"title":"LLM的前世今生","text":"时代的发展速度在变的越来越快。1975年，半导体芯片领域的著名论断————摩尔定律被提出（集成电路上可容纳的晶体管数目，每隔约两年便会增加一倍）。半导体行业大致按照摩尔定律发展了半个多世纪，带来了电脑，互联网，手机等事物。如今，从2017年开始到2025年即将结束，从NLP（自然语言处理）到LLM（通用大语言模型），深度学习技术的发展，并行计算GPU的进步，智能时代还会需要半个世纪的时间才能到达吗？ 不是，身处技术爆炸的时刻，时代发展速度只会越来越快，直到遇到人本困境。本文旨在介绍LLM大语言模型技术是如何从曾经的NLP任务，到通用预训练模型（Pre-Trained Model），再到未来展望的自学习模型（或者是AI Agent）。 自然语言处理：NLP（Natural Language Process）任务很好理解，就是试图让机器掌握人类使用的自然语言，诸如语音识别，外语翻译，舆情监控等任务都是其中的一部分。NLP有2个核心的任务：分别是【自然语言理解—NL Understand】和【自然语言生成—NL Generate】。 NLU是所有支持机器理解文本内容的方法模型或任务的总称，即能够进行常见的文本分类、序列标注、信息抽取等任务。简单来说，是指自然语言理解的运用希望机器人能够像人一样，具备正常的语言理解能力。例如，意图识别和实体提取的关键技能。 NLG （自然语言生成）是NLP的另一项核心任务，主要目的是降低人类和机器之间的沟通鸿沟，将非语言格式的数据转换成人类可以理解的语言格式。","link":"/2025/12/14/Summary/"},{"title":"手写Transformer-Decoder","text":"今年是2025年，参加组会的你一定会遇到某位师兄或师姐说，上周读了《Transformer is All You Need》论文作汇报。然后就会展示出这张经典的图片并说，大语言模型由Encoder和Decoder组成，巴拉巴拉。你的导师似懂非懂点了点头，你也点了点头，原来LLM就是这样，开始玩起了手机。 事实上Transformer结构确实主要由Encoder、Decoder 两个部分组成，两个部分分别具有不一样的结构和输入输出。 但针对 Encoder、Decoder的特点，面对不同的人物和微调需求，出现了不同的、对 Transformer 进行优化的思路。例如，Google 仅选择了 Encoder 层，通过将 Encoder 层进行堆叠，再提出不同的预训练任务-掩码语言模型（Masked Language Model，MLM），打造了一统自然语言理解（Natural Language Understanding，NLU）任务的代表模型——BERT。而 OpenAI 则选择了 Decoder 层，使用原有的语言模型（Language Model，LM）任务，通过不断增加模型参数和预训练语料，打造了在 NLG（Natural Language Generation，自然语言生成）任务上优势明显的 GPT 系列模型，也是现今大火的LLM的基座模型。当然，还有一种思路是同时保留 Encoder 与 Decoder，打造预训练的 Transformer 模型，例如由 Google 发布的 T5模型。 总的来说，短短十年不到的时间，就按顺序出现了Encoder-Only、Encoder-Decoder、Decoder-Only三种主流预训练模型。 具体来说: BERT 采用了 Encoder-Only 结构，只包含编码器部分；而 GPT 采用了 Decoder-Only 结构，只包含解码器部分。T5 则采用了 Encoder-Decoder 结构， 其中最火热的当属业界主流技术Decoder-Only，Decoder-Only作为GPT的模型，真正让LLM为世人所知。这种结构只需要接受序列数据，输出序列数据，就可以完成翻译，问答对话等多任务。其本质是自回归模型。 以下内容大量参考chaofa用代码打点酱油的博客，模型技术以当前（2025.12）最好的开源模型——Llama开源文档为准 面试过程中让写 transformers Decoder 一定要沟通清楚是写一个 CausalLM decoder 还是原版的，原版的比较复杂，一般也不会让写。这里的 Decoder 一般指的是 CausalLM，具体变化是少了 encoder 部分的输入，所以也就没有了 encoder and decoder cross attention。因为重点希望写 CausalLM，所以没有 Cross attention 和 也省略了 token embedding 这一步。 知识点这里有很多细节可以多说 关于归一化方法的说明： 按照归一化方法来分，主要分为LayerNorm，BatchNorm，RMSNorm以及DeepNorm。如果按照归一化位置来分类，包括 postNorm 和 preNorm，但具体来说用postNorm比较多。 BatchNorm对数据的一定维度在batch数据中归一化，一般应用于图像。这种方法很难适用于序列数据，对于序列数据而言，在batch维度做归一意义不大，而且一个batch内的序列长度不同。LayerNorm是针对序列数据提出的一种归一化方法，主要在layer维度进行归一化，即对整个序列进行归一化。layerNorm会计算一个layer的所有activation的均值和方差，利用均值和方差进行归一化。RMSNorm的提出是为了提升layerNorm的训练速度提出的。RMSNorm也是一种layerNorm，只是归一化的方法不同。相比layerNorm中利用均值和方差进行归一化，RMSNorm 利用均方根进行归一化。 $$LayerNorm: \\dfrac{x-Ex}{\\sqrt{Dx+\\epsilon}}*\\gamma + \\beta$$ transformers decoder 的流程是：input -&gt; self-attention -&gt; cross-attention -&gt; FFN causalLM decoder 的流程是 input -&gt; self-attention -&gt; FFN其他 [self-attention, FFN] 是一个 block，一般会有很多的 block FFN 矩阵有两次变化，一次升维度，一次降维度。其中 LLaMA 对于 GPT 的改进还有把 GeLU 变成了 SwishGLU，多了一个矩阵。所以一般升维会从 4h -&gt; 4h * 2 / 3 FFN是常见的前馈神经网络，主要经历一个升维再降维的过程（生成器基本都这样），为什么常见都是升4维呢？从工程角度来说，这是权衡训练成本和实际效果后的选择。从理论角度来说，常见的激活函数都会导致近50%的信息损失（参考ReLU），两层的结构说明最后会导致只有50% * 50% = 25%的信息保留，为了控制前后信息量不大变化，选择升4维。 原版的 transformers 用 post-norm, 后面 gpt2, llama 系列用的是 pre-norm。其中 llama 系列一般用 RMSNorm 代替 GPT and transformers decoder 中的 LayerNorm。 具体来说用postNorm比较多，具体的原因可以看之前这一篇 为什么大模型结构设计中往往使用postNorm而不用preNorm？ Decoder部分的代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108import mathimport torchimport torch.nn as nnimport warningswarnings.filterwarnings(action=&quot;ignore&quot;)# 写一个Blockclass SimpleDecoder(nn.Module): def __init__(self, hidden_dim, n_heads, dropout = 0.1): super().__init__() self.n_heads = n_heads self.head_dim = hidden_dim // n_heads self.dropout = dropout # 这里按照 transformers 中的 decoder 来写，用 post_norm 的方式实现，主要有 残差链接 # eps 是为了防止溢出；其中 llama 系列的模型一般用的是 RMSnorm 以及 pre-norm（为了稳定性） # RMSnorm 没有一个 recenter 的操作，而 layernorm 是让模型重新变成 均值为 0，方差为 1 # RMS 使用 w平方根均值进行归一化 $\\sqrt{\\frac{1}{n} \\sum_{1}^{n}{a_i^2} }$ self.layernorm_att = nn.LayerNorm(hidden_dim, eps =0.00001) self.q_proj = nn.Linear(hidden_dim, hidden_dim) self.k_proj = nn.Linear(hidden_dim, hidden_dim) self.v_proj = nn.Linear(hidden_dim, hidden_dim) self.o_proj = nn.Linear(hidden_dim, hidden_dim) self.drop_att = nn.Dropout(self.dropout) #准备FFN前馈神经网络 self.up_proj = nn.Linear(hidden_dim, hidden_dim * 4) self.down_proj = nn.Linear(hidden_dim * 4, hidden_dim) self.layernorm_ffn = nn.LayerNorm(hidden_dim, eps = 0.00001) self.act_fn = nn.ReLU() self.drop_ffn = nn.Dropout(self.dropout) def attention_output(self, Q, K , V, attention_mask = None): #计算两者相关性 K = K.transpose(2,3) # [batch_size, n_heads, head_dim, seq_len] # 还没有softmax的att_weight att_weight = (Q @ K) / math.sqrt(self.head_dim) #attention mask进行依次调整：变成causal_attention if attention_mask is not None: #变成下三角矩阵 attention_mask = attention_mask.tril() #提取下三角部分单向注意力 att_weight = att_weight.masked_fill(attention_mask == 0, float(&quot;-1e20&quot;)) #对0部分参与一个极小值，在softmax后为0 else: #人工构造下三角Attention mask attention_mask = torch.ones_like(att_weight).tril() att_weight = att_weight.masked_fill(attention_mask == 0, float(&quot;-1e20&quot;)) att_weight = torch.softmax(att_weight, dim = -1) # print(att_weight) # Dropout论文最早提出的时候被拒稿，认为只有工程而无理论。再看看今天Dropout无处不在，果然是“入关后自有大儒为我辩证” att_weight = self.drop_att(att_weight) mid_output = att_weight @ V # shape [batch_size, n_heads, seq_len, head_dim] #contiguous()函数在PyTorch中用于确保张量（tensor）在内存中的数据是连续存储的。当执行transpose()、permute()等改变维度顺序的操作后，张量可能会变得“非连续”（不按行优先或列优先存储）。contiguous()的作用是返回一个新的、内存连续的张量副本，这样view()（重塑形状）等需要连续内存的操作才能正确执行，提升计算效率。 mid_output = mid_output.transpose(1,2).contiguous() batch_size, seq_len, _, _ = mid_output.size() mid_output = mid_output.view(batch_size, seq_len, -1) # O矩阵只是权重矩阵，多头注意力机制中，对拼接后的矩阵处理 output = self.o_proj(mid_output) return output def attention_block(self, X, attention_mask = None): batch_size, seq_len, _ = X.size() Q = self.q_proj(X).view(batch_size, seq_len, self.n_heads, -1).transpose(1,2) K = self.k_proj(X).view(batch_size, seq_len, self.n_heads, -1).transpose(1,2) V = self.v_proj(X).view(batch_size, seq_len, self.n_heads, -1).transpose(1,2) output = self.attention_output(Q,K,V,attention_mask = attention_mask) return self.layernorm_att(X + output) def ffn_block(self, X): # act_fn = ReLU up = self.act_fn(self.up_proj(X)) down = self.down_proj(up) #dropout down = self.drop_ffn(down) # norm return self.layernorm_ffn(X + down) def forward(self, X, attention_mask = None): # X 一般假设是已经经过embedding的输入， （batch_size, seq_len, hidden_dim） # attention_mask 一般指的是 tokenizer 后返回的 mask 结果，表示哪些样本需要忽略 # shape 一般是： (batch_size, n_heads, seq_len) att_output = self.attention_block(X, attention_mask = attention_mask) ffn_output = self.ffn_block(att_output) return ffn_output# 测试x = torch.rand(3, 4, 64) # [batch_size, seq_len, hidden_dim]net = SimpleDecoder(64, 8) # hidden_dim = 64,n_heads = 8mask = ( torch.tensor([[1, 1, 1, 1], [1, 1, 0, 0], [1, 1, 1, 0]]) .unsqueeze(1) .unsqueeze(2) .repeat(1, 8, 4, 1))net(x, mask).shape 关于推理过程如果你认真看了上面的代码部分，不知道你会不会产生这样的疑问：大语言模型不是根据当前的字预测下一个字是什么吗？请问一下推理时prefill阶段为什么需要计算所有token的q。预测next token的话不是只需要计算上一个token的q就好了么？因此只需要算prefill最后一个token的q就行了？ 这是一个非常非常非常好的问题👍 核心原因是 Decoder 中有很多层，假设只有一层，你的思考是没有问题。但是 decoder 有很多 block，上一层（假设第2层）的输入是下一层的输出，下一层（第一层）的输出依赖于 qkv 的计算，那么计算第二层的 q k v的时候，如果不知道第一层的 q k v（或者output），那么第二层的 k v 就无法计算出来了。因为 prefill 之前没有缓存好每一层的 k v 是什么。 那么换句话说，如果能缓存好每一层的K，V矩阵，推理速度将会大大增加！ 唯一需要考虑的问题是：如何高效的实现KV矩阵缓存。我们将来到KV Cache ——百倍推理加速技术","link":"/2025/12/11/TransformerDecoder/"},{"title":"一些有意思的算法题","text":"不想刷LeetCode啊！！！ 言归正传，写代码还是很有意思的事情。我大学读的是数学专业，一开始只觉得写代码是计算机专业该干的事，后来惊奇地发现，有些算法莫名与那些恐怖的数学知识相关，而这些算法都是为了解决一个具体的有趣问题。这里整理一些LeetCode中有趣的题目，更多还是结合数学的内容。 Q1: 爬楼梯这个问题我应该是最早在电影《少年班》里看到的，那个神童摇着龟壳，根据掉落铜钱来回答的场景太神棍，印象很深。 假设你正在爬楼梯。需要 n 阶你才能到达楼顶。每次你可以爬 1 或 2 个台阶。你有多少种不同的方法可以爬到楼顶呢？比如2级台阶有两种方法：1+1、2；3级台阶有三种方法：1+1+1、1+2、2+1； 从递归的角度容易考虑到，记上$n$级台阶的方法数量为$f(n)$，那么前一步一定在$n-1$级台阶或者$n-2$级台阶，有$f(n)=f(n-1)+f(n-2)$等式成立。这就是斐波那契数列的通项$1,2,3,5,8,13,21….$，自然很容易的写出这样的算法。 12345678910def climbStairs(self, n: int) -&gt; int: if n &lt;= 2: return n # Fibonacci-like sequence first, second = 1, 2 for i in range(3, n + 1): first, second = second, first + second return second 事实上对于$f(n)=f(n-1)+f(n-2)$的齐次线性递推式，除了用高中在数列中用到的特征方程方法来求出斐波那契数列的通项公式外，还可以将齐次线性递推式写为矩阵，使用矩阵快速幂方法来实现。后者的优势在于，对于某些特殊的非齐次线性递推式，也可以处理。 Q2: 只出现一次的数 给你一个 非空 整数数组 nums ，除了某个元素只出现一次以外，其余每个元素均出现两次。找出那个只出现了一次的元素。你必须设计并实现线性时间复杂度的算法来解决此问题，且该算法只使用常量额外空间。 这道题难度easy，但是其解法中的位运算方法实在太优雅（依赖于其余数只出现偶数次），让我想起很多年前见到过一个位运算求N皇后问题的牛逼解法。这正好借这个机会展示二进制的魅力。 1234567891011def singleNumber(self, nums: List[int]) -&gt; int: &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; n = len(nums) index = 0 for i in range(n): # 异或有结合律和交换律 index ^= nums[i] return index 位运算的魅力： N &amp; (N-1)","link":"/2025/12/11/%E4%B8%80%E4%BA%9B%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E7%AE%97%E6%B3%95%E9%A2%98/"}],"tags":[{"name":"大语言模型","slug":"大语言模型","link":"tagsd:\\Blogs\\source\\img\\wechat.jpg/大语言模型/"},{"name":"算法","slug":"算法","link":"tagsd:\\Blogs\\source\\img\\wechat.jpg/算法/"}],"categories":[{"name":"LLM学习系列","slug":"LLM学习系列","link":"/categories/LLM%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97/"}],"pages":[]}