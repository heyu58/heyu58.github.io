{"posts":[{"title":"当代孔乙己-Attention架构的四种写法","text":"LLM（Large Language Model）发展方兴未艾，从ChatGPT横空出世横扫英文互联网，到DeepSeek惊讶世人宣告低成本LLM可行性，2025年短短一年便造就了人工智能领域如此震撼的估值繁荣（or bubble），不得不让人感慨《三体》中“技术爆炸”一词提出的先见性。 饭总是要一口口吃，想从事LLM相关领域，自然要从《Attention is All You Need》，这篇不止在科学界有着划时代意义，更在传播学上展示命名的魅力的论文开始阅读。在AI相关的面试中，经常会要求对Transformer架构有足够的理解，通常通过手写self-attention机制来验证。这里给出了Attention机制从简到繁的四种写法 什么是$Self-Attention$？Attention机制通常在已实现Embedding+位置编码后进行。引入Attention机制主要是由于Embedding和位置编码依然不够表示所有的信息。 举个例子 “你不要打人” 和 “你快去打水” 中 “打” 字含义截然不同，但仅靠Embedding和位置编码两者并不能区别，这就需要结合上下文来增加更多信息，Attention就是这种结合的方式。 $$ SelfAttention(X)=softmax(\\frac{Q\\cdot K}{\\sqrt{d}})\\cdot V $$ $Q = K = V = W \\cdot X$，其中Q，K，V对应不同的权重矩阵W matmul和@符号是一样的作用 为什么要除以$\\sqrt{d}$：一是防止梯度消失（参考Kaiming Initialization参数初始化）；二是为了让QK的内积分布保持和输入一样 爱因斯坦方程表达式用法：torch.einsum(“bqd,bkd-&gt;bqk”,X,X).shape X.repeat(1,1,3)表示在不同维度进行repeat操作，也可以用tensor.expand操作 12345import mathimport warningsimport torchimport torch.nn as nnwarnings.filterwarnings(action='ignore') 第一种写法：简洁123456789101112131415161718192021222324class SelfAttention_1(nn.Module): # 父类是pytorch中的核心Module def __init__(self, hidden_dim): super(SelfAttention_1,self).__init__()# 调用父类的构造方法，并传递所有参数 self.hidden_dim = hidden_dim self.Q = nn.Linear(hidden_dim, hidden_dim) self.K = nn.Linear(hidden_dim, hidden_dim) self.V = nn.Linear(hidden_dim, hidden_dim) def forward(self,X): # X.shape = [batch_size, seq_len, hidden_dim] # 根据输入X的形状创建Q,K,V Q = self.Q(X) K = self.K(X) V = self.V(X) # K需要改为shape = [batch_size, seq_len, hidden_dim] QK = Q @ K.transpose(-1,-2)# K.transpose(1,2) softmax_QK = torch.softmax( QK / math.sqrt(self.hidden_dim), dim = -1) # 在最后一维上计算softmax output = softmax_QK @ V return output X = torch.rand(3,2,4)net = SelfAttention_1(4)net(X) 第二种写法：效率优化12345678910111213141516171819class SelfAttention_2(nn.Module): def __init__(self,hidden_dim): super().__init__() self.hidden_dim = hidden_dim self.proj = nn.Linear(hidden_dim, hidden_dim*3) #直接将Q，K，V网络合并 self.output_proj = nn.Linear(hidden_dim,hidden_dim) def forward(self,X): # X.shape = [batch_size, seq_len, hidden_dim] QKV = self.proj(X) # [batch_size, seq_len, dim*3] # torch.split(x, chunks_size, dim) Q,K,V = torch.split(QKV , self.hidden_dim , dim =-1) QK = torch.softmax(Q @ K.transpose(-1,-2) / math.sqrt(self.hidden_dim), dim = -1) output = QK @ V return self.output_proj(output) X = torch.rand(3,2,4)net = SelfAttention_2(4)net(X) 第三种写法：加上细节，更完整 Attention计算时有dropout层，设置的位置很有趣 Attention计算时会加入attention_mask，因为样本会进行padding操作 MultiHeadAttention过程中，除QKV三个矩阵外，还有output对应的投影矩阵 12345678910111213141516171819202122232425262728293031323334353637383940414243class SelfAttention_3(nn.Module): def __init__(self, hidden_dim): super().__init__() self.hidden_dim = hidden_dim self.proj = nn.Linear(hidden_dim, hidden_dim*3) self.att_drop = nn.Dropout(0.1) self.output_proj = nn.Linear(hidden_dim, hidden_dim) def forward(self, X,attention_mask = None): # attention_mask = [batch_size, seq_len] # X.shape = [batch_size, seq_len, hidden_dim] QKV = self.proj(X) Q, K, V = torch.split(QKV,self.hidden_dim, dim=-1) QK = Q @ K.transpose(-1,-2) / math.sqrt(self.hidden_dim) if attention_mask is not None: # mask的目的：屏蔽掉不该参与注意力计算的位置（无效padding、未来token屏蔽、无效区域） # masked_fill(condition, value) PyTorch 的方法：将 condition 为 True 的位置替换成 value # value = float(&quot;-1e20&quot;)是因为在softmax后值接近于0 QK = QK.masked_fill(attention_mask == 0, float(&quot;-1e20&quot;)) QK = torch.softmax(QK,dim = -1) # 这里在 BERT中的官方代码也说很奇怪，但是原文中这么用了，所以继承了下来 # （用于 output 后面会更符合直觉？） QK= self.att_drop(QK) output = QK @ V return self.output_proj(output) X = torch.rand(3,4,2)b = torch.tensor( [ [1, 1, 1, 0], [1, 1, 0, 0], [1, 0, 0, 0], ])print(f&quot;mask的初始形状{b.shape}&quot;)# unsqueeze(dim)表示在指定位置增加维度mask = b.unsqueeze(dim=1).repeat(1, 4, 1)print(f&quot;mask：{mask}&quot;)net = SelfAttention_3(2)net(X,mask) 第四种写法：从单头到多头一般在实际上的训练过程中都会使用 Multi Head, 而且其实也仅仅是 每个 Head 做完 Self-Attention 得到结果之后，进行拼接，然后过一个 output 投影矩阵 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class MultiHeadAttention(nn.Module): def __init__(self,hidden_dim, n_head): super().__init__() self.n_head = n_head self.hidden_dim = hidden_dim self.head_dim = hidden_dim // n_head # 注意hidden_dim= head_dim * n_head self.Q = nn.Linear(hidden_dim,hidden_dim) self.K = nn.Linear(hidden_dim,hidden_dim) self.V = nn.Linear(hidden_dim,hidden_dim) self.att_dropout = nn.Dropout(0.1) self.proj = nn.Linear(hidden_dim, hidden_dim) def forward(self,X,attention_mask=None): # attention_mask = [batch_size, seq_len] # X.shape = [batch_size, seq_len, hidden_dim] batch_size, seq_len, _ = X.size() Q = self.Q(X) K = self.Q(X) V = self.Q(X) # 改变Q，K，V的shape为[batch_size,seq_len, n_head, head_dim] Q_state = Q.reshape(batch_size, seq_len, self.n_head, self.head_dim).permute(0,2,1,3) K_state = K.reshape(batch_size, seq_len, self.n_head, self.head_dim).permute(0,2,1,3) V_state = V.reshape(batch_size, seq_len, self.n_head, self.head_dim).permute(0,2,1,3) QK = Q_state @ K_state.transpose(-1,-2) / math.sqrt(self.head_dim) if attention_mask is not None: QK = QK.masked_fill(attention_mask == 0, float(&quot;-1e20&quot;)) # 对最后一个维度softmax QK = torch.softmax(QK, dim = 3) QK = self.att_dropout(QK) print(QK.shape) print(V_state.shape) output = QK @ V_state print(output.shape) output = output.reshape(batch_size,seq_len, -1) return self.proj(output) attention_mask = ( torch.tensor( [ [0, 1], [0, 0], [1, 0], ] ) .unsqueeze(1) .unsqueeze(2) .expand(3, 8, 2, 2))x = torch.rand(3, 2, 128)net = MultiHeadAttention(128, 8)net(x, attention_mask)","link":"/2025/12/10/Attention/"},{"title":"我是谁","text":"很久以前看B站《我的音乐你听吗?》，算是这么些年罕见的高质量原创音乐舞台，也记住了很多优秀作品。印象最深的是《我是谁》这首歌。那时还是无忧无虑的大二时光，没有意识到无论是自己的人生还是国家都在一个关键点。但我还是如歌词里唱着一样，被张佩服用石头砸了一下头，完事醒了之后，忘了我是谁。 时代变化如此之快，身处其中的时候总是感觉不到，等停下来才后知后觉，怎么没买英伟达的股票，还有黄金。数据的爆炸已经让人脑无法荷载，这个时代的数据已经成为了“房间里的大象”，愈来愈烈的舆论争吵与人群对立不过是反反复复在说明一件事：所有人都是盲人摸象，只能看到自己眼中的世界，然后通过“触摸大象”来加深这种刻板。 每个人都应该被宣判死刑，看似还活着的人不过是数据意义上的行尸走肉。","link":"/2025/12/06/%E6%88%91%E6%98%AF%E8%B0%81/"},{"title":"小球带动大球-LoRA","text":"大语言模型竞争激烈，众多的基座模型在评分榜上你追我赶，只为实现人工智能的梦想–通用AGI。但是在实际业务场景中，我们并不需要一个全职全能的语言模型。作为客服机器人，不需要对政治历史和军事或者科技有什么了解，只要搞清楚商品就好了；作为问诊机器人，不需要对艺术史或者音乐有什么了解，只要搞的清楚基础病即可。大语言模型在这种场景显得牛刀杀鸡了。 这时候，微调技术应运而生。","link":"/2025/12/10/LoRA/"}],"tags":[{"name":"大语言模型","slug":"大语言模型","link":"/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"name":"生活","slug":"生活","link":"/tags/%E7%94%9F%E6%B4%BB/"}],"categories":[{"name":"LLM学习系列","slug":"LLM学习系列","link":"/categories/LLM%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97/"}],"pages":[]}