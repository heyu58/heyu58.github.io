{"posts":[{"title":"蒸馏技术","text":"最早上手学蒸馏，感觉和生成模型的GAN技术很像，让生成模型使用判别器的判别结果来训练。具体来说，蒸馏是指通过训练小模型去学习大模型的软输出（伪概率），而非仅仅学习硬分类标签。这种技术在保持大模型基本能力的同时，大幅减少参数量，使得高性能模型能部署在移动设备等资源受限硬件环境中","link":"/2025/12/17/Distillation/"},{"title":"当代孔乙己-Attention机制的四种写法","text":"大语言模型（LLM）发展方兴未艾，从ChatGPT横空出世横扫英文互联网，到DeepSeek惊讶世人宣告低成本LLM可行性，2025年短短一年便造就了人工智能领域如此震撼的估值繁荣（or bubble），不得不让人感慨《三体》中 “技术爆炸” 一词提出的先见性。 一切从《Attention is All You Need》开始。这篇论文彻底抛弃了长序列预测的LSTM和RNN模型，在学术界有着划时代意义；更在传播学上展示了一个好的名字到底有多重要。 以下内容大量参考chaofa用代码打点酱油的博客 在AI相关的面试中，经常会要求对Transformer架构有足够的理解，通常通过手写self-attention机制来验证。这里给出了Attention机制从简到繁的四种写法 什么是$Self-Attention$？$Attention$ 机制通常在已实现 $Embedding$ + 位置编码后进行。引入 $Attention$ 机制主要是由于 $Embedding$ 和位置编码依然不够表示所有的信息。 举个例子 “你不要打人” 和 “你快去打水” 中 “打” 字含义截然不同，但仅靠 $Embedding$ 和位置编码两者并不能区别，这就需要结合上下文来增加更多信息，$Attention$ 就是这种结合的方式。 $$SelfAttention(X)=softmax(\\frac{Q\\cdot K}{\\sqrt{d}})\\cdot V$$ $Q = K = V = W_{Q/K/V} \\cdot X$，其中$Q，K，V$分别对应不同（形状也不同）的权重矩阵W matmul和@符号是一样的作用，矩阵乘法 为什么要除以$\\sqrt{d}$：一是防止梯度消失（参考Kaiming Initialization参数初始化）；二是为了让QK的内积分布保持和输入一样 爱因斯坦方程表达式用法：torch.einsum(“bqd,bkd-&gt;bqk”,X,X).shape 为什么使用softmax?注意力机制，本质上是想找与查询向量最相关的隐向量序列，实际上是想找，onehot(argmax)，比如的任务是中视野中找苹果，直接依据苹果的特征查找苹果。但是这个求解是无法计算梯度，所以才用onehot(argmax)的光滑版本softmax，以获得良好的梯度特性。是一种数学上的妥协 X.repeat(1,1,3)表示在不同维度进行repeat操作，也可以用tensor.expand操作 12345import mathimport warningsimport torchimport torch.nn as nnwarnings.filterwarnings(action='ignore') 第一种写法：简洁123456789101112131415161718192021222324class SelfAttention_1(nn.Module): # 父类是pytorch中的核心Module def __init__(self, hidden_dim): super(SelfAttention_1,self).__init__()# 调用父类的构造方法，并传递所有参数 self.hidden_dim = hidden_dim self.Q = nn.Linear(hidden_dim, hidden_dim) self.K = nn.Linear(hidden_dim, hidden_dim) self.V = nn.Linear(hidden_dim, hidden_dim) def forward(self,X): # X.shape = [batch_size, seq_len, hidden_dim] # 根据输入X的形状创建Q,K,V Q = self.Q(X) K = self.K(X) V = self.V(X) # K需要改为shape = [batch_size, seq_len, hidden_dim] QK = Q @ K.transpose(-1,-2)# K.transpose(1,2) softmax_QK = torch.softmax( QK / math.sqrt(self.hidden_dim), dim = -1) # 在最后一维上计算softmax output = softmax_QK @ V return output X = torch.rand(3,2,4)net = SelfAttention_1(4)net(X) 第二种写法：效率优化12345678910111213141516171819class SelfAttention_2(nn.Module): def __init__(self,hidden_dim): super().__init__() self.hidden_dim = hidden_dim self.proj = nn.Linear(hidden_dim, hidden_dim*3) #直接将Q，K，V网络合并 self.output_proj = nn.Linear(hidden_dim,hidden_dim) def forward(self,X): # X.shape = [batch_size, seq_len, hidden_dim] QKV = self.proj(X) # [batch_size, seq_len, dim*3] # torch.split(x, chunks_size, dim) Q,K,V = torch.split(QKV , self.hidden_dim , dim =-1) QK = torch.softmax(Q @ K.transpose(-1,-2) / math.sqrt(self.hidden_dim), dim = -1) output = QK @ V return self.output_proj(output) X = torch.rand(3,2,4)net = SelfAttention_2(4)net(X) 第三种写法：加上细节attention_mask，更完整 Attention计算时有dropout层，设置的位置很有趣 Attention计算时会加入attention_mask，因为样本会进行padding操作（不同长度的序列输入，需要相同的维度Attention_map，这要求在QKV以外进行mask处理） MultiHeadAttention过程中，除QKV三个矩阵外，还有output对应的投影矩阵 12345678910111213141516171819202122232425262728293031323334353637383940414243class SelfAttention_3(nn.Module): def __init__(self, hidden_dim): super().__init__() self.hidden_dim = hidden_dim self.proj = nn.Linear(hidden_dim, hidden_dim*3) self.att_drop = nn.Dropout(0.1) self.output_proj = nn.Linear(hidden_dim, hidden_dim) def forward(self, X,attention_mask = None): # attention_mask = [batch_size, seq_len] # X.shape = [batch_size, seq_len, hidden_dim] QKV = self.proj(X) Q, K, V = torch.split(QKV,self.hidden_dim, dim=-1) QK = Q @ K.transpose(-1,-2) / math.sqrt(self.hidden_dim) if attention_mask is not None: # mask的目的：屏蔽掉不该参与注意力计算的位置（无效padding、未来token屏蔽、无效区域） # masked_fill(condition, value) PyTorch 的方法：将 condition 为 True 的位置替换成 value # value = float(&quot;-1e20&quot;)是因为在softmax后值接近于0 QK = QK.masked_fill(attention_mask == 0, float(&quot;-1e20&quot;)) QK = torch.softmax(QK,dim = -1) # 这里在 BERT中的官方代码也说很奇怪，但是原文中这么用了，所以继承了下来 # （用于 output 后面会更符合直觉？） QK= self.att_drop(QK) output = QK @ V return self.output_proj(output) X = torch.rand(3,4,2)b = torch.tensor( [ [1, 1, 1, 0], [1, 1, 0, 0], [1, 0, 0, 0], ])print(f&quot;mask的初始形状{b.shape}&quot;)# unsqueeze(dim)表示在指定位置增加维度mask = b.unsqueeze(dim=1).repeat(1, 4, 1)print(f&quot;mask：{mask}&quot;)net = SelfAttention_3(2)net(X,mask) 第四种写法：从单头到多头一般在实际上的训练过程中都会使用 Multi Head, 多头注意力的实现核心是torch.permute()函数（支持高维转置操作）, 而且其实也仅仅是每个 Head 做完 Self-Attention 得到结果之后，进行拼接，然后过一个 output 投影矩阵。本质上是一种集成学习，与券商研报里的因子打分相比还是更高明一点。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667class MultiHeadAttention(nn.Module): def __init__(self,hidden_dim, n_head): super().__init__() self.n_head = n_head self.hidden_dim = hidden_dim self.head_dim = hidden_dim // n_head # 注意hidden_dim= head_dim * n_head # 将最后一个维度进行拆分，一个诸葛亮拆成n_head个臭皮匠 self.Q = nn.Linear(hidden_dim,hidden_dim) self.K = nn.Linear(hidden_dim,hidden_dim) self.V = nn.Linear(hidden_dim,hidden_dim) self.att_dropout = nn.Dropout(0.1) self.proj = nn.Linear(hidden_dim, hidden_dim) def forward(self,X,attention_mask=None): # attention_mask = [batch_size, seq_len] # X.shape = [batch_size, seq_len, hidden_dim] batch_size, seq_len, _ = X.size() Q = self.Q(X) K = self.Q(X) V = self.Q(X) # 下面reshape改变Q，K，V的shape为[batch_size,seq_len, n_head, head_dim] # 多头注意力训练建立在认知：训练多个小模型难度低于训练一个大模型 Q_state = Q.reshape(batch_size, seq_len, self.n_head, self.head_dim).permute(0,2,1,3) K_state = K.reshape(batch_size, seq_len, self.n_head, self.head_dim).permute(0,2,1,3) V_state = V.reshape(batch_size, seq_len, self.n_head, self.head_dim).permute(0,2,1,3) QK = Q_state @ K_state.transpose(-1,-2) / math.sqrt(self.head_dim) if attention_mask is not None: QK = QK.masked_fill(attention_mask == 0, float(&quot;-1e20&quot;)) # 或者有些实现是 attention_mask == 0 时 mask，看具体代码 # 对最后一个维度head_dim进行softmax平滑化 QK = torch.softmax(QK, dim = 3) QK = self.att_dropout(QK) print(QK.shape) print(V_state.shape) output = QK @ V_state print(output.shape) output = output.reshape(batch_size,seq_len, -1) return self.proj(output) #在 PyTorch 的 MultiHeadAttention（或自己实现的）里，attention_mask 的形状通常是：(batch_size, num_heads, query_len, key_len) # 所以我们要把这个 (batch_size, seq_len) 的 mask 扩展成 (batch_size, num_heads, seq_len, seq_len)，并且要广播到每个 head 上#最终得到的 attention_mask[batch, head, i, j] 表示：#第 batch 个样本的第 head 个头中，第 i 个 query 能不能看到第 j 个 keyattention_mask = ( torch.tensor( [ [0, 1], [0, 0], [1, 0], ] )# shape: (3, 2) 原始mask，0=可见，1=不可见（或反过来，取决于实现） .unsqueeze(1)# → (3, 1, 2) .unsqueeze(2)# → (3, 1, 1, 2) .expand(3, 8, 2, 2)# → (3, 8, 2, 2))x = torch.rand(3, 2, 128)# 八头注意力net = MultiHeadAttention(128, 8)net(x, attention_mask) 这里再次解释一下，为什么现在现在的代码实现都是 q k v 的投影矩阵都是分开写的，这是因为现在的模型很大，本身可能会做 张量并行，流水线并行等方式，所以分开写问题也不大（分开写很清晰），可能是加速效果并不明显。","link":"/2025/12/10/Attention/"},{"title":"LLM中的LoRA实现","text":"AI时代，诸如OpenAI、Google、阿里巴巴、Meta等大公司可以依靠恐怖的计算资源来训练基座大模型，但对于对资源有限的企业或课题组来说，如何高效、快速对模型进行领域或任务的微调，以低成本地使用 LLM 完成目标任务，是非常重要的。 涌现能力区分 LLM （Large Language Model）与传统 PLM（Pre-trained Language Model） 最显著的特征即是 LLM 具备涌现能力 。涌现能力是指同样的模型架构与预训练任务下，某些能力在小型模型中不明显，但在大型模型中特别突出。可以类比到物理学中的相变现象，涌现能力的显现就像是模型性能随着规模增大而迅速提升，超过了随机水平，也就是我们常说的量变引起了质变。 一般来说，LLM 指包含数百亿（或更多）参数的语言模型，它们往往在数 T token 语料上通过多卡分布式集群进行预训练，具备远超出传统预训练模型的文本理解与生成能力。不过，随着 LLM 研究的不断深入，多种参数尺寸的 LLM 逐渐丰富，广义的 LLM 一般覆盖了从十亿参数（如 Qwen-1.5B）到千亿参数（如 Grok-314B）的所有大型语言模型。只要模型展现出涌现能力，即在一系列复杂任务上表现出远超传统预训练模型（如 BERT、T5）的能力与潜力，都可以称之为 LLM。 在这种恐怖的参数规模下，调教大语言模型（更新模型参数）所需的电力资源，显卡资源都是恐怖的。 首先需要回顾一下，对 LLM 进行预训练和进行有监督微调的核心差异在于什么。 目前成型的 LLM 一般通过 Pretrain-SFT-RLHF 三阶段来训练，Pretrain 阶段，会对海量无监督文本进行自监督建模，来学习文本语义规则和文本中的世界知识；SFT 阶段，一般通过对 Pretrain 好的模型进行指令微调，即训练模型根据用户指令完成对应任务，从而使模型能够遵循用户指令，根据用户指令进行规划、行动和输出。因此，Pretrain 和 SFT 均使用 CLM （Causal Language Model）建模，其核心差异在于，Pretrain 使用海量无监督文本进行训练，模型直接对文本执行“预测下一个 token”的任务；而 SFT 使用构建成对的指令对数据，模型根据输入的指令，建模后续的输出。反映到具体的训练实现上，Pretrain 会对全部 text 进行 loss 计算，要求模型对整个文本实现建模预测；而 SFT 仅对输出进行 loss 计算，不计算指令部分的 loss。。但是，由于 LLM 参数量大，训练数据多，通过上述方式对模型进行训练（主要指 SFT 及 RLHF）需要调整模型全部参数，资源压力非常大。 这时候，微调技术应运而生。 微调（Fine Tuning）对所有参数进行训练调整的全量微调太过昂贵，需要大量的GPU显存，个人的显卡上很难实现，因此各种参数高效（Parameter-Efficient）的方法层出不穷，最受大家欢迎的就是 LoRA方法《LoRA: Low-Rank Adaptation of Large Language Models》 LoRA 有很多的优点，节约显存，训练快，效果损失较小（相对于全参数微调），推理的时候不增加耗时，可以做一个插入式组件使用。缺点当然也有，那就是还是会有一些效果的损失。 减少显存占用的主要原因是训练参数变小了（比如只对qkv层做LoRA） 与其他高效微调方法相比，LoRA 存在以下优势： 以针对不同的下游任务构建小型 LoRA 模块，从而在共享预训练模型参数基础上有效地切换下游任务。 LoRA 使用自适应优化器（Adaptive Optimizer），不需要计算梯度或维护大多数参数的优化器状态，训练更有效、硬件门槛更低。 LoRA 使用简单的线性设计，在部署时将可训练矩阵与冻结权重合并，不存在推理延迟。 LoRA 与其他方法正交，可以组合。 因此，LoRA 成为目前高效微调 LLM 的主流方法，尤其是对于资源受限、有监督训练数据受限的情况下，LoRA 微调往往会成为 LLM 微调的首选方法。 LoRA核心原理LoRA的核心原理非常简单，任意一个矩阵$W_0$都可以做低秩分解，把一个很大的矩阵拆分成两个小矩阵$A,B$，在训练过程汇总不去改变$W_0$参数，而是去改变$AB$。具体可以表示为$$W_{new} = W_0 + AB$$最终在训练计算的时候是$$h=(W_0+AB)x$$但是一般来说，$AB$会进行一定的缩放，使用$\\dfrac{\\alpha}{r}$作为缩放因子，所以最终会写成$$h=(W_0+\\dfrac{\\alpha}{r}AB)x \\ s.t.\\ W_0\\in\\mathbb{R}^{n\\times m},A\\in\\mathbb{R}^{n\\times r},B\\in\\mathbb{R}^{r\\times m}$$其中$r &lt;&lt; n,m$，甚至可以设置为1 为什么只优化AB两个矩阵就可以了呢？这里面的假设是what？ W不是满秩的，里面有大量参数是冗余的，那么其实可以用更接近满秩的矩阵AB来替代 LoRA代码Pytorch实现12345678910111213141516171819202122232425262728import torchimport torch.nn as nnimport torch.nn.functional as Fimport mathclass LinearLoRALayer(nn.Module): def __init__(self, in_features,out_features,merge=False,rank=8,lora_alpha=16,dropout=16): super().__init__() self.in_features = in_features self.out_features = out_features self.merge = merge self.rank = rank #linear weight的shape是（out_features, in_features）正确的做法是xW^T self.linear = nn.Linear(in_features, out_features) if rank &gt; 0: #这里标记lora_a和lora_b是可训练参数 self.lora_a = nn.Parameter( torch.zeros(out_features, rank) ) #lora_a 需要初始化为高斯分布 # nn.init.kaiming_normal_(self.lora_a, a=0.01) self.lora","link":"/2025/12/10/LoRA/"},{"title":"RAG检索增强生成模型","text":"RAG技术发展的源于生成模型无法避免的幻觉问题。具体来说纯生成模型，尤其是基于大语言模型的生成模型，虽然在生成连贯、流畅的文本方面表现出色，但它们有时会产生与事实不符的信息。这种现象被称为幻觉（hallucination）。幻觉的产生通常是由于模型在训练过程中学习到的信息是不完整的，或者模型在尝试生成看似合理但实际上并非基于真实信息的内容时过度自信。最新的研究表明，在token超过一定数量后（100k），模型的胡说八道几乎无法避免。 什么是RAG？RAG全称为检索增强生成，(Retrieval Augmented Generation，RAG)。从名字来看，这种技术结合了检索 (Retrieval) 和**生成 (Generation)**两个过程，旨在提高机器生成文本的相关性、准确性和多样性。 RAG技术使用外部数据源的信息辅助文本生成，通过访问外部数据库检索得到有关的信息（通常以chunk形式返回所需信息）。把检索得到的信息与用户源问题合并为提示，让大语言模型从包含外部信息的提示中学习知识（in-context learning）并生成正确答案。从而缓解了幻觉问题。 缓解幻觉问题的方法常见的还有：提示词约束，CoT推理链，RLHF反馈等 RAG的核心部件可以参考流程图 向量数据库（Vector Database）：一种专门设计用来高效存储和检索向量数据的数据库系统。数据经常以高维向量的形式存在，比如文本、图片或其他类型的数据经过嵌入模型转换成的向量。这些向量代表了原始数据的特征和语义信息，可以用于各种相似性搜索和数据分析任务。向量数据库通过优化这类向量数据的存储结构和检索算法，提供了一种高效率的方式来处理大规模向量集合。检索增强生成模型的一个关键步骤是从大规模数据集中检索相关信息，这些信息随后用于辅助生成模型产生回答。向量数据库能够快速检索到与查询向量最相似的数据向量，从而大大加快了这一过程，提高了信息检索的效率和准确性。 查询检索（Retriever）：检索器可以从一个大规模的文档集合或知识库中检索出与给定查询最相关的信息，这个过程是通过比较查询的表示（通常是一个向量）和文档集合中每个文档的表示来完成的，通过检索器，检索增强生成系统能够访问到更广泛的、实时更新的信息，从而扩展了模型处理问题时的知识范围。目标是找到最能帮助生成系统回答查询的信息。 重新排序（Re-ranker）：我们可以在许多文本文档中执行语义搜索，相关文档可能有数万到数百亿个。但由于大语言模型对于传递文本量有限制，我们需要对文档质量进行排序，然后返回top-k文档用于下一步检索生成。在重排器中，给定查询和文档对，将输出相似性得分。我们使用这个分数根据与我们的查询的相关性对文档进行重新排序。 生成回答（Generator）在整个检索增强生成系统中，生成器是负责将检索到的信息生成最终文本输出的组件，它利用这些检索到的信息来构建回答或完成特定的文本生成任务。生成器将检索到的多个文档或信息片段综合考虑，融合它们的内容来构建一个连贯、逻辑一致的输出。这个过程涉及到语言生成的各个方面，包括词汇选择、语法结构和内容连贯性等。 因为检索增强生成不仅用于事实推理，还可以被用于生成代码、图像等，因此面对不同下游任务，生成器模型已经衍生出多种变体。Transformer模型经常应用于文本生成任务；VisualGPT常用于从图像生成文本描述；Stable Diffusion主要用于根据文本提示生成图像；Codex则专注于从文本描述生成代码。 外部数据：知识文档的准备文件类型多样，知识源可能有多种形式，如Word，TXT文件，CSV数据表，Excel表格，PDF文件，HTML网页，图片，视频等等（这些内容已经涉及多模态） 所以第一步就是进行文档加载器（PDF提取）或多模态技术（OCR，矢量图检索），将知识源（私有，专业，时效）转化为大语言模型能够理解的文本序列信息。 第二步，数据清洗是必不可少的一步。 基本文本清理：规范格式，去除特殊字符、不相关信息、重复内容、冗余信息 实体解析：专有名词标准化，特别标注 数据增强：使用同义词、专家解释甚至其他语言的翻译来增加语料库的多样性。 用户反馈循环 时间敏感数据 此外，文档可能会过长，所以另一项关键步骤文档切片，英文名chunk。关于文档分块方法的选择，有以下参考： 固定大小分块：nothing to talk。通常会保持块之间的一些重叠，以确保语义的连贯性。 内容分块：根据标点符号或段落分割，或直接用更高级的NLTK或spaCy库提供的句子分割功能。 递归分块：多数情况下推荐方法。具体来说，在Langchain中会先通过段落换行符分割。然后检查这些块的大小，如果不超过一定阈值，则该块被保留。对于大小超过标准的块，使用单换行符再次分割。以此类推，不断根据块大小更新更小的分块规则（如空格，句号）。这种方法显然更加灵活，挑战也在此：需要制定精细规则来决定何时和如何分割文本。 特殊结构分块：主要针对Markdown、Latex、Code等特殊文件。 然而还是要决定chunk的大小，如何选择分块大小呢？ 不同的嵌入模型有最佳的输入大小，如Openai的text-embedding-ada-002的模型在256或512大小的chunk上效果更好 长篇文章或文档等，较大分块。对社媒评论或帖子，小分块。 尝试从128开始。 向量检索准备：嵌入模型与向量数据库嵌入模型的核心任务是将文本转换为向量形式，存储到专用的向量数据库中。如果选择Word2Vec模型，尽管功能强大，但其生成的词向量是静态的。一旦模型训练完成，每个词的向量就固定不变，这在处理一词多义情况时可能存在问题。而如果选择自注意力机制的模型，如BERT，能够提供动态的词义理解。可以调整不同语义下的词向量。 有些项目为了让模型对特定垂直领域的词汇有更好的理解，会嵌入模型进行微调。但在这里我们并不推荐这种方法，一方面其对训练数据的质量有较高要求，另一方面也需要较多的人力物力投入，且效果未必理想，最终得不偿失。在这种情况下，对于具体应该如何选择嵌入模型，可以参考Hugging Face推出的嵌入模型排行榜，这个排行榜提供了多种模型的性能比较，能帮助我们做出更明智的选择。同时，要注意并非所有嵌入模型都支持中文，因此在选择时应查阅模型说明。 关于向量数据库也有一些需要注意的地方。只使用向量搜索可能不够，最好使用一些元数据以支持关键词查询的方法。 当在向量数据库中存储向量数据时，某些数据库支持将向量与元数据（即非向量化的数据）一同存储。为向量添加元数据标注是一种提高检索效率的有效策略，它在处理搜索结果时发挥着重要作用。 例如，日期就是一种常见的元数据标签。它能够帮助我们根据时间顺序进行筛选。设想一下，如果我们正在开发一款允许用户查询他们电子邮件历史记录的应用程序。在这种情况下，日期最近的电子邮件可能与用户的查询更相关。然而，从嵌入的角度来看，我们无法直接判断这些邮件与用户查询的相似度。通过将每封电子邮件的日期作为元数据附加到其嵌入中，我们可以在检索过程中优先考虑最近日期的邮件，从而提高搜索结果的相关性。 此外，我们还可以添加诸如章节或小节的引用，文本的关键信息、小节标题或关键词等作为元数据。这些元数据不仅有助于改进知识检索的准确性，还能为最终用户提供更加丰富和精确的搜索体验。 查询检索在RAG系统中，用户的查询问题被转化为向量，然后在向量数据库中进行匹配。不难想象，查询的措辞会直接影响搜索结果。如果搜索结果不理想，可以尝试以下几种方法对问题进行重写，以提升召回效果： 结合历史对话的重新表述：在向量空间中，对人类来说看似相同的两个问题其向量大小并不一定很相似。我们可以直接利用LLM 重新表述问题来进行尝试。此外，在进行多轮对话时，用户的提问中的某个词可能会指代上文中的部分信息，因此可以将历史信息和用户提问一并交给LLM重新表述。 假设文档嵌入（HyDE）：HyDE的核心思想是接收用户提问后，先让LLM在没有外部知识的情况下生成一个假设性的回复。然后，将这个假设性回复和原始查询一起用于向量检索。假设回复可能包含虚假信息，但蕴含着LLM认为相关的信息和文档模式，有助于在知识库中寻找类似的文档。 退后提示（Step Back Prompting）：如果果原始查询太复杂或返回的信息太广泛，我们可以选择生成一个抽象层次更高的“退后”问题，与原始问题一起用于检索，以增加返回结果的数量。例如，原问题是“桌子君在特定时期去了哪所学校”，而退后问题可能是关于他的“教育历史”。这种更高层次的问题可能更容易找到答案。 多查询检索/多路召回（Multi Query Retrieval）：使用LLM生成多个搜索查询，特别适用于一个问题可能需要依赖多个子问题的情况。 通过这些方法，RAG系统能够更精准地处理和响应复杂的用户查询，从而提升整体的搜索效率和准确性。 终于我们把查询问题准备好了，可以进入向量数据库进行检索。在具体的检索过程中，我们可以根据向量数据库的特定设置来优化一些检索参数，以下是一些常见的可设定参数： 稀疏和稠密搜索权重：稠密搜索即通过向量进行搜索。然而，在某些场景下可能存在限制，此时可以尝试使用原始字符串进行关键字匹配的稀疏搜索。一种有效的稀疏搜索算法是最佳匹配25（BM25），它基于统计输入短语中的单词频率，频繁出现的单词得分较低，而稀有的词被视为关键词，得分会较高。我们可以结合稀疏和稠密搜索得出最终结果。向量数据库通常允许设定两者对最终结果评分的权重比例，如0.6表示40%的得分来自稀疏搜索，60%来自稠密搜索。 结果数量（topK）：检索结果的数量是另一个关键因素。足够的检索结果可以确保系统覆盖到用户查询的各个方面。在回答多方面或复杂问题时，更多的结果提供了丰富的语境，有助于RAG系统更好地理解问题的上下文和隐含细节。但需注意，结果数量过多可能导致信息过载，降低回答准确性并增加系统的时间和资源成 相似度度量方法：计算两个向量相似度的方法也是一个可选参数。这包括使用欧式距离和Jaccard距离计算两个向量的差异，以及利用余弦相似度衡量夹角的相似性。通常，余弦相似度更受青睐，因为它不受向量长度的影响，只反映方向上的相似度。这使得模型能够忽略文本长度差异，专注于内容的语义相似性。需要注意的是，并非所有嵌入模型都支持所有度量方法，具体可参考所用嵌入模型的说明。 在完成语义搜索的优化步骤后，我们能够检索到语义上最相似的文档，但不知你是否注意到一个关键问题：语义最相似是否总代表最相关？答案是不一定。例如，当用户查询“最新上映的科幻电影推荐”时，可能得到的结果是“科幻电影的历史演变”，虽然从语义上这与科幻电影相关，但并未直接回应用户关于最新电影的查询。 重排模型可以帮助我们缓解这个问题，重排模型通过对初始检索结果进行更深入的相关性评估和排序，确保最终展示给用户的结果更加符合其查询意图。这一过程通常由深度学习模型实现，如Cohere的Rerank模型。这些模型会考虑更多的特征，如查询意图、词汇的多重语义、用户的历史行为和上下文信息等。 举个例子，对于查询“最新上映的科幻电影推荐”，在首次检索阶段，系统可能基于关键词返回包括科幻电影的历史文章、科幻小说介绍、最新电影的新闻等结果。然后，在重排阶段，模型会对这些结果进行深入分析，并将最相关、最符合用户查询意图的结果（如最新上映的科幻电影列表、评论或推荐）排在前面，同时将那些关于科幻电影历史或不太相关的内容排在后面。这样，重排模型就能有效提升检索结果的相关性和准确性，更好地满足用户的需求。 在实践中，使用RAG构建系统时都应考虑尝试重排方法，以评估其是否能够提高系统性能。 生成回答终于我们来到最后一步——LLM生成回答。LLM是生成响应的核心组件。与嵌入模型类似，可以根据自己的需求选择LLM，例如开放模型与专有模型、推理成本、上下文长度等。此外，可以使用一些LLM开发框架来搭建RAG系统，比如，LlamaIndex或LangChain。这两个框架都拥有比较好用的debugging工具，可以让我们定义回调函数，查看使用了哪些上下文，检查检索结果来自哪个文档等等。 如何评价RAG项目效果？针对检索环节评估： MRR平均倒数排名（Mean Reciprocal Rank）：用于衡量搜索引擎，推荐系统等根据查询返回多个结果的相关性。结果列表中，第i个结果匹配，分数为$\\dfrac{1}{i}$ Hits Rate命中率 NDCG 针对生成环节评估： 非量化：完整性、正确性、相关性（主观打分） 量化：Rouge-L方法需要准备人工的标准摘要集，将生成内容与标准摘要集对比，计算最长公共子序列来衡量生成文本和参考文本的相似性。计算精确率与召回率，最终得到F1-score作为Rouge-L最终分数。 由于Rouge-L注重最长公共子序列，这意味它比Rouge-1或Rouge-2更能衡量文本生成的结构和顺序是否与参考文本接近。因此，它在衡量文段的连贯性和句子顺序上具有优势。 RAG项目常见的优化策略（总结）RAG项目各个环节均有着极大的优化空间，下面我们穿插12个具体的优化策略来一次讲解。下面介绍的方法在AI开发框架langchain和LLamaindex中有具体实现。 1、知识文档准备阶段：（1）数据清洗（2）分块处理 2、嵌入模型：（1）优选嵌入模型 3、向量数据库：（1）元数据（支持关键词查询） 4、查询索引阶段：（1）多级索引（2）索引/查询算法（3）查询转换（4）检索参数（5）高级检索策略（6）重排模型 5、生成回答阶段：（1）提示词（2）大语言模型","link":"/2025/12/17/RAG/"},{"title":"LLM的前世今生","text":"时代的发展速度在变的越来越快。1975年，半导体芯片领域的著名论断————摩尔定律被提出（集成电路上可容纳的晶体管数目，每隔约两年便会增加一倍）。半导体行业大致按照摩尔定律发展了半个多世纪，带来了电脑，互联网，手机等事物。如今，从2017年开始到2025年即将结束，从NLP（自然语言处理）到LLM（通用大语言模型），深度学习技术的发展，并行计算GPU的进步，智能时代还会需要半个世纪的时间才能到达吗？ 不是，身处技术爆炸的时刻，时代发展速度只会越来越快，直到遇到人本困境。本文旨在介绍LLM大语言模型技术是如何从曾经的NLP任务，到通用预训练模型（Pre-Trained Model），再到未来展望的自学习模型（或者是AI Agent）。 自然语言处理：NLP（Natural Language Process）任务很好理解，就是试图让机器掌握人类使用的自然语言，诸如语音识别，外语翻译，舆情监控等任务都是其中的一部分。NLP有2个核心的任务：分别是【自然语言理解—NL Understand】和【自然语言生成—NL Generate】。 NLU是所有支持机器理解文本内容的方法模型或任务的总称，即能够进行常见的文本分类、序列标注、信息抽取等任务。简单来说，是指自然语言理解的运用希望机器人能够像人一样，具备正常的语言理解能力。例如，意图识别和实体提取的关键技能。 NLG （自然语言生成）是NLP的另一项核心任务，主要目的是降低人类和机器之间的沟通鸿沟，将非语言格式的数据转换成人类可以理解的语言格式。","link":"/2025/12/14/Summary/"},{"title":"手写Transformer-Decoder","text":"今年是2025年，参加组会的你一定会遇到某位师兄或师姐说，上周读了《Transformer is All You Need》论文作汇报。然后就会展示出这张经典的图片并说，大语言模型由Encoder和Decoder组成，巴拉巴拉。你的导师似懂非懂点了点头，你也点了点头，原来LLM就是这样，开始玩起了手机。 事实上Transformer结构确实主要由Encoder、Decoder 两个部分组成，两个部分分别具有不一样的结构和输入输出。 但针对 Encoder、Decoder的特点，面对不同的人物和微调需求，出现了不同的、对 Transformer 进行优化的思路。例如，Google 仅选择了 Encoder 层，通过将 Encoder 层进行堆叠，再提出不同的预训练任务-掩码语言模型（Masked Language Model，MLM），打造了一统自然语言理解（Natural Language Understanding，NLU）任务的代表模型——BERT。而 OpenAI 则选择了 Decoder 层，使用原有的语言模型（Language Model，LM）任务，通过不断增加模型参数和预训练语料，打造了在 NLG（Natural Language Generation，自然语言生成）任务上优势明显的 GPT 系列模型，也是现今大火的LLM的基座模型。当然，还有一种思路是同时保留 Encoder 与 Decoder，打造预训练的 Transformer 模型，例如由 Google 发布的 T5模型。 总的来说，短短十年不到的时间，就按顺序出现了Encoder-Only、Encoder-Decoder、Decoder-Only三种主流预训练模型。 具体来说: BERT 采用了 Encoder-Only 结构，只包含编码器部分；而 GPT 采用了 Decoder-Only 结构，只包含解码器部分。T5 则采用了 Encoder-Decoder 结构， 其中最火热的当属业界主流技术Decoder-Only，Decoder-Only作为GPT的模型，真正让LLM为世人所知。这种结构只需要接受序列数据，输出序列数据，就可以完成翻译，问答对话等多任务。其本质是自回归模型。 以下内容大量参考chaofa用代码打点酱油的博客，模型技术以当前（2025.12）最好的开源模型——Llama开源文档为准 面试过程中让写 transformers Decoder 一定要沟通清楚是写一个 CausalLM decoder 还是原版的，原版的比较复杂，一般也不会让写。这里的 Decoder 一般指的是 CausalLM，具体变化是少了 encoder 部分的输入，所以也就没有了 encoder and decoder cross attention。因为重点希望写 CausalLM，所以没有 Cross attention 和 也省略了 token embedding 这一步。 知识点这里有很多细节可以多说 关于归一化方法的说明： 按照归一化方法来分，主要分为LayerNorm，BatchNorm，RMSNorm以及DeepNorm。如果按照归一化位置来分类，包括 postNorm 和 preNorm，但具体来说用postNorm比较多。 BatchNorm对数据的一定维度在batch数据中归一化，一般应用于图像。这种方法很难适用于序列数据，对于序列数据而言，在batch维度做归一意义不大，而且一个batch内的序列长度不同。LayerNorm是针对序列数据提出的一种归一化方法，主要在layer维度进行归一化，即对整个序列进行归一化。layerNorm会计算一个layer的所有activation的均值和方差，利用均值和方差进行归一化。RMSNorm的提出是为了提升layerNorm的训练速度提出的。RMSNorm也是一种layerNorm，只是归一化的方法不同。相比layerNorm中利用均值和方差进行归一化，RMSNorm 利用均方根进行归一化。 $$LayerNorm: \\dfrac{x-Ex}{\\sqrt{Dx+\\epsilon}}*\\gamma + \\beta$$ transformers decoder 的流程是：input -&gt; self-attention -&gt; cross-attention -&gt; FFN causalLM decoder 的流程是 input -&gt; self-attention -&gt; FFN其他 [self-attention, FFN] 是一个 block，一般会有很多的 block FFN 矩阵有两次变化，一次升维度，一次降维度。其中 LLaMA 对于 GPT 的改进还有把 GeLU 变成了 SwishGLU，多了一个矩阵。所以一般升维会从 4h -&gt; 4h * 2 / 3 FFN是常见的前馈神经网络，主要经历一个升维再降维的过程（生成器基本都这样），为什么常见都是升4维呢？从工程角度来说，这是权衡训练成本和实际效果后的选择。从理论角度来说，常见的激活函数都会导致近50%的信息损失（参考ReLU），两层的结构说明最后会导致只有50% * 50% = 25%的信息保留，为了控制前后信息量不大变化，选择升4维。 原版的 transformers 用 post-norm, 后面 gpt2, llama 系列用的是 pre-norm。其中 llama 系列一般用 RMSNorm 代替 GPT and transformers decoder 中的 LayerNorm。 具体来说用postNorm比较多，具体的原因可以看之前这一篇 为什么大模型结构设计中往往使用postNorm而不用preNorm？ Decoder部分的代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108import mathimport torchimport torch.nn as nnimport warningswarnings.filterwarnings(action=&quot;ignore&quot;)# 写一个Blockclass SimpleDecoder(nn.Module): def __init__(self, hidden_dim, n_heads, dropout = 0.1): super().__init__() self.n_heads = n_heads self.head_dim = hidden_dim // n_heads self.dropout = dropout # 这里按照 transformers 中的 decoder 来写，用 post_norm 的方式实现，主要有 残差链接 # eps 是为了防止溢出；其中 llama 系列的模型一般用的是 RMSnorm 以及 pre-norm（为了稳定性） # RMSnorm 没有一个 recenter 的操作，而 layernorm 是让模型重新变成 均值为 0，方差为 1 # RMS 使用 w平方根均值进行归一化 $\\sqrt{\\frac{1}{n} \\sum_{1}^{n}{a_i^2} }$ self.layernorm_att = nn.LayerNorm(hidden_dim, eps =0.00001) self.q_proj = nn.Linear(hidden_dim, hidden_dim) self.k_proj = nn.Linear(hidden_dim, hidden_dim) self.v_proj = nn.Linear(hidden_dim, hidden_dim) self.o_proj = nn.Linear(hidden_dim, hidden_dim) self.drop_att = nn.Dropout(self.dropout) #准备FFN前馈神经网络 self.up_proj = nn.Linear(hidden_dim, hidden_dim * 4) self.down_proj = nn.Linear(hidden_dim * 4, hidden_dim) self.layernorm_ffn = nn.LayerNorm(hidden_dim, eps = 0.00001) self.act_fn = nn.ReLU() self.drop_ffn = nn.Dropout(self.dropout) def attention_output(self, Q, K , V, attention_mask = None): #计算两者相关性 K = K.transpose(2,3) # [batch_size, n_heads, head_dim, seq_len] # 还没有softmax的att_weight att_weight = (Q @ K) / math.sqrt(self.head_dim) #attention mask进行依次调整：变成causal_attention if attention_mask is not None: #变成下三角矩阵 attention_mask = attention_mask.tril() #提取下三角部分单向注意力 att_weight = att_weight.masked_fill(attention_mask == 0, float(&quot;-1e20&quot;)) #对0部分参与一个极小值，在softmax后为0 else: #人工构造下三角Attention mask attention_mask = torch.ones_like(att_weight).tril() att_weight = att_weight.masked_fill(attention_mask == 0, float(&quot;-1e20&quot;)) att_weight = torch.softmax(att_weight, dim = -1) # print(att_weight) # Dropout论文最早提出的时候被拒稿，认为只有工程而无理论。再看看今天Dropout无处不在，果然是“入关后自有大儒为我辩证” att_weight = self.drop_att(att_weight) mid_output = att_weight @ V # shape [batch_size, n_heads, seq_len, head_dim] #contiguous()函数在PyTorch中用于确保张量（tensor）在内存中的数据是连续存储的。当执行transpose()、permute()等改变维度顺序的操作后，张量可能会变得“非连续”（不按行优先或列优先存储）。contiguous()的作用是返回一个新的、内存连续的张量副本，这样view()（重塑形状）等需要连续内存的操作才能正确执行，提升计算效率。 mid_output = mid_output.transpose(1,2).contiguous() batch_size, seq_len, _, _ = mid_output.size() mid_output = mid_output.view(batch_size, seq_len, -1) # O矩阵只是权重矩阵，多头注意力机制中，对拼接后的矩阵处理 output = self.o_proj(mid_output) return output def attention_block(self, X, attention_mask = None): batch_size, seq_len, _ = X.size() Q = self.q_proj(X).view(batch_size, seq_len, self.n_heads, -1).transpose(1,2) K = self.k_proj(X).view(batch_size, seq_len, self.n_heads, -1).transpose(1,2) V = self.v_proj(X).view(batch_size, seq_len, self.n_heads, -1).transpose(1,2) output = self.attention_output(Q,K,V,attention_mask = attention_mask) return self.layernorm_att(X + output) def ffn_block(self, X): # act_fn = ReLU up = self.act_fn(self.up_proj(X)) down = self.down_proj(up) #dropout down = self.drop_ffn(down) # norm return self.layernorm_ffn(X + down) def forward(self, X, attention_mask = None): # X 一般假设是已经经过embedding的输入， （batch_size, seq_len, hidden_dim） # attention_mask 一般指的是 tokenizer 后返回的 mask 结果，表示哪些样本需要忽略 # shape 一般是： (batch_size, n_heads, seq_len) att_output = self.attention_block(X, attention_mask = attention_mask) ffn_output = self.ffn_block(att_output) return ffn_output# 测试x = torch.rand(3, 4, 64) # [batch_size, seq_len, hidden_dim]net = SimpleDecoder(64, 8) # hidden_dim = 64,n_heads = 8mask = ( torch.tensor([[1, 1, 1, 1], [1, 1, 0, 0], [1, 1, 1, 0]]) .unsqueeze(1) .unsqueeze(2) .repeat(1, 8, 4, 1))net(x, mask).shape 关于推理过程如果你认真看了上面的代码部分，不知道你会不会产生这样的疑问：大语言模型不是根据当前的字预测下一个字是什么吗？请问一下推理时prefill阶段为什么需要计算所有token的q。预测next token的话不是只需要计算上一个token的q就好了么？因此只需要算prefill最后一个token的q就行了？ 这是一个非常非常非常好的问题👍 核心原因是 Decoder 中有很多层，假设只有一层，你的思考是没有问题。但是 decoder 有很多 block，上一层（假设第2层）的输入是下一层的输出，下一层（第一层）的输出依赖于 qkv 的计算，那么计算第二层的 q k v的时候，如果不知道第一层的 q k v（或者output），那么第二层的 k v 就无法计算出来了。因为 prefill 之前没有缓存好每一层的 k v 是什么。 那么换句话说，如果能缓存好每一层的K，V矩阵，推理速度将会大大增加！ 唯一需要考虑的问题是：如何高效的实现KV矩阵缓存。我们将来到KV Cache ——百倍推理加速技术","link":"/2025/12/11/TransformerDecoder/"},{"title":"一些有意思的算法题","text":"不想刷LeetCode啊！！！ 言归正传，写代码还是很有意思的事情。我大学读的是数学专业，一开始只觉得写代码是计算机专业该干的事，后来惊奇地发现，有些算法莫名与那些恐怖的数学知识相关，而这些算法都是为了解决一个具体的有趣问题。这里整理一些LeetCode中有趣的题目，更多还是结合数学的内容。 Q1: 爬楼梯这个问题我应该是最早在电影《少年班》里看到的，那个神童摇着龟壳，根据掉落铜钱来回答的场景太神棍，印象很深。 假设你正在爬楼梯。需要 n 阶你才能到达楼顶。每次你可以爬 1 或 2 个台阶。你有多少种不同的方法可以爬到楼顶呢？比如2级台阶有两种方法：1+1、2；3级台阶有三种方法：1+1+1、1+2、2+1； 从递归的角度容易考虑到，记上$n$级台阶的方法数量为$f(n)$，那么前一步一定在$n-1$级台阶或者$n-2$级台阶，有$f(n)=f(n-1)+f(n-2)$等式成立。这就是斐波那契数列的通项$1,2,3,5,8,13,21….$，自然很容易的写出这样的算法。 12345678910def climbStairs(self, n: int) -&gt; int: if n &lt;= 2: return n # Fibonacci-like sequence first, second = 1, 2 for i in range(3, n + 1): first, second = second, first + second return second 事实上对于$f(n)=f(n-1)+f(n-2)$的齐次线性递推式，除了用高中在数列中用到的特征方程方法来求出斐波那契数列的通项公式外，还可以将齐次线性递推式写为矩阵，使用矩阵快速幂方法来实现。后者的优势在于，对于某些特殊的非齐次线性递推式，也可以处理。 Q2: 只出现一次的数 给你一个 非空 整数数组 nums ，除了某个元素只出现一次以外，其余每个元素均出现两次。找出那个只出现了一次的元素。你必须设计并实现线性时间复杂度的算法来解决此问题，且该算法只使用常量额外空间。 这道题难度easy，但是其解法中的位运算方法实在太优雅（依赖于其余数只出现偶数次），让我想起很多年前见到过一个位运算求N皇后问题的牛逼解法。这正好借这个机会展示二进制的魅力。 1234567891011def singleNumber(self, nums: List[int]) -&gt; int: &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; n = len(nums) index = 0 for i in range(n): # 异或有结合律和交换律 index ^= nums[i] return index 位运算的魅力： binary_num = ((num &amp; (1 &lt;&lt; np.arange(10,-1,-1)))&gt;0).astype(int) #利用位运算的二进制计算技巧 N &amp; (N-1)","link":"/2025/12/11/%E4%B8%80%E4%BA%9B%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E7%AE%97%E6%B3%95%E9%A2%98/"},{"title":"数据科学岗-面试经历一","text":"一个互联网大厂的数据科学岗，面试在12月某天晚上，时长预计1h（实际快1个半小时），岗位偏数据分析。 面试过程 自我介绍 目前在哪里，专业经历，有几段实习经历，希望未来向什么方向发展，珍惜这个实习机会，实习时长保证。 Small Talk： 可能是看我简历上有不同领域（一个算法技术方向，一个金融量化方向）的实习经历，聊了很多职业规划和对LLM在数据科学领域的看法。 SQL题和概率题： SQL题做的很差，问题大致类型都是每日每类Topk问题（主要是分析函数掌握的太不好了，本次面试表现最差的点），概率题是关于丢筛子赌徒类问题，easy。 了解AB实验吗？流程是什么样的： 从因果推断的角度来说的，有实验设计来实现完全随机化的就是AB实验，无法实现完全随机化的只能做观察性研究，巴拉巴啦。 多重比较方法了解吗？ 这里我的回答太不专业了。因为不记得具体内容，我按理解来说，“多重比较”听上去是对是对多组样本比较，作假设检验，可能存在需要Bonferroni修正来控制总体显著性水平。后续问了假设检验的基本概念等。 业务场景面试： 业务发现本月用户支付次数下降10%，你会按什么流程来检验这个过程- 回答先考虑未知协变量影响（季节，同业竞争双十一，数据计算口径，外部环境如天气），再缩小范围考虑相关协变量影响（不同地区表现区别，具体那些时间点的差异，转化过程问题，渠道）有一个社群新功能，如何对这个功能做AB检验，目标变量是什么？回答：考虑访问概率，判断该功能是否能促进互动意愿。 反问环节 问了业务题目如何回答：面试官很友善，说大致都有但不够清晰，从“维度拆解”的解读回答更完整。社群互动业务有UV，活跃度，总人数，首次使用，从用户角度来说，从社群来说，特殊字符占比，平均互评次数等等指标 总结：互联网大厂面试官都很友善，不管能不能通过，都要珍惜面试机会来学习。对代码能力会在线检验（刷LeetCode必不可少）；专业能力一定要自信，谈理论的时候一定要自信，少说“可能”，“也许”，根据假设大胆开口；业务场景自己多总结，面试前多看类似的行业分析报告，有些思考分析的思路口诀还是很有意义的； 多重比较多重比较（multiple comparisons）是指方差分析后，对各样本平均数间是否有显著差异的假设检验的统称。严格来说属于事后检验。 多重比较要从方差分析ANOVA开始讲起，每个统计学的同学肯定都会先学两样本的T检验，然后又开始学习ANOVA方差分析。虽然理论上来说，可以使用多个 t 检验来替代方差分析（ANOVA），但这种方法并不推荐，主要有以下几个原因： 增加第一类错误率和多重比较问题：每次进行 t 检验时，都会有一个预设的显著性水平（通常为 5% 或 p &lt; 0.05），这表示我们接受错误结论的概率为 5%。如果对多个组进行多次 t 检验，这些个别的错误概率会累积，导致总体第一类错误率显著上升。这种累积错误称为“多重比较问题”。例如，如果对 5 个组进行配对比较（总共 10 次 t 检验），假设每次 t 检验的显著性水平为 0.05，那么实际的总错误概率将远超过 5%。这意味着，随着比较次数的增加，出现假阳性（错误拒绝原假设）的风险显著增加，导致研究结果不可靠。 效率低下和复杂性增加：当你比较 三个或更多组时，需要对每两个组进行一次 t 检验。随着组数的增加，所需的 t 检验次数会迅速增长。例如，4 组数据需要进行 6 次 t 检验，5 组需要 10 次，依此类推。这不仅增加了计算量和分析时间，还显著提高了累积的第一类错误风险。这种增加的复杂性和错误风险是多重比较问题的直接体现。 丢失整体信息和无法检验总体显著性：多个 t 检验只能分别比较每两个组之间的差异，无法给出所有组间差异的整体统计结论，并可能忽略一些更复杂的模式和交互效应。相反，方差分析（ANOVA）能够同时考虑所有组之间的差异，提供一个关于组间总变异的综合分析，并一次性检测多组之间的总体差异。在 ANOVA 中，如果结果显示有显著的总体差异，研究者还可以进一步进行事后多重比较检验（如 Tukey HSD、Bonferroni 校正等），以确定哪些具体组之间存在差异，并在控制总体错误率的前提下得出可靠的结论。 多重比较控制和效能保障：ANOVA 之后的多重比较方法（如 Tukey HSD 检验、Bonferroni 校正等）专门设计用来解决多重比较问题。这些方法能够有效地控制整体的第一类错误率，同时进行多组间的配对比较。而简单地使用多个 t 检验，无法实现对整体错误率的有效控制。 因此，在需要比较多组数据时，使用“ ANOVA + 事后多重 ”的综合比较方法可以提供更高的效能和准确性。 事后检验（Post-hoc testing）如果使用 ANOVA 发现了显著的效果，意味着至少有两个组之间存在显著差异，但它并不能具体指出哪些组之间的差异显著。为此，需要进行事后检验（Post-hoc tests），也就是上述提到得多重比较。在进行多个假设检验时，注意控制总体犯第一类错误的概率（族错误率，Family-Wise Error Rate, FWER） 控制FWER的方法: Bonferroni校正：基于Bonferroni不等式，若进行$m$次独立检验，将显著性水平调整为$\\frac{\\alpha}{m}$，则$FWER\\leq\\alpha$。 这种方法非常简单，适用性广，但显然太过保守，$m$很大时功效很低 Holm校正（逐步Bonferroni）：按p值从小到大排序，逐步比较，比Bonferroni更有效（功效更高）。将$m$个$p$值从小到大排序$p_{(1)}\\leq p_{(2)}\\leq … \\leq p_{(m)}$，从最小的开始，若$p_{(i)}\\leq\\frac{\\alpha}{m-i+1}$则拒绝相应假设；否则停止。 比Bonferroni功效高，且仍控制FWER。但仍相对保守。 Sidak校正：假设检验独立，调整每个检验的显著性水平为$1-(1-\\alpha)^{\\frac{1}{m}}$ 当检验独立时，比Bonferroni稍强,特别在处理大量比较时更有效。但依赖于独立性假设。 特定样本分布的事后检验方法这类方法常用于方差分析（ANOVA）后的事后两两比较，考虑了多重比较的依赖结构。 Tukey’s HSD（Honestly Significant Difference）：适用方差齐性、样本量平衡（各组样本量相等或接近）。基于Studentized Range分布（q分布）计算临界值，控制所有两两比较的FWER。检验统计量$q=\\frac{\\bar{X_i}-\\bar{X_j}}{\\sqrt{MSE/n}}$ 功效高于Bonferroni，且专门为两两比较设计。 Games-Howell检验：在方差不齐和样本量不等的情况下非常有效，是处理这些情况的最佳方法。基于Welch’s t检验的扩展，使用调整后的自由度和Studentized Range分布。检验统计量$q=\\frac{\\bar{X_i}-\\bar{X_j}}{\\sqrt{\\frac{s_i^2}{n_i}+\\frac{s_j^2}{n_j}}}$，唯一需要注意的时需要利用Welch-Satterthwaite公式计算调整自由度。 优点：稳健处理方差不齐和样本量不等，是Tukey’s HSD在异方差下的替代。 Dunnett检验:适用多个实验组与一个对照组的比较。原理：基于多元t分布控制FWER，比Bonferroni更有效。 非参数方法的事后检验：当数据不满足正态性、方差齐性等参数假设时，非参数方法在事后多重比较中尤为重要。它们主要基于秩和检验，并对多重比较问题进行调整。 Steel-Dwass 检验（非参数版 Tukey HSD）：在 Kruskal-Wallis 检验（非参数版的单因素ANOVA）显著后，进行所有组间的两两比较。 有足够样本量后，直接控制族错误率（FWER），是最严格的非参数事后比较方法之一。 Dunn 检验（带校正）:同样在Kruskal-Wallis检验后使用，但更灵活，尤其适用于样本量不等或只关注特定比较的情况。 方法简单直观，但依赖于大样本近似。需要通过p值校正来控制错误率，常用且易于实现。 Studentized Range分布Studentized Range分布（又称q分布）是多重比较检验中用于控制族错误率（FWER）的关键概率分布。其定义如下： 设我们有k个独立的正态随机变量$Y_1,Y_2,…,Y_k$，每个服从$N(\\mu,\\sigma^2)$，且有一个独立的方差估计$s^2$（基于v自由度的卡方分布）。则统计量： $$q=\\dfrac{\\max(Y_i)-\\min(Y_i)}{s}$$ 服从自由度为k和ν的Studentized Range分布，记为$q(k,ν)$。在多重比较中，$Y_i$通常代表各组的样本均值（标准化后），$s$是标准误的估计，$ν$是误差自由度（如ANOVA中的残差自由度）。 当同时进行多个两两比较时，我们需控制所有比较中至少犯一次第一类错误的概率（FWER）。若使用普通t检验，FWER会随比较次数增加而急剧膨胀。例如，进行10次独立$t$检验（$\\alpha=0.05$），FWER可达约0.40。 Studentized Range分布解决了这一问题，它考虑了所有均值间的极差分布，从而为多重比较提供准确的临界值。其核心思想是：当所有总体均值相等时，样本均值的极差（最大值与最小值之差）服从Studentized Range分布。因此，我们可以基于该分布确定一个临界值，使得极差超过该值的概率为$\\alpha$，从而控制所有两两比较的FWER。","link":"/2025/12/25/Interview/"},{"title":"Agent智能体","text":"Agent智能体可谓是当前（2025-12）最火热的话题了。据我在大厂当产品经理的朋友说，是个团队都在做AI-agent产品。本文主要基于Hugging Face提出的smolagent课程来阐述。 比较典型的Agent例子如 Devin software engineer 能独立完成整个编程项目，某种购物智能体可以自动浏览同一商品所有平台的价格和优惠，寻找全网最低价。 何时使用智能体框架在理解Agent智能体框架之前，我们最好先搞清楚，什么时候才需要Agent？构建围绕大语言模型（LLMs）的应用时，并不总是需要智能体框架。它们在工作流中提供了灵活性，可以高效地解决特定任务，但并非总是必需的。 Airflow静态工作流有时，预定义的工作流足以满足用户请求，比如Apache Airflow 是优秀的工作流编排工具（Workflow Orchestration），擅长定义静态、预定义的 DAG（有向无环图），调度任务、处理依赖、监控执行。适合 ETL 数据管道、批处理、定时任务等确定性强、可预测的流程。可以用 Airflow 解决的场景：固定步骤的自动化，如每日数据清洗 + 模型训练 + 报告生成。集成 LLM 的简单工作流（如 Airflow AI SDK 支持 @task.llm 装饰器调用模型、结构化输出、条件分支）。将 AI Agent 作为 Airflow 中的一个任务节点，Airflow 负责整体调度和可靠性。 Agent：一个能够进行推理、规划和与环境交互的人工智能模型如果构建智能体的方法很简单，比如一系列提示，使用纯代码可能就足够了。使用Agent的最重要环节是，是否存在无法定义工作内容的抽象环节。 AI Agent 是动态循环Re-Act方法（感知 → 规划 → 行动 → 反思），能处理不确定性、自主分解任务、迭代优化。对于复杂目标（如“帮我规划一次旅行”或“开发一个 App”），Airflow 需要人为定义所有分支，而 AI Agent 可以自主推理和适应。 此时，工作流变得更加复杂，需要不断更新，和用户交互，传统的静态Airflow显然无法解决需求随时改变的问题。基于这些想法，我们已经可以确定Agent对一些功能的需求： 一个驱动系统的大语言模型引擎。 智能体可以访问的工具列表。 用于从大语言模型输出中提取工具调用的解析器。 与解析器同步的系统提示。 一个记忆系统。 错误日志和重试机制以控制大语言模型的错误。 智能体的核心大脑：LLM大语言模型 (LLM) 的基本原理简单却极其有效：其目标是在给定一系列前一个 token 的情况下，预测下一个 token。这里的“token”是 LLM 处理信息的基本单位。你可以把 “token”想象成“单词”，但出于效率考虑，LLM 并不直接使用整个单词。你可以通过tokenizer编码器来认识不同语言究竟是如何确定最小token的。 每个大语言模型 (LLM) 都有一些特定于该模型的特殊 token，例如，用于指示序列、消息或响应的开始或结束。此外，我们传递给模型的输入提示也使用特殊 token 进行结构化。其中最重要的是序列结束 token (EOS，End of Sequence token) 聊天模版大部分用户通常通过聊天界面与智能体交互。但实际上LLM只是在做下一句话预测，它是如何想一个人一样聊天的呢？我们需要理解 LLMs 如何管理聊天 与AI对话，本质是交换消息。在后台，这些消息会被连接并格式化成模型可以理解的提示。一个英文实例如下： 聊天模版将对话消息（用户和助手轮次）整理成所选 LLM 需要的特定格式。 智能体的手：工具tools目前为止，智能体空有大脑，却无法行动（你可以让他生成对话告诉你目前最流行的歌，却无法让他直接打开APP播放流行歌），为了实行Action，我们需要tools。 工具类型 描述 网络搜索 允许智能体从互联网获取最新信息 图像生成 允许根据文本生成图像 信息检索 从外部源检索信息 API接口 与外部 API 交互（GitHub、YouTube、Spotify 等） 智能体的行动：思考-行动-观察循环这三个组件在一个持续的循环中协同工作。用编程的类比来说，智能体使用一个 while 循环：循环持续进行，直到智能体的目标被实现。 ReAct 方法让智能体完成目标操作的核心方法是 ReAct 方法，即”推理”（Reasoning/Think）与”行动”（Acting/Act）的结合。ReAct 是一种简单的提示技术（属于Prompt工程内容），在让 LLM 解码后续 token 前添加”Let’s think step by step”（让我们逐步思考）的提示。 通过提示模型”逐步思考”，可以引导解码过程生成计划而非直接输出最终解决方案，因为模型被鼓励将问题分解为子任务。 这种方法使模型能够更详细地考虑各个子步骤，通常比直接生成最终方案产生更少错误。 推理策略受到广泛关注，这体现在 Deepseek R1 或 OpenAI 的 o1 等模型的开发中。这些模型经过微调，被训练为“先思考再回答”。它们通过特殊标记（ 和 ）来界定 思考 部分。这不仅是类似 ReAct 的提示技巧，更是通过分析数千个示范案例，让模型学习生成这些思考段的训练方法。 常见的智能体开发框架 框架 描述 开发者 smolagent 轻量级框架、快速原型、透明易调试 Hugging Face LangGraph 生产级开源框架LangChain的扩展，支持复杂、可控的代理工作流 LangChain AI LlamaIndex 开源（原GPTindex）、适用数据密集型任务如RAG、私有数据代理 LlamaIndex 一个简单的智能体框架：smolagent12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273from smolagents import CodeAgent,DuckDuckGoSearchTool, HfApiModel,load_tool,toolimport datetimeimport requestsimport pytzimport yamlfrom tools.final_answer import FinalAnswerToolfrom Gradio_UI import GradioUI# 工具是Agent与外部信息互动的重要内容，定义工具tool最重要的是：# 为你的函数提供输入和输出类型，例如 get_current_time_in_timezone(timezone: str) -&gt; str:#格式良好的文档字符串。smolagents 期望所有参数在文档字符串中都有文字描述。@tooldef my_custom_tool(arg1:str, arg2:int)-&gt; str: #it's import to specify the return type #Keep this format for the description / args / args description but feel free to modify the tool &quot;&quot;&quot;A tool that does nothing yet Args: arg1: the first argument arg2: the second argument &quot;&quot;&quot; return &quot;What magic will you build ?&quot;@tooldef get_current_time_in_timezone(timezone: str) -&gt; str: &quot;&quot;&quot;A tool that fetches the current local time in a specified timezone. Args: timezone: A string representing a valid timezone (e.g., 'America/New_York'). &quot;&quot;&quot; try: # Create timezone object tz = pytz.timezone(timezone) # Get current time in that timezone local_time = datetime.datetime.now(tz).strftime(&quot;%Y-%m-%d %H:%M:%S&quot;) return f&quot;The current local time in {timezone} is: {local_time}&quot; except Exception as e: return f&quot;Error fetching time for timezone '{timezone}': {str(e)}&quot;final_answer = FinalAnswerTool()# If the agent does not answer, the model is overloaded, please use another model or the following Hugging Face Endpoint that also contains qwen2.5 coder:# model_id='https://pflgm2locj2t89co.us-east-1.aws.endpoints.huggingface.cloud' model = HfApiModel(max_tokens=2096,temperature=0.5,model_id='Qwen/Qwen2.5-Coder-32B-Instruct',# 很有可能该模型被超多人使用，过载导致回答困难custom_role_conversions=None,)# Import tool from Hubimage_generation_tool = load_tool(&quot;agents-course/text-to-image&quot;, trust_remote_code=True)with open(&quot;prompts.yaml&quot;, 'r') as stream: prompt_templates = yaml.safe_load(stream) agent = CodeAgent( model=model, tools=[final_answer], ## 在此处添加自定义的工具tool(记得不要删去final answer) max_steps=6, verbosity_level=1, grammar=None, planning_interval=None, name=None, description=None, prompt_templates=prompt_templates)GradioUI(agent).launch()","link":"/2025/12/30/agent/"},{"title":"特征工程心得总结","text":"在C端业务中，特征工程往往比模型本身更决定最终效果的上限。 C端数据分析中的结构化特征工程：从统计学视角的完整实践指南在C端业务中，特征工程往往比模型本身更决定最终效果的上限。 为什么这么说？因为C端数据通常具有以下几个典型特征： 高噪声（用户行为极度随机） 非平稳（节假日、版本迭代、热点事件、竞品动作都会造成分布剧烈漂移） 高基数类别（商品、内容、用户兴趣标签、搜索词等） 强时序性 + 强交互性（用户-内容、用户-商品、用户-用户之间的交叉行为） 严重长尾分布（头部20%内容/商品贡献80%+曝光与点击） 这些特性决定了：C端模型对特征的依赖程度远高于B端模型，也决定了特征工程必须有更强的结构化思维和统计学纪律。 本文尝试从统计学的视角，把C端特征工程拆解成四个清晰且可落地的模块，并给出每个模块的核心统计考量、常见陷阱、C端典型做法，以及监控与迭代的闭环思路。 一、特征理解与探索：先花80%的时间搞清楚“这是什么数据”C端数据分析中最常见的死亡方式之一就是：还没搞清楚数据在讲什么故事，就开始疯狂堆特征。 1.1 业务语义与因果假设先行在动手之前，先回答这几个问题（建议写下来）： 目标变量到底在衡量什么？（是次留、首日付费、7日LTV、内容消费时长、单次session深度、分享率……） 这个目标受哪些核心环节驱动？（曝光 → 点击 → 停留 → 交互 → 转化） 哪些是可控因素？哪些是不可控的混杂因素？ 时间窗口如何切分才符合业务节奏？（短视频可能是分钟级，电商可能是自然日/自然周） 1.2 必须做的分布与代表性诊断（C端最容易翻车的点） 检查维度 推荐统计工具/方法 C端常见异常表现 建议处理方向 缺失比例与模式 missingno、按时间/设备/版本分组缺失率热图 新用户/低活跃用户缺失率显著更高 MAR假设下多用模型插补，MNAR慎删 异常值（点击率、停留时长） IQR×1.5/3、孤立森林、极值分位点业务规则 0.1%用户贡献了30%总时长 winsorize / 业务规则截断 / 分模型 长尾分布严重程度 洛伦兹曲线、基尼系数、头部10%/1%占比 Top 1%内容占总播放量60%+ log变换、分桶、target encoding 时间趋势与周期性 按自然日/周/小时聚合目标变量折线图 + STL分解 周末/节假日/晚高峰明显抬升 加入节日 dummy、周中/周末 dummy 新老用户/设备差异 分群KS检验、PSI 新用户点击率比老用户低40%+ 新老/冷启动/设备分桶或分模型 一句话总结这一步的核心纪律：任何一次删除、截断、插补、变换之前，都要先用统计检验或可视化确认“这件事对不同子群体是否一致”。 二、特征构建与变换：把“脏乱差”的原始行为变成“模型友好”的信号C端特征工程80%的战斗力其实发生在这里。 2.1 数值型特征的统计变换路线图C端最常见的数值特征：曝光、点击、完播率、停留秒数、点赞/评论/分享数、消费金额、连续登录天数…… 推荐的处理路径（优先级从高到低）： 先做领域常识截断（比统计方法更重要） 最大曝光次数、最大单次停留秒数、单日最大付费金额…… 长尾压缩（几乎必做） log1p(x) log1p(x / 一些业务分母) √x（次之） Box-Cox / Yeo-Johnson（当你有精力调参时） 分桶（提升可解释性 &amp; 非线性捕捉） 等频分桶（最常用） 监督分桶（卡方、决策树分桶） 业务分桶（新/老、冷/温/热、0/1-5/6-20/21+……） 标准化 / 中心化 全局StandardScaler（慎用，C端分布漂移严重） GroupBy标准化（按用户、按内容类型、按新老用户……） RobustScaler（推荐） 相对值 / 比率特征（C端杀手锏） 用户近7日平均点击率 - 全局平均点击率 该内容近24h完播率 / 该作者历史完播率 当前session停留时长 / 该用户历史平均session停留 2.2 类别特征的C端高危处理方式对比 编码方式 优点 C端致命缺点 推荐场景 OneHot 简单、无序假设 维度爆炸、内存爆炸 基数&lt;50的强分类（如性别、OS版本） Label Encoding 节省空间 引入虚假序关系 有自然序的（如会员等级） Frequency 保留流行度信息 热门类别主导梯度 辅助特征 Target Encoding 直接引入目标信息 严重数据泄露 &amp; 过拟合风险 需要交叉验证/噪声/平滑 WOE + IV 可解释性强、风控体系成熟 假设单调性、对长尾不友好 排序/召回粗排阶段 Entity Embedding 捕捉复杂语义关系 需要大量数据、训练成本高 头部内容/商品/用户场景 C端目前工业界最主流的组合（2024-2025主流打法）： 高基数离散 → Target Encoding（加噪声/交叉验证/历史均值平滑） 中基数离散 → WOE / Count + Log / Frequency 超高基数（搜索词、商品标题）→ 先做Text Embedding → 再聚类/降维/直接作为稠密特征 2.3 C端最有杀伤力的几类统计聚合特征 用户行为序列统计（近N天/次） 近7/14/30/60/90日 曝光/点击/完播/点赞/分享/收藏/关注/付费 次数 &amp; 金额 上述指标的环比/同比变化率 方差、偏度、峰度（捕捉用户波动性） 内容/商品侧热度统计（时间衰减重要） 过去1h/3h/6h/12h/24h/3d/7d 的曝光/点击/完播/互动量（指数衰减或幂律衰减） 热度排名百分位（rank percentile） 交叉统计（用户×内容双维度） 用户对该作者/该一级类目/该二级类目的历史偏好度 用户对相似内容（embedding余弦相似度TopK）的平均行为 三、特征选择与评估：C端宁可多也不愿漏，但必须有纪律C端常见误区：特征从300个堆到3000个，以为越多越好。 统计学真实世界经验：特征超过一定量级后，边际收益快速递减，而方差爆炸、训练时间指数增长、线上稳定性下降。 推荐分三层做特征筛选： 层1：粗筛（快、无模型）方差阈值 + 缺失率阈值 + IV值（&gt;0.02） + 单变量 spearman / AUC &gt; 0.52~0.55 层2：模型内嵌筛选（中速、有信号） LightGBM / XGBoost / CatBoost 的 gain / split / cover 重要性 permutation importance（更可靠但更慢） SHAP value 全局重要性排序 层3：精选 + 稳定性验证 Boruta（基于影子特征） Recursive Feature Elimination（RFE） 在最近3-6个月不同时间窗口上跑重要性，保留稳定性高的特征（排名方差小的） 四、特征监控与迭代：C端特征就是“活的”C端特征衰减速度远超B端。 必须建立的监控体系（建议每周/双周跑一次）： 单特征健康度 PSI（分训练/验证/线上最近7/14/30天） IV值随时间衰减曲线 单特征预测力（AUC / normalized gini）衰减 群体分布漂移 新老用户分群PSI 不同内容垂类PSI 不同地域/设备PSI 特征重要性漂移 Top30特征重要性排名相关系数（spearman） 新增Top10特征的来源分析（是新构造？还是老特征突然崛起？） 业务指标代理 线上KS / NDCG@K / Recall@K / GMV / DAU 等随时间变化 是否能找到特征端的变化解释业务指标变化 迭代节奏建议（C端真实经验）： 每周：跑一次全量特征重要性 &amp; PSI看板 每两周：补充/替换5-15个新特征（通常来自业务新玩法、新曝光策略、新内容类型） 每1-2个月：做一次全量特征池清洗（删除持续3个月贡献度&lt;1%的特征） 每季度：重新跑一次 embedding / 图网络 / 序列模型，刷新稠密特征 最后用一句话总结C端特征工程的统计学本质： 用最严格的统计纪律和时间稳定性要求，去对抗C端最严重的噪声、非平稳、长尾与高基数，最终让模型看到的是一个相对“干净、平稳、可解释”的信号世界。 （全文完）","link":"/2026/01/15/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"},{"title":"pytorch框架细节","text":"记录了在学习DIDL2过程中，从前对深度学习一知半解或者不清楚细节的内容 预备知识数据操作pytorch中运行一些操作可能会导致为新结果分配内存。 例如，如果我们用Y = X + Y，我们将取消引用Y指向的张量，而是指向新分配的内存处的张量。 在下面的例子中，我们用Python的id()函数演示了这一点， 它给我们提供了内存中引用对象的确切地址。 运行Y = Y + X后，我们会发现id(Y)指向另一个位置。 这是因为Python首先计算Y + X，为结果分配新的内存，然后使Y指向内存中的这个新位置。 123456before = id(Y)Y = Y + Xid(Y) == beforebefore = id(X)X += Y # X += Y 和 X[:] = X + Y效果一致，都是在原来的地址重新计算（原地操作，节省内存）id(X) == before 数据预处理以pandas包处理为例。*“NaN”*项代表缺失值。 为了处理缺失的数据，典型的方法包括插值法和删除法， 其中插值法用一个替代值弥补缺失值，而删除法则直接忽略缺失值。 在删除法中，需要注意的是确定缺失类型，是否是有条件缺失 在这里，我们将考虑插值法。通过位置索引iloc，我们将data分成inputs和outputs， 其中前者为data的前两列，而后者为data的最后一列。 对于inputs中缺少的数值，我们用同一列的均值替换“NaN”项。 123inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]inputs = inputs.fillna(inputs.mean())print(inputs) 对于离散值和类别值，它的值只是起到分类的作用而无数值意义，正确的做法是转换为哑变量，即0-1属性变量 12inputs = pd.get_dummies(inputs, dummy_na=True) #dummy_na表示是否将NaN视为一类print(inputs) 由于pandas软件包是Python中常用的数据分析工具，pandas可以与张量兼容。数据转换为张量格式后，可以通过更多张量函数来进一步操作。 12X = torch.tensor(inputs.to_numpy(dtype=float))y = torch.tensor(outputs.to_numpy(dtype=float)) 关于张量tensorn维数组，也称为张量（tensor）。 例如，向量是一阶张量，矩阵是二阶张量。 无论使用哪个深度学习框架，它的张量类（在MXNet中为ndarray， 在PyTorch和TensorFlow中为Tensor）都与Numpy的ndarray类似。 但深度学习框架又比Numpy的ndarray多一些重要功能： 首先，GPU很好地支持加速计算，而NumPy仅支持CPU计算； 其次，张量类支持自动微分。 这些功能使得张量类更适合深度学习。 12345678910A = torch.arange(24, dtype=torch.float32).reshape(2, 3, 4) #常见创建tensor的操作，可指定数值类型A.numel() #获取张量元素个数sum_A = A.sum(axis=1) #按列求和（对列进行降维求和）sum_A = A.sum(axis=1, keepdims=True) #（非降维求和）A.cumsum(axis=0) #沿行轴计算元素累积和torch.mv(A, x) #矩阵与向量乘积torch.mm(A, B) #矩阵与矩阵乘积u = torch.tensor([3.0, -4.0])torch.norm(u) 更多关于线性代数部分的张量计算可以参考Geometry and Linear Algebraic Operations 关于导数与微分一元函数部分的导数与微分自动跳过，直接来到多元函数的部分，梯度向量由n个偏导数给出： $$\\nabla_xf(x)=\\left[\\dfrac{\\partial f(\\mathbf{x})}{x_1},\\dfrac{\\partial f(\\mathbf{x})}{x_2},…\\dfrac{\\partial f(\\mathbf{x})}{x_n}\\right]$$ 对所有$\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$,都有$\\nabla_x\\mathbf{Ax}=\\mathbf{A}^T$ 对所有$\\mathbf{A}\\in\\mathbb{R}^{n\\times m}$,都有$\\nabla_x\\mathbf{x}^T\\mathbf{A}=\\mathbf{A}$ 对所有$\\mathbf{A}\\in\\mathbb{R}^{n\\times n}$,都有$\\nabla_x\\mathbf{x}^T\\mathbf{Ax}=(\\mathbf{A}+\\mathbf{A}^T)\\mathbf{x}$ 对任何矩阵$\\mathbf{A}$，都有$\\nabla_{\\mathbf{A}}||\\mathbf{A}||^2_F=2\\mathbf{A}$ 求导是几乎所有深度学习优化算法的关键步骤。 虽然求导的计算很简单，只需要一些基本的微积分。 但对于复杂的模型，手工进行更新是一件很痛苦的事情（而且经常容易出错）。 深度学习框架通过自动计算导数，即自动微分（automatic differentiation）来加快求导。 实际中，根据设计好的模型，系统会构建一个计算图（computational graph）， 来跟踪计算是哪些数据通过哪些操作组合起来产生输出。 自动微分使系统能够随后反向传播梯度。 这里，反向传播backward()（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。 12345678910111213x = torch.arange(4.0)x.requires_grad_(True) # 等价于x=torch.arange(4.0,requires_grad=True) print(f&quot;x的默认梯度{x.grad}&quot;) # 默认值是Noney1 = x @ xy1.backward()print(f&quot;y1 = x@x的梯度{x.grad}&quot;)x.grad.zero_() # 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值y2 = x * xy2.sum().backward() # 求梯度需要对标量print(f&quot;y2 = x*x的梯度{x.grad}&quot;)x.grad.zero_() 分离变量detach() :有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设y是作为x的函数计算的，而z则是作为y和x的函数计算的。 想象一下，我们想计算z关于x的梯度，但由于某种原因，希望将y视为一个常数， 并且只考虑到x在y被计算后发挥的作用。 这里可以分离y来返回一个新变量u，该变量与y具有相同的值，但丢弃计算图中如何计算y的任何信息。换句话说，梯度不会向后流经u到x。 1234567x=torch.arange(4.0,requires_grad=True) # 创建带梯度的张量 x.grad.zero_()y = x * xu = y.detach() # 分离y来返回一个新变量uz = u * x z.sum().backward() # 只计算z关于x的梯度x.grad == u # 梯度是否等于分离变量u 关于概率关于概率学中随机性一词的哲学争论终止于1933年，科尔莫戈罗夫提出概率论公理系统。 有了该公理系统，我们可以避免任何关于随机性的哲学争论； 相反，我们可以用数学语言严格地推理。笔者认为概率论最重要的成果就是两条主线中心极限定理和贝叶斯定理 中心极限定理：可由特征函数结合斯特林公式推导。对于独立同分布（i.i.d., 即 independent and identically distributed）、且数学期望和方差有限的随机变量序列的标准化和以标准正态分布为极限 贝叶斯定理：从条件概率出发，Bayes定理（Bayes’ theorem）$P(A|B)=\\dfrac{P(B|A)P(A)}{P(B)}$ 我们实战演练一下！ 假设一个医生对患者进行艾滋病病毒（HIV）测试。 这个测试是相当准确的，如果患者健康但测试显示他患病，这个概率只有1%； 如果患者真正感染HIV，它永远不会检测不出。 我们使用$D_1$来表示诊断结果（如果阳性，则为1，如果阴性，则为0）， $H$来表示感染艾滋病病毒的状态（如果阳性，则为1，如果阴性，则为0）。 下面是两种不同的检测方法，请在先验知识$P(H=1)=0.0015$尝试计算$P(H=1|D_1=1)$和$P(H=1|D_1=1,D_2=1)$ 诊断结果 H=1 H=0 $P(D_1=1|H)$ 1 0.01 $P(D_1=0|H)$ 0 0.99 诊断结果 H=1 H=0 $P(D_2=1|H)$ 0.98 0.03 $P(D_2=0|H)$ 0.02 0.97","link":"/2026/01/19/pytorch-trick/"}],"tags":[{"name":"大语言模型","slug":"大语言模型","link":"tagsd:\\Blogs\\source\\img\\wechat.jpg/大语言模型/"},{"name":"算法","slug":"算法","link":"tagsd:\\Blogs\\source\\img\\wechat.jpg/算法/"},{"name":"面经","slug":"面经","link":"tagsd:\\Blogs\\source\\img\\wechat.jpg/面经/"},{"name":"数据分析","slug":"数据分析","link":"tagsd:\\Blogs\\source\\img\\wechat.jpg/数据分析/"},{"name":"深度学习","slug":"深度学习","link":"tagsd:\\Blogs\\source\\img\\wechat.jpg/深度学习/"}],"categories":[{"name":"LLM学习系列","slug":"LLM学习系列","link":"/categories/LLM%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97/"},{"name":"实习","slug":"实习","link":"/categories/%E5%AE%9E%E4%B9%A0/"},{"name":"统计学系列","slug":"统计学系列","link":"/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E7%B3%BB%E5%88%97/"},{"name":"基本功系列","slug":"基本功系列","link":"/categories/%E5%9F%BA%E6%9C%AC%E5%8A%9F%E7%B3%BB%E5%88%97/"}],"pages":[]}