{"posts":[{"title":"LLMçš„å‰ä¸–ä»Šç”Ÿ","text":"æ—¶ä»£çš„å‘å±•é€Ÿåº¦åœ¨å˜çš„è¶Šæ¥è¶Šå¿«ã€‚1975å¹´ï¼ŒåŠå¯¼ä½“èŠ¯ç‰‡é¢†åŸŸçš„è‘—åè®ºæ–­â€”â€”â€”â€”æ‘©å°”å®šå¾‹è¢«æå‡ºï¼ˆé›†æˆç”µè·¯ä¸Šå¯å®¹çº³çš„æ™¶ä½“ç®¡æ•°ç›®ï¼Œæ¯éš”çº¦ä¸¤å¹´ä¾¿ä¼šå¢åŠ ä¸€å€ï¼‰ã€‚åŠå¯¼ä½“è¡Œä¸šå¤§è‡´æŒ‰ç…§æ‘©å°”å®šå¾‹å‘å±•äº†åŠä¸ªå¤šä¸–çºªï¼Œå¸¦æ¥äº†ç”µè„‘ï¼Œäº’è”ç½‘ï¼Œæ‰‹æœºç­‰äº‹ç‰©ã€‚å¦‚ä»Šï¼Œä»2017å¹´å¼€å§‹åˆ°2025å¹´å³å°†ç»“æŸï¼Œä»NLPï¼ˆè‡ªç„¶è¯­è¨€å¤„ç†ï¼‰åˆ°LLMï¼ˆé€šç”¨å¤§è¯­è¨€æ¨¡å‹ï¼‰ï¼Œæ·±åº¦å­¦ä¹ æŠ€æœ¯çš„å‘å±•ï¼Œå¹¶è¡Œè®¡ç®—GPUçš„è¿›æ­¥ï¼Œæ™ºèƒ½æ—¶ä»£è¿˜ä¼šéœ€è¦åŠä¸ªä¸–çºªçš„æ—¶é—´æ‰èƒ½åˆ°è¾¾å—ï¼Ÿ ä¸æ˜¯ï¼Œèº«å¤„æŠ€æœ¯çˆ†ç‚¸çš„æ—¶åˆ»ï¼Œæ—¶ä»£å‘å±•é€Ÿåº¦åªä¼šè¶Šæ¥è¶Šå¿«ï¼Œç›´åˆ°é‡åˆ°äººæœ¬å›°å¢ƒã€‚æœ¬æ–‡æ—¨åœ¨ä»‹ç»LLMå¤§è¯­è¨€æ¨¡å‹æŠ€æœ¯æ˜¯å¦‚ä½•ä»æ›¾ç»çš„NLPä»»åŠ¡ï¼Œåˆ°é€šç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼ˆPre-Trained Modelï¼‰ï¼Œå†åˆ°æœªæ¥å±•æœ›çš„è‡ªå­¦ä¹ æ¨¡å‹ï¼ˆæˆ–è€…æ˜¯AI Agentï¼‰ã€‚ è‡ªç„¶è¯­è¨€å¤„ç†ï¼šNLPï¼ˆNatural Language Processï¼‰ä»»åŠ¡å¾ˆå¥½ç†è§£ï¼Œå°±æ˜¯è¯•å›¾è®©æœºå™¨æŒæ¡äººç±»ä½¿ç”¨çš„è‡ªç„¶è¯­è¨€ï¼Œè¯¸å¦‚è¯­éŸ³è¯†åˆ«ï¼Œå¤–è¯­ç¿»è¯‘ï¼Œèˆ†æƒ…ç›‘æ§ç­‰ä»»åŠ¡éƒ½æ˜¯å…¶ä¸­çš„ä¸€éƒ¨åˆ†ã€‚NLPæœ‰2ä¸ªæ ¸å¿ƒçš„ä»»åŠ¡ï¼šåˆ†åˆ«æ˜¯ã€è‡ªç„¶è¯­è¨€ç†è§£â€”NL Understandã€‘å’Œã€è‡ªç„¶è¯­è¨€ç”Ÿæˆâ€”NL Generateã€‘ã€‚ NLUæ˜¯æ‰€æœ‰æ”¯æŒæœºå™¨ç†è§£æ–‡æœ¬å†…å®¹çš„æ–¹æ³•æ¨¡å‹æˆ–ä»»åŠ¡çš„æ€»ç§°ï¼Œå³èƒ½å¤Ÿè¿›è¡Œå¸¸è§çš„æ–‡æœ¬åˆ†ç±»ã€åºåˆ—æ ‡æ³¨ã€ä¿¡æ¯æŠ½å–ç­‰ä»»åŠ¡ã€‚ç®€å•æ¥è¯´ï¼Œæ˜¯æŒ‡è‡ªç„¶è¯­è¨€ç†è§£çš„è¿ç”¨å¸Œæœ›æœºå™¨äººèƒ½å¤Ÿåƒäººä¸€æ ·ï¼Œå…·å¤‡æ­£å¸¸çš„è¯­è¨€ç†è§£èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œæ„å›¾è¯†åˆ«å’Œå®ä½“æå–çš„å…³é”®æŠ€èƒ½ã€‚ NLG ï¼ˆè‡ªç„¶è¯­è¨€ç”Ÿæˆï¼‰æ˜¯NLPçš„å¦ä¸€é¡¹æ ¸å¿ƒä»»åŠ¡ï¼Œä¸»è¦ç›®çš„æ˜¯é™ä½äººç±»å’Œæœºå™¨ä¹‹é—´çš„æ²Ÿé€šé¸¿æ²Ÿï¼Œå°†éè¯­è¨€æ ¼å¼çš„æ•°æ®è½¬æ¢æˆäººç±»å¯ä»¥ç†è§£çš„è¯­è¨€æ ¼å¼ã€‚","link":"/2025/12/14/Summary/"},{"title":"LLMä¸­çš„LoRAå®ç°","text":"AIæ—¶ä»£ï¼Œè¯¸å¦‚OpenAIã€Googleã€é˜¿é‡Œå·´å·´ã€Metaç­‰å¤§å…¬å¸å¯ä»¥ä¾é ææ€–çš„è®¡ç®—èµ„æºæ¥è®­ç»ƒåŸºåº§å¤§æ¨¡å‹ï¼Œä½†å¯¹äºå¯¹èµ„æºæœ‰é™çš„ä¼ä¸šæˆ–è¯¾é¢˜ç»„æ¥è¯´ï¼Œå¦‚ä½•é«˜æ•ˆã€å¿«é€Ÿå¯¹æ¨¡å‹è¿›è¡Œé¢†åŸŸæˆ–ä»»åŠ¡çš„å¾®è°ƒï¼Œä»¥ä½æˆæœ¬åœ°ä½¿ç”¨ LLM å®Œæˆç›®æ ‡ä»»åŠ¡ï¼Œæ˜¯éå¸¸é‡è¦çš„ã€‚ æ¶Œç°èƒ½åŠ›åŒºåˆ† LLM ï¼ˆLarge Language Modelï¼‰ä¸ä¼ ç»Ÿ PLMï¼ˆPre-trained Language Modelï¼‰ æœ€æ˜¾è‘—çš„ç‰¹å¾å³æ˜¯ LLM å…·å¤‡æ¶Œç°èƒ½åŠ› ã€‚æ¶Œç°èƒ½åŠ›æ˜¯æŒ‡åŒæ ·çš„æ¨¡å‹æ¶æ„ä¸é¢„è®­ç»ƒä»»åŠ¡ä¸‹ï¼ŒæŸäº›èƒ½åŠ›åœ¨å°å‹æ¨¡å‹ä¸­ä¸æ˜æ˜¾ï¼Œä½†åœ¨å¤§å‹æ¨¡å‹ä¸­ç‰¹åˆ«çªå‡ºã€‚å¯ä»¥ç±»æ¯”åˆ°ç‰©ç†å­¦ä¸­çš„ç›¸å˜ç°è±¡ï¼Œæ¶Œç°èƒ½åŠ›çš„æ˜¾ç°å°±åƒæ˜¯æ¨¡å‹æ€§èƒ½éšç€è§„æ¨¡å¢å¤§è€Œè¿…é€Ÿæå‡ï¼Œè¶…è¿‡äº†éšæœºæ°´å¹³ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬å¸¸è¯´çš„é‡å˜å¼•èµ·äº†è´¨å˜ã€‚ ä¸€èˆ¬æ¥è¯´ï¼ŒLLM æŒ‡åŒ…å«æ•°ç™¾äº¿ï¼ˆæˆ–æ›´å¤šï¼‰å‚æ•°çš„è¯­è¨€æ¨¡å‹ï¼Œå®ƒä»¬å¾€å¾€åœ¨æ•° T token è¯­æ–™ä¸Šé€šè¿‡å¤šå¡åˆ†å¸ƒå¼é›†ç¾¤è¿›è¡Œé¢„è®­ç»ƒï¼Œå…·å¤‡è¿œè¶…å‡ºä¼ ç»Ÿé¢„è®­ç»ƒæ¨¡å‹çš„æ–‡æœ¬ç†è§£ä¸ç”Ÿæˆèƒ½åŠ›ã€‚ä¸è¿‡ï¼Œéšç€ LLM ç ”ç©¶çš„ä¸æ–­æ·±å…¥ï¼Œå¤šç§å‚æ•°å°ºå¯¸çš„ LLM é€æ¸ä¸°å¯Œï¼Œå¹¿ä¹‰çš„ LLM ä¸€èˆ¬è¦†ç›–äº†ä»åäº¿å‚æ•°ï¼ˆå¦‚ Qwen-1.5Bï¼‰åˆ°åƒäº¿å‚æ•°ï¼ˆå¦‚ Grok-314Bï¼‰çš„æ‰€æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ã€‚åªè¦æ¨¡å‹å±•ç°å‡ºæ¶Œç°èƒ½åŠ›ï¼Œå³åœ¨ä¸€ç³»åˆ—å¤æ‚ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè¿œè¶…ä¼ ç»Ÿé¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚ BERTã€T5ï¼‰çš„èƒ½åŠ›ä¸æ½œåŠ›ï¼Œéƒ½å¯ä»¥ç§°ä¹‹ä¸º LLMã€‚ åœ¨è¿™ç§ææ€–çš„å‚æ•°è§„æ¨¡ä¸‹ï¼Œè°ƒæ•™å¤§è¯­è¨€æ¨¡å‹ï¼ˆæ›´æ–°æ¨¡å‹å‚æ•°ï¼‰æ‰€éœ€çš„ç”µåŠ›èµ„æºï¼Œæ˜¾å¡èµ„æºéƒ½æ˜¯ææ€–çš„ã€‚ é¦–å…ˆéœ€è¦å›é¡¾ä¸€ä¸‹ï¼Œå¯¹ LLM è¿›è¡Œé¢„è®­ç»ƒå’Œè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒçš„æ ¸å¿ƒå·®å¼‚åœ¨äºä»€ä¹ˆã€‚ ç›®å‰æˆå‹çš„ LLM ä¸€èˆ¬é€šè¿‡ Pretrain-SFT-RLHF ä¸‰é˜¶æ®µæ¥è®­ç»ƒï¼ŒPretrain é˜¶æ®µï¼Œä¼šå¯¹æµ·é‡æ— ç›‘ç£æ–‡æœ¬è¿›è¡Œè‡ªç›‘ç£å»ºæ¨¡ï¼Œæ¥å­¦ä¹ æ–‡æœ¬è¯­ä¹‰è§„åˆ™å’Œæ–‡æœ¬ä¸­çš„ä¸–ç•ŒçŸ¥è¯†ï¼›SFT é˜¶æ®µï¼Œä¸€èˆ¬é€šè¿‡å¯¹ Pretrain å¥½çš„æ¨¡å‹è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œå³è®­ç»ƒæ¨¡å‹æ ¹æ®ç”¨æˆ·æŒ‡ä»¤å®Œæˆå¯¹åº”ä»»åŠ¡ï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿéµå¾ªç”¨æˆ·æŒ‡ä»¤ï¼Œæ ¹æ®ç”¨æˆ·æŒ‡ä»¤è¿›è¡Œè§„åˆ’ã€è¡ŒåŠ¨å’Œè¾“å‡ºã€‚å› æ­¤ï¼ŒPretrain å’Œ SFT å‡ä½¿ç”¨ CLM ï¼ˆCausal Language Modelï¼‰å»ºæ¨¡ï¼Œå…¶æ ¸å¿ƒå·®å¼‚åœ¨äºï¼ŒPretrain ä½¿ç”¨æµ·é‡æ— ç›‘ç£æ–‡æœ¬è¿›è¡Œè®­ç»ƒï¼Œæ¨¡å‹ç›´æ¥å¯¹æ–‡æœ¬æ‰§è¡Œâ€œé¢„æµ‹ä¸‹ä¸€ä¸ª tokenâ€çš„ä»»åŠ¡ï¼›è€Œ SFT ä½¿ç”¨æ„å»ºæˆå¯¹çš„æŒ‡ä»¤å¯¹æ•°æ®ï¼Œæ¨¡å‹æ ¹æ®è¾“å…¥çš„æŒ‡ä»¤ï¼Œå»ºæ¨¡åç»­çš„è¾“å‡ºã€‚åæ˜ åˆ°å…·ä½“çš„è®­ç»ƒå®ç°ä¸Šï¼ŒPretrain ä¼šå¯¹å…¨éƒ¨ text è¿›è¡Œ loss è®¡ç®—ï¼Œè¦æ±‚æ¨¡å‹å¯¹æ•´ä¸ªæ–‡æœ¬å®ç°å»ºæ¨¡é¢„æµ‹ï¼›è€Œ SFT ä»…å¯¹è¾“å‡ºè¿›è¡Œ loss è®¡ç®—ï¼Œä¸è®¡ç®—æŒ‡ä»¤éƒ¨åˆ†çš„ lossã€‚ã€‚ä½†æ˜¯ï¼Œç”±äº LLM å‚æ•°é‡å¤§ï¼Œè®­ç»ƒæ•°æ®å¤šï¼Œé€šè¿‡ä¸Šè¿°æ–¹å¼å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼ˆä¸»è¦æŒ‡ SFT åŠ RLHFï¼‰éœ€è¦è°ƒæ•´æ¨¡å‹å…¨éƒ¨å‚æ•°ï¼Œèµ„æºå‹åŠ›éå¸¸å¤§ã€‚ è¿™æ—¶å€™ï¼Œå¾®è°ƒæŠ€æœ¯åº”è¿è€Œç”Ÿã€‚ å¾®è°ƒï¼ˆFine Tuningï¼‰å¯¹æ‰€æœ‰å‚æ•°è¿›è¡Œè®­ç»ƒè°ƒæ•´çš„å…¨é‡å¾®è°ƒå¤ªè¿‡æ˜‚è´µï¼Œéœ€è¦å¤§é‡çš„GPUæ˜¾å­˜ï¼Œä¸ªäººçš„æ˜¾å¡ä¸Šå¾ˆéš¾å®ç°ï¼Œå› æ­¤å„ç§å‚æ•°é«˜æ•ˆï¼ˆParameter-Efficientï¼‰çš„æ–¹æ³•å±‚å‡ºä¸ç©·ï¼Œæœ€å—å¤§å®¶æ¬¢è¿çš„å°±æ˜¯ LoRAæ–¹æ³•ã€ŠLoRA: Low-Rank Adaptation of Large Language Modelsã€‹ LoRA æœ‰å¾ˆå¤šçš„ä¼˜ç‚¹ï¼ŒèŠ‚çº¦æ˜¾å­˜ï¼Œè®­ç»ƒå¿«ï¼Œæ•ˆæœæŸå¤±è¾ƒå°ï¼ˆç›¸å¯¹äºå…¨å‚æ•°å¾®è°ƒï¼‰ï¼Œæ¨ç†çš„æ—¶å€™ä¸å¢åŠ è€—æ—¶ï¼Œå¯ä»¥åšä¸€ä¸ªæ’å…¥å¼ç»„ä»¶ä½¿ç”¨ã€‚ç¼ºç‚¹å½“ç„¶ä¹Ÿæœ‰ï¼Œé‚£å°±æ˜¯è¿˜æ˜¯ä¼šæœ‰ä¸€äº›æ•ˆæœçš„æŸå¤±ï¼ˆç¬‘ï¼‰ã€‚ ä¸å…¶ä»–é«˜æ•ˆå¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒLoRA å­˜åœ¨ä»¥ä¸‹ä¼˜åŠ¿ï¼š ä»¥é’ˆå¯¹ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡æ„å»ºå°å‹ LoRA æ¨¡å—ï¼Œä»è€Œåœ¨å…±äº«é¢„è®­ç»ƒæ¨¡å‹å‚æ•°åŸºç¡€ä¸Šæœ‰æ•ˆåœ°åˆ‡æ¢ä¸‹æ¸¸ä»»åŠ¡ã€‚ LoRA ä½¿ç”¨è‡ªé€‚åº”ä¼˜åŒ–å™¨ï¼ˆAdaptive Optimizerï¼‰ï¼Œä¸éœ€è¦è®¡ç®—æ¢¯åº¦æˆ–ç»´æŠ¤å¤§å¤šæ•°å‚æ•°çš„ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œè®­ç»ƒæ›´æœ‰æ•ˆã€ç¡¬ä»¶é—¨æ§›æ›´ä½ã€‚ LoRA ä½¿ç”¨ç®€å•çš„çº¿æ€§è®¾è®¡ï¼Œåœ¨éƒ¨ç½²æ—¶å°†å¯è®­ç»ƒçŸ©é˜µä¸å†»ç»“æƒé‡åˆå¹¶ï¼Œä¸å­˜åœ¨æ¨ç†å»¶è¿Ÿã€‚ LoRA ä¸å…¶ä»–æ–¹æ³•æ­£äº¤ï¼Œå¯ä»¥ç»„åˆã€‚ å› æ­¤ï¼ŒLoRA æˆä¸ºç›®å‰é«˜æ•ˆå¾®è°ƒ LLM çš„ä¸»æµæ–¹æ³•ï¼Œå°¤å…¶æ˜¯å¯¹äºèµ„æºå—é™ã€æœ‰ç›‘ç£è®­ç»ƒæ•°æ®å—é™çš„æƒ…å†µä¸‹ï¼ŒLoRA å¾®è°ƒå¾€å¾€ä¼šæˆä¸º LLM å¾®è°ƒçš„é¦–é€‰æ–¹æ³•ã€‚ LoRAæ ¸å¿ƒåŸç†LoRAä»£ç Pytorchå®ç°","link":"/2025/12/10/LoRA/"},{"title":"å½“ä»£å­”ä¹™å·±-Attentionæœºåˆ¶çš„å››ç§å†™æ³•","text":"å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‘å±•æ–¹å…´æœªè‰¾ï¼Œä»ChatGPTæ¨ªç©ºå‡ºä¸–æ¨ªæ‰«è‹±æ–‡äº’è”ç½‘ï¼Œåˆ°DeepSeekæƒŠè®¶ä¸–äººå®£å‘Šä½æˆæœ¬LLMå¯è¡Œæ€§ï¼Œ2025å¹´çŸ­çŸ­ä¸€å¹´ä¾¿é€ å°±äº†äººå·¥æ™ºèƒ½é¢†åŸŸå¦‚æ­¤éœ‡æ’¼çš„ä¼°å€¼ç¹è£ï¼ˆor bubbleï¼‰ï¼Œä¸å¾—ä¸è®©äººæ„Ÿæ…¨ã€Šä¸‰ä½“ã€‹ä¸­ â€œæŠ€æœ¯çˆ†ç‚¸â€ ä¸€è¯æå‡ºçš„å…ˆè§æ€§ã€‚ ä¸€åˆ‡ä»ã€ŠAttention is All You Needã€‹å¼€å§‹ã€‚è¿™ç¯‡è®ºæ–‡å½»åº•æŠ›å¼ƒäº†é•¿åºåˆ—é¢„æµ‹çš„LSTMå’ŒRNNæ¨¡å‹ï¼Œåœ¨å­¦æœ¯ç•Œæœ‰ç€åˆ’æ—¶ä»£æ„ä¹‰ï¼›æ›´åœ¨ä¼ æ’­å­¦ä¸Šå±•ç¤ºäº†ä¸€ä¸ªå¥½çš„åå­—åˆ°åº•æœ‰å¤šé‡è¦ã€‚ ä»¥ä¸‹å†…å®¹å¤§é‡å‚è€ƒchaofaç”¨ä»£ç æ‰“ç‚¹é…±æ²¹çš„åšå®¢ åœ¨AIç›¸å…³çš„é¢è¯•ä¸­ï¼Œç»å¸¸ä¼šè¦æ±‚å¯¹Transformeræ¶æ„æœ‰è¶³å¤Ÿçš„ç†è§£ï¼Œé€šå¸¸é€šè¿‡æ‰‹å†™self-attentionæœºåˆ¶æ¥éªŒè¯ã€‚è¿™é‡Œç»™å‡ºäº†Attentionæœºåˆ¶ä»ç®€åˆ°ç¹çš„å››ç§å†™æ³• ä»€ä¹ˆæ˜¯$Self-Attention$ï¼Ÿ$Attention$ æœºåˆ¶é€šå¸¸åœ¨å·²å®ç° $Embedding$ + ä½ç½®ç¼–ç åè¿›è¡Œã€‚å¼•å…¥ $Attention$ æœºåˆ¶ä¸»è¦æ˜¯ç”±äº $Embedding$ å’Œä½ç½®ç¼–ç ä¾ç„¶ä¸å¤Ÿè¡¨ç¤ºæ‰€æœ‰çš„ä¿¡æ¯ã€‚ ä¸¾ä¸ªä¾‹å­ â€œä½ ä¸è¦æ‰“äººâ€ å’Œ â€œä½ å¿«å»æ‰“æ°´â€ ä¸­ â€œæ‰“â€ å­—å«ä¹‰æˆªç„¶ä¸åŒï¼Œä½†ä»…é  $Embedding$ å’Œä½ç½®ç¼–ç ä¸¤è€…å¹¶ä¸èƒ½åŒºåˆ«ï¼Œè¿™å°±éœ€è¦ç»“åˆä¸Šä¸‹æ–‡æ¥å¢åŠ æ›´å¤šä¿¡æ¯ï¼Œ$Attention$ å°±æ˜¯è¿™ç§ç»“åˆçš„æ–¹å¼ã€‚ $$SelfAttention(X)=softmax(\\frac{Q\\cdot K}{\\sqrt{d}})\\cdot V$$ $Q = K = V = W_{Q/K/V} \\cdot X$ï¼Œå…¶ä¸­$Qï¼ŒKï¼ŒV$åˆ†åˆ«å¯¹åº”ä¸åŒï¼ˆå½¢çŠ¶ä¹Ÿä¸åŒï¼‰çš„æƒé‡çŸ©é˜µW matmulå’Œ@ç¬¦å·æ˜¯ä¸€æ ·çš„ä½œç”¨ï¼ŒçŸ©é˜µä¹˜æ³• ä¸ºä»€ä¹ˆè¦é™¤ä»¥$\\sqrt{d}$ï¼šä¸€æ˜¯é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±ï¼ˆå‚è€ƒKaiming Initializationå‚æ•°åˆå§‹åŒ–ï¼‰ï¼›äºŒæ˜¯ä¸ºäº†è®©QKçš„å†…ç§¯åˆ†å¸ƒä¿æŒå’Œè¾“å…¥ä¸€æ · çˆ±å› æ–¯å¦æ–¹ç¨‹è¡¨è¾¾å¼ç”¨æ³•ï¼štorch.einsum(â€œbqd,bkd-&gt;bqkâ€,X,X).shape ä¸ºä»€ä¹ˆä½¿ç”¨softmax?æ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ¬è´¨ä¸Šæ˜¯æƒ³æ‰¾ä¸æŸ¥è¯¢å‘é‡æœ€ç›¸å…³çš„éšå‘é‡åºåˆ—ï¼Œå®é™…ä¸Šæ˜¯æƒ³æ‰¾ï¼Œonehot(argmax)ï¼Œæ¯”å¦‚çš„ä»»åŠ¡æ˜¯ä¸­è§†é‡ä¸­æ‰¾è‹¹æœï¼Œç›´æ¥ä¾æ®è‹¹æœçš„ç‰¹å¾æŸ¥æ‰¾è‹¹æœã€‚ä½†æ˜¯è¿™ä¸ªæ±‚è§£æ˜¯æ— æ³•è®¡ç®—æ¢¯åº¦ï¼Œæ‰€ä»¥æ‰ç”¨onehot(argmax)çš„å…‰æ»‘ç‰ˆæœ¬softmaxï¼Œä»¥è·å¾—è‰¯å¥½çš„æ¢¯åº¦ç‰¹æ€§ã€‚æ˜¯ä¸€ç§æ•°å­¦ä¸Šçš„å¦¥å X.repeat(1,1,3)è¡¨ç¤ºåœ¨ä¸åŒç»´åº¦è¿›è¡Œrepeatæ“ä½œï¼Œä¹Ÿå¯ä»¥ç”¨tensor.expandæ“ä½œ 12345import mathimport warningsimport torchimport torch.nn as nnwarnings.filterwarnings(action='ignore') ç¬¬ä¸€ç§å†™æ³•ï¼šç®€æ´123456789101112131415161718192021222324class SelfAttention_1(nn.Module): # çˆ¶ç±»æ˜¯pytorchä¸­çš„æ ¸å¿ƒModule def __init__(self, hidden_dim): super(SelfAttention_1,self).__init__()# è°ƒç”¨çˆ¶ç±»çš„æ„é€ æ–¹æ³•ï¼Œå¹¶ä¼ é€’æ‰€æœ‰å‚æ•° self.hidden_dim = hidden_dim self.Q = nn.Linear(hidden_dim, hidden_dim) self.K = nn.Linear(hidden_dim, hidden_dim) self.V = nn.Linear(hidden_dim, hidden_dim) def forward(self,X): # X.shape = [batch_size, seq_len, hidden_dim] # æ ¹æ®è¾“å…¥Xçš„å½¢çŠ¶åˆ›å»ºQ,K,V Q = self.Q(X) K = self.K(X) V = self.V(X) # Kéœ€è¦æ”¹ä¸ºshape = [batch_size, seq_len, hidden_dim] QK = Q @ K.transpose(-1,-2)# K.transpose(1,2) softmax_QK = torch.softmax( QK / math.sqrt(self.hidden_dim), dim = -1) # åœ¨æœ€åä¸€ç»´ä¸Šè®¡ç®—softmax output = softmax_QK @ V return output X = torch.rand(3,2,4)net = SelfAttention_1(4)net(X) ç¬¬äºŒç§å†™æ³•ï¼šæ•ˆç‡ä¼˜åŒ–12345678910111213141516171819class SelfAttention_2(nn.Module): def __init__(self,hidden_dim): super().__init__() self.hidden_dim = hidden_dim self.proj = nn.Linear(hidden_dim, hidden_dim*3) #ç›´æ¥å°†Qï¼ŒKï¼ŒVç½‘ç»œåˆå¹¶ self.output_proj = nn.Linear(hidden_dim,hidden_dim) def forward(self,X): # X.shape = [batch_size, seq_len, hidden_dim] QKV = self.proj(X) # [batch_size, seq_len, dim*3] # torch.split(x, chunks_size, dim) Q,K,V = torch.split(QKV , self.hidden_dim , dim =-1) QK = torch.softmax(Q @ K.transpose(-1,-2) / math.sqrt(self.hidden_dim), dim = -1) output = QK @ V return self.output_proj(output) X = torch.rand(3,2,4)net = SelfAttention_2(4)net(X) ç¬¬ä¸‰ç§å†™æ³•ï¼šåŠ ä¸Šç»†èŠ‚attention_maskï¼Œæ›´å®Œæ•´ Attentionè®¡ç®—æ—¶æœ‰dropoutå±‚ï¼Œè®¾ç½®çš„ä½ç½®å¾ˆæœ‰è¶£ Attentionè®¡ç®—æ—¶ä¼šåŠ å…¥attention_maskï¼Œå› ä¸ºæ ·æœ¬ä¼šè¿›è¡Œpaddingæ“ä½œï¼ˆä¸åŒé•¿åº¦çš„åºåˆ—è¾“å…¥ï¼Œéœ€è¦ç›¸åŒçš„ç»´åº¦Attention_mapï¼Œè¿™è¦æ±‚åœ¨QKVä»¥å¤–è¿›è¡Œmaskå¤„ç†ï¼‰ MultiHeadAttentionè¿‡ç¨‹ä¸­ï¼Œé™¤QKVä¸‰ä¸ªçŸ©é˜µå¤–ï¼Œè¿˜æœ‰outputå¯¹åº”çš„æŠ•å½±çŸ©é˜µ 12345678910111213141516171819202122232425262728293031323334353637383940414243class SelfAttention_3(nn.Module): def __init__(self, hidden_dim): super().__init__() self.hidden_dim = hidden_dim self.proj = nn.Linear(hidden_dim, hidden_dim*3) self.att_drop = nn.Dropout(0.1) self.output_proj = nn.Linear(hidden_dim, hidden_dim) def forward(self, X,attention_mask = None): # attention_mask = [batch_size, seq_len] # X.shape = [batch_size, seq_len, hidden_dim] QKV = self.proj(X) Q, K, V = torch.split(QKV,self.hidden_dim, dim=-1) QK = Q @ K.transpose(-1,-2) / math.sqrt(self.hidden_dim) if attention_mask is not None: # maskçš„ç›®çš„ï¼šå±è”½æ‰ä¸è¯¥å‚ä¸æ³¨æ„åŠ›è®¡ç®—çš„ä½ç½®ï¼ˆæ— æ•ˆpaddingã€æœªæ¥tokenå±è”½ã€æ— æ•ˆåŒºåŸŸï¼‰ # masked_fill(condition, value) PyTorch çš„æ–¹æ³•ï¼šå°† condition ä¸º True çš„ä½ç½®æ›¿æ¢æˆ value # value = float(&quot;-1e20&quot;)æ˜¯å› ä¸ºåœ¨softmaxåå€¼æ¥è¿‘äº0 QK = QK.masked_fill(attention_mask == 0, float(&quot;-1e20&quot;)) QK = torch.softmax(QK,dim = -1) # è¿™é‡Œåœ¨ BERTä¸­çš„å®˜æ–¹ä»£ç ä¹Ÿè¯´å¾ˆå¥‡æ€ªï¼Œä½†æ˜¯åŸæ–‡ä¸­è¿™ä¹ˆç”¨äº†ï¼Œæ‰€ä»¥ç»§æ‰¿äº†ä¸‹æ¥ # ï¼ˆç”¨äº output åé¢ä¼šæ›´ç¬¦åˆç›´è§‰ï¼Ÿï¼‰ QK= self.att_drop(QK) output = QK @ V return self.output_proj(output) X = torch.rand(3,4,2)b = torch.tensor( [ [1, 1, 1, 0], [1, 1, 0, 0], [1, 0, 0, 0], ])print(f&quot;maskçš„åˆå§‹å½¢çŠ¶{b.shape}&quot;)# unsqueeze(dim)è¡¨ç¤ºåœ¨æŒ‡å®šä½ç½®å¢åŠ ç»´åº¦mask = b.unsqueeze(dim=1).repeat(1, 4, 1)print(f&quot;maskï¼š{mask}&quot;)net = SelfAttention_3(2)net(X,mask) ç¬¬å››ç§å†™æ³•ï¼šä»å•å¤´åˆ°å¤šå¤´ä¸€èˆ¬åœ¨å®é™…ä¸Šçš„è®­ç»ƒè¿‡ç¨‹ä¸­éƒ½ä¼šä½¿ç”¨ Multi Head, å¤šå¤´æ³¨æ„åŠ›çš„å®ç°æ ¸å¿ƒæ˜¯torch.permute()å‡½æ•°ï¼ˆæ”¯æŒé«˜ç»´è½¬ç½®æ“ä½œï¼‰, è€Œä¸”å…¶å®ä¹Ÿä»…ä»…æ˜¯æ¯ä¸ª Head åšå®Œ Self-Attention å¾—åˆ°ç»“æœä¹‹åï¼Œè¿›è¡Œæ‹¼æ¥ï¼Œç„¶åè¿‡ä¸€ä¸ª output æŠ•å½±çŸ©é˜µã€‚æœ¬è´¨ä¸Šæ˜¯ä¸€ç§é›†æˆå­¦ä¹ ï¼Œä¸åˆ¸å•†ç ”æŠ¥é‡Œçš„å› å­æ‰“åˆ†ç›¸æ¯”è¿˜æ˜¯æ›´é«˜æ˜ä¸€ç‚¹ã€‚ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667class MultiHeadAttention(nn.Module): def __init__(self,hidden_dim, n_head): super().__init__() self.n_head = n_head self.hidden_dim = hidden_dim self.head_dim = hidden_dim // n_head # æ³¨æ„hidden_dim= head_dim * n_head # å°†æœ€åä¸€ä¸ªç»´åº¦è¿›è¡Œæ‹†åˆ†ï¼Œä¸€ä¸ªè¯¸è‘›äº®æ‹†æˆn_headä¸ªè‡­çš®åŒ  self.Q = nn.Linear(hidden_dim,hidden_dim) self.K = nn.Linear(hidden_dim,hidden_dim) self.V = nn.Linear(hidden_dim,hidden_dim) self.att_dropout = nn.Dropout(0.1) self.proj = nn.Linear(hidden_dim, hidden_dim) def forward(self,X,attention_mask=None): # attention_mask = [batch_size, seq_len] # X.shape = [batch_size, seq_len, hidden_dim] batch_size, seq_len, _ = X.size() Q = self.Q(X) K = self.Q(X) V = self.Q(X) # ä¸‹é¢reshapeæ”¹å˜Qï¼ŒKï¼ŒVçš„shapeä¸º[batch_size,seq_len, n_head, head_dim] # å¤šå¤´æ³¨æ„åŠ›è®­ç»ƒå»ºç«‹åœ¨è®¤çŸ¥ï¼šè®­ç»ƒå¤šä¸ªå°æ¨¡å‹éš¾åº¦ä½äºè®­ç»ƒä¸€ä¸ªå¤§æ¨¡å‹ Q_state = Q.reshape(batch_size, seq_len, self.n_head, self.head_dim).permute(0,2,1,3) K_state = K.reshape(batch_size, seq_len, self.n_head, self.head_dim).permute(0,2,1,3) V_state = V.reshape(batch_size, seq_len, self.n_head, self.head_dim).permute(0,2,1,3) QK = Q_state @ K_state.transpose(-1,-2) / math.sqrt(self.head_dim) if attention_mask is not None: QK = QK.masked_fill(attention_mask == 0, float(&quot;-1e20&quot;)) # æˆ–è€…æœ‰äº›å®ç°æ˜¯ attention_mask == 0 æ—¶ maskï¼Œçœ‹å…·ä½“ä»£ç  # å¯¹æœ€åä¸€ä¸ªç»´åº¦head_dimè¿›è¡Œsoftmaxå¹³æ»‘åŒ– QK = torch.softmax(QK, dim = 3) QK = self.att_dropout(QK) print(QK.shape) print(V_state.shape) output = QK @ V_state print(output.shape) output = output.reshape(batch_size,seq_len, -1) return self.proj(output) #åœ¨ PyTorch çš„ MultiHeadAttentionï¼ˆæˆ–è‡ªå·±å®ç°çš„ï¼‰é‡Œï¼Œattention_mask çš„å½¢çŠ¶é€šå¸¸æ˜¯ï¼š(batch_size, num_heads, query_len, key_len) # æ‰€ä»¥æˆ‘ä»¬è¦æŠŠè¿™ä¸ª (batch_size, seq_len) çš„ mask æ‰©å±•æˆ (batch_size, num_heads, seq_len, seq_len)ï¼Œå¹¶ä¸”è¦å¹¿æ’­åˆ°æ¯ä¸ª head ä¸Š#æœ€ç»ˆå¾—åˆ°çš„ attention_mask[batch, head, i, j] è¡¨ç¤ºï¼š#ç¬¬ batch ä¸ªæ ·æœ¬çš„ç¬¬ head ä¸ªå¤´ä¸­ï¼Œç¬¬ i ä¸ª query èƒ½ä¸èƒ½çœ‹åˆ°ç¬¬ j ä¸ª keyattention_mask = ( torch.tensor( [ [0, 1], [0, 0], [1, 0], ] )# shape: (3, 2) åŸå§‹maskï¼Œ0=å¯è§ï¼Œ1=ä¸å¯è§ï¼ˆæˆ–åè¿‡æ¥ï¼Œå–å†³äºå®ç°ï¼‰ .unsqueeze(1)# â†’ (3, 1, 2) .unsqueeze(2)# â†’ (3, 1, 1, 2) .expand(3, 8, 2, 2)# â†’ (3, 8, 2, 2))x = torch.rand(3, 2, 128)# å…«å¤´æ³¨æ„åŠ›net = MultiHeadAttention(128, 8)net(x, attention_mask) è¿™é‡Œå†æ¬¡è§£é‡Šä¸€ä¸‹ï¼Œä¸ºä»€ä¹ˆç°åœ¨ç°åœ¨çš„ä»£ç å®ç°éƒ½æ˜¯ q k v çš„æŠ•å½±çŸ©é˜µéƒ½æ˜¯åˆ†å¼€å†™çš„ï¼Œè¿™æ˜¯å› ä¸ºç°åœ¨çš„æ¨¡å‹å¾ˆå¤§ï¼Œæœ¬èº«å¯èƒ½ä¼šåš å¼ é‡å¹¶è¡Œï¼Œæµæ°´çº¿å¹¶è¡Œç­‰æ–¹å¼ï¼Œæ‰€ä»¥åˆ†å¼€å†™é—®é¢˜ä¹Ÿä¸å¤§ï¼ˆåˆ†å¼€å†™å¾ˆæ¸…æ™°ï¼‰ï¼Œå¯èƒ½æ˜¯åŠ é€Ÿæ•ˆæœå¹¶ä¸æ˜æ˜¾ã€‚","link":"/2025/12/10/Attention/"},{"title":"æ‰‹å†™Transformer-Decoder","text":"ä»Šå¹´æ˜¯2025å¹´ï¼Œå‚åŠ ç»„ä¼šçš„ä½ ä¸€å®šä¼šé‡åˆ°æŸä½å¸ˆå…„æˆ–å¸ˆå§è¯´ï¼Œä¸Šå‘¨è¯»äº†ã€ŠTransformer is All You Needã€‹è®ºæ–‡ä½œæ±‡æŠ¥ã€‚ç„¶åå°±ä¼šå±•ç¤ºå‡ºè¿™å¼ ç»å…¸çš„å›¾ç‰‡å¹¶è¯´ï¼Œå¤§è¯­è¨€æ¨¡å‹ç”±Encoderå’ŒDecoderç»„æˆï¼Œå·´æ‹‰å·´æ‹‰ã€‚ä½ çš„å¯¼å¸ˆä¼¼æ‡‚éæ‡‚ç‚¹äº†ç‚¹å¤´ï¼Œä½ ä¹Ÿç‚¹äº†ç‚¹å¤´ï¼ŒåŸæ¥LLMå°±æ˜¯è¿™æ ·ï¼Œå¼€å§‹ç©èµ·äº†æ‰‹æœºã€‚ äº‹å®ä¸ŠTransformerç»“æ„ç¡®å®ä¸»è¦ç”±Encoderã€Decoder ä¸¤ä¸ªéƒ¨åˆ†ç»„æˆï¼Œä¸¤ä¸ªéƒ¨åˆ†åˆ†åˆ«å…·æœ‰ä¸ä¸€æ ·çš„ç»“æ„å’Œè¾“å…¥è¾“å‡ºã€‚ ä½†é’ˆå¯¹ Encoderã€Decoderçš„ç‰¹ç‚¹ï¼Œé¢å¯¹ä¸åŒçš„äººç‰©å’Œå¾®è°ƒéœ€æ±‚ï¼Œå‡ºç°äº†ä¸åŒçš„ã€å¯¹ Transformer è¿›è¡Œä¼˜åŒ–çš„æ€è·¯ã€‚ä¾‹å¦‚ï¼ŒGoogle ä»…é€‰æ‹©äº† Encoder å±‚ï¼Œé€šè¿‡å°† Encoder å±‚è¿›è¡Œå †å ï¼Œå†æå‡ºä¸åŒçš„é¢„è®­ç»ƒä»»åŠ¡-æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMasked Language Modelï¼ŒMLMï¼‰ï¼Œæ‰“é€ äº†ä¸€ç»Ÿè‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNatural Language Understandingï¼ŒNLUï¼‰ä»»åŠ¡çš„ä»£è¡¨æ¨¡å‹â€”â€”BERTã€‚è€Œ OpenAI åˆ™é€‰æ‹©äº† Decoder å±‚ï¼Œä½¿ç”¨åŸæœ‰çš„è¯­è¨€æ¨¡å‹ï¼ˆLanguage Modelï¼ŒLMï¼‰ä»»åŠ¡ï¼Œé€šè¿‡ä¸æ–­å¢åŠ æ¨¡å‹å‚æ•°å’Œé¢„è®­ç»ƒè¯­æ–™ï¼Œæ‰“é€ äº†åœ¨ NLGï¼ˆNatural Language Generationï¼Œè‡ªç„¶è¯­è¨€ç”Ÿæˆï¼‰ä»»åŠ¡ä¸Šä¼˜åŠ¿æ˜æ˜¾çš„ GPT ç³»åˆ—æ¨¡å‹ï¼Œä¹Ÿæ˜¯ç°ä»Šå¤§ç«çš„LLMçš„åŸºåº§æ¨¡å‹ã€‚å½“ç„¶ï¼Œè¿˜æœ‰ä¸€ç§æ€è·¯æ˜¯åŒæ—¶ä¿ç•™ Encoder ä¸ Decoderï¼Œæ‰“é€ é¢„è®­ç»ƒçš„ Transformer æ¨¡å‹ï¼Œä¾‹å¦‚ç”± Google å‘å¸ƒçš„ T5æ¨¡å‹ã€‚ æ€»çš„æ¥è¯´ï¼ŒçŸ­çŸ­åå¹´ä¸åˆ°çš„æ—¶é—´ï¼Œå°±æŒ‰é¡ºåºå‡ºç°äº†Encoder-Onlyã€Encoder-Decoderã€Decoder-Onlyä¸‰ç§ä¸»æµé¢„è®­ç»ƒæ¨¡å‹ã€‚ å…·ä½“æ¥è¯´: BERT é‡‡ç”¨äº† Encoder-Only ç»“æ„ï¼ŒåªåŒ…å«ç¼–ç å™¨éƒ¨åˆ†ï¼›è€Œ GPT é‡‡ç”¨äº† Decoder-Only ç»“æ„ï¼ŒåªåŒ…å«è§£ç å™¨éƒ¨åˆ†ã€‚T5 åˆ™é‡‡ç”¨äº† Encoder-Decoder ç»“æ„ï¼Œ å…¶ä¸­æœ€ç«çƒ­çš„å½“å±ä¸šç•Œä¸»æµæŠ€æœ¯Decoder-Onlyï¼ŒDecoder-Onlyä½œä¸ºGPTçš„æ¨¡å‹ï¼ŒçœŸæ­£è®©LLMä¸ºä¸–äººæ‰€çŸ¥ã€‚è¿™ç§ç»“æ„åªéœ€è¦æ¥å—åºåˆ—æ•°æ®ï¼Œè¾“å‡ºåºåˆ—æ•°æ®ï¼Œå°±å¯ä»¥å®Œæˆç¿»è¯‘ï¼Œé—®ç­”å¯¹è¯ç­‰å¤šä»»åŠ¡ã€‚å…¶æœ¬è´¨æ˜¯è‡ªå›å½’æ¨¡å‹ã€‚ ä»¥ä¸‹å†…å®¹å¤§é‡å‚è€ƒchaofaç”¨ä»£ç æ‰“ç‚¹é…±æ²¹çš„åšå®¢ï¼Œæ¨¡å‹æŠ€æœ¯ä»¥å½“å‰ï¼ˆ2025.12ï¼‰æœ€å¥½çš„å¼€æºæ¨¡å‹â€”â€”Llamaå¼€æºæ–‡æ¡£ä¸ºå‡† é¢è¯•è¿‡ç¨‹ä¸­è®©å†™ transformers Decoder ä¸€å®šè¦æ²Ÿé€šæ¸…æ¥šæ˜¯å†™ä¸€ä¸ª CausalLM decoder è¿˜æ˜¯åŸç‰ˆçš„ï¼ŒåŸç‰ˆçš„æ¯”è¾ƒå¤æ‚ï¼Œä¸€èˆ¬ä¹Ÿä¸ä¼šè®©å†™ã€‚è¿™é‡Œçš„ Decoder ä¸€èˆ¬æŒ‡çš„æ˜¯ CausalLMï¼Œå…·ä½“å˜åŒ–æ˜¯å°‘äº† encoder éƒ¨åˆ†çš„è¾“å…¥ï¼Œæ‰€ä»¥ä¹Ÿå°±æ²¡æœ‰äº† encoder and decoder cross attentionã€‚å› ä¸ºé‡ç‚¹å¸Œæœ›å†™ CausalLMï¼Œæ‰€ä»¥æ²¡æœ‰ Cross attention å’Œ ä¹Ÿçœç•¥äº† token embedding è¿™ä¸€æ­¥ã€‚ çŸ¥è¯†ç‚¹è¿™é‡Œæœ‰å¾ˆå¤šç»†èŠ‚å¯ä»¥å¤šè¯´ å…³äºå½’ä¸€åŒ–æ–¹æ³•çš„è¯´æ˜ï¼š æŒ‰ç…§å½’ä¸€åŒ–æ–¹æ³•æ¥åˆ†ï¼Œä¸»è¦åˆ†ä¸ºLayerNormï¼ŒBatchNormï¼ŒRMSNormä»¥åŠDeepNormã€‚å¦‚æœæŒ‰ç…§å½’ä¸€åŒ–ä½ç½®æ¥åˆ†ç±»ï¼ŒåŒ…æ‹¬ postNorm å’Œ preNormï¼Œä½†å…·ä½“æ¥è¯´ç”¨postNormæ¯”è¾ƒå¤šã€‚ BatchNormå¯¹æ•°æ®çš„ä¸€å®šç»´åº¦åœ¨batchæ•°æ®ä¸­å½’ä¸€åŒ–ï¼Œä¸€èˆ¬åº”ç”¨äºå›¾åƒã€‚è¿™ç§æ–¹æ³•å¾ˆéš¾é€‚ç”¨äºåºåˆ—æ•°æ®ï¼Œå¯¹äºåºåˆ—æ•°æ®è€Œè¨€ï¼Œåœ¨batchç»´åº¦åšå½’ä¸€æ„ä¹‰ä¸å¤§ï¼Œè€Œä¸”ä¸€ä¸ªbatchå†…çš„åºåˆ—é•¿åº¦ä¸åŒã€‚LayerNormæ˜¯é’ˆå¯¹åºåˆ—æ•°æ®æå‡ºçš„ä¸€ç§å½’ä¸€åŒ–æ–¹æ³•ï¼Œä¸»è¦åœ¨layerç»´åº¦è¿›è¡Œå½’ä¸€åŒ–ï¼Œå³å¯¹æ•´ä¸ªåºåˆ—è¿›è¡Œå½’ä¸€åŒ–ã€‚layerNormä¼šè®¡ç®—ä¸€ä¸ªlayerçš„æ‰€æœ‰activationçš„å‡å€¼å’Œæ–¹å·®ï¼Œåˆ©ç”¨å‡å€¼å’Œæ–¹å·®è¿›è¡Œå½’ä¸€åŒ–ã€‚RMSNormçš„æå‡ºæ˜¯ä¸ºäº†æå‡layerNormçš„è®­ç»ƒé€Ÿåº¦æå‡ºçš„ã€‚RMSNormä¹Ÿæ˜¯ä¸€ç§layerNormï¼Œåªæ˜¯å½’ä¸€åŒ–çš„æ–¹æ³•ä¸åŒã€‚ç›¸æ¯”layerNormä¸­åˆ©ç”¨å‡å€¼å’Œæ–¹å·®è¿›è¡Œå½’ä¸€åŒ–ï¼ŒRMSNorm åˆ©ç”¨å‡æ–¹æ ¹è¿›è¡Œå½’ä¸€åŒ–ã€‚ $$LayerNorm: \\dfrac{x-Ex}{\\sqrt{Dx+\\epsilon}}*\\gamma + \\beta$$ transformers decoder çš„æµç¨‹æ˜¯ï¼šinput -&gt; self-attention -&gt; cross-attention -&gt; FFN causalLM decoder çš„æµç¨‹æ˜¯ input -&gt; self-attention -&gt; FFNå…¶ä»– [self-attention, FFN] æ˜¯ä¸€ä¸ª blockï¼Œä¸€èˆ¬ä¼šæœ‰å¾ˆå¤šçš„ block FFN çŸ©é˜µæœ‰ä¸¤æ¬¡å˜åŒ–ï¼Œä¸€æ¬¡å‡ç»´åº¦ï¼Œä¸€æ¬¡é™ç»´åº¦ã€‚å…¶ä¸­ LLaMA å¯¹äº GPT çš„æ”¹è¿›è¿˜æœ‰æŠŠ GeLU å˜æˆäº† SwishGLUï¼Œå¤šäº†ä¸€ä¸ªçŸ©é˜µã€‚æ‰€ä»¥ä¸€èˆ¬å‡ç»´ä¼šä» 4h -&gt; 4h * 2 / 3 FFNæ˜¯å¸¸è§çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œä¸»è¦ç»å†ä¸€ä¸ªå‡ç»´å†é™ç»´çš„è¿‡ç¨‹ï¼ˆç”Ÿæˆå™¨åŸºæœ¬éƒ½è¿™æ ·ï¼‰ï¼Œä¸ºä»€ä¹ˆå¸¸è§éƒ½æ˜¯å‡4ç»´å‘¢ï¼Ÿä»å·¥ç¨‹è§’åº¦æ¥è¯´ï¼Œè¿™æ˜¯æƒè¡¡è®­ç»ƒæˆæœ¬å’Œå®é™…æ•ˆæœåçš„é€‰æ‹©ã€‚ä»ç†è®ºè§’åº¦æ¥è¯´ï¼Œå¸¸è§çš„æ¿€æ´»å‡½æ•°éƒ½ä¼šå¯¼è‡´è¿‘50%çš„ä¿¡æ¯æŸå¤±ï¼ˆå‚è€ƒReLUï¼‰ï¼Œä¸¤å±‚çš„ç»“æ„è¯´æ˜æœ€åä¼šå¯¼è‡´åªæœ‰50% * 50% = 25%çš„ä¿¡æ¯ä¿ç•™ï¼Œä¸ºäº†æ§åˆ¶å‰åä¿¡æ¯é‡ä¸å¤§å˜åŒ–ï¼Œé€‰æ‹©å‡4ç»´ã€‚ åŸç‰ˆçš„ transformers ç”¨ post-norm, åé¢ gpt2, llama ç³»åˆ—ç”¨çš„æ˜¯ pre-normã€‚å…¶ä¸­ llama ç³»åˆ—ä¸€èˆ¬ç”¨ RMSNorm ä»£æ›¿ GPT and transformers decoder ä¸­çš„ LayerNormã€‚ å…·ä½“æ¥è¯´ç”¨postNormæ¯”è¾ƒå¤šï¼Œå…·ä½“çš„åŸå› å¯ä»¥çœ‹ä¹‹å‰è¿™ä¸€ç¯‡ ä¸ºä»€ä¹ˆå¤§æ¨¡å‹ç»“æ„è®¾è®¡ä¸­å¾€å¾€ä½¿ç”¨postNormè€Œä¸ç”¨preNormï¼Ÿ Decoderéƒ¨åˆ†çš„ä»£ç 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108import mathimport torchimport torch.nn as nnimport warningswarnings.filterwarnings(action=&quot;ignore&quot;)# å†™ä¸€ä¸ªBlockclass SimpleDecoder(nn.Module): def __init__(self, hidden_dim, n_heads, dropout = 0.1): super().__init__() self.n_heads = n_heads self.head_dim = hidden_dim // n_heads self.dropout = dropout # è¿™é‡ŒæŒ‰ç…§ transformers ä¸­çš„ decoder æ¥å†™ï¼Œç”¨ post_norm çš„æ–¹å¼å®ç°ï¼Œä¸»è¦æœ‰ æ®‹å·®é“¾æ¥ # eps æ˜¯ä¸ºäº†é˜²æ­¢æº¢å‡ºï¼›å…¶ä¸­ llama ç³»åˆ—çš„æ¨¡å‹ä¸€èˆ¬ç”¨çš„æ˜¯ RMSnorm ä»¥åŠ pre-normï¼ˆä¸ºäº†ç¨³å®šæ€§ï¼‰ # RMSnorm æ²¡æœ‰ä¸€ä¸ª recenter çš„æ“ä½œï¼Œè€Œ layernorm æ˜¯è®©æ¨¡å‹é‡æ–°å˜æˆ å‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º 1 # RMS ä½¿ç”¨ wå¹³æ–¹æ ¹å‡å€¼è¿›è¡Œå½’ä¸€åŒ– $\\sqrt{\\frac{1}{n} \\sum_{1}^{n}{a_i^2} }$ self.layernorm_att = nn.LayerNorm(hidden_dim, eps =0.00001) self.q_proj = nn.Linear(hidden_dim, hidden_dim) self.k_proj = nn.Linear(hidden_dim, hidden_dim) self.v_proj = nn.Linear(hidden_dim, hidden_dim) self.o_proj = nn.Linear(hidden_dim, hidden_dim) self.drop_att = nn.Dropout(self.dropout) #å‡†å¤‡FFNå‰é¦ˆç¥ç»ç½‘ç»œ self.up_proj = nn.Linear(hidden_dim, hidden_dim * 4) self.down_proj = nn.Linear(hidden_dim * 4, hidden_dim) self.layernorm_ffn = nn.LayerNorm(hidden_dim, eps = 0.00001) self.act_fn = nn.ReLU() self.drop_ffn = nn.Dropout(self.dropout) def attention_output(self, Q, K , V, attention_mask = None): #è®¡ç®—ä¸¤è€…ç›¸å…³æ€§ K = K.transpose(2,3) # [batch_size, n_heads, head_dim, seq_len] # è¿˜æ²¡æœ‰softmaxçš„att_weight att_weight = (Q @ K) / math.sqrt(self.head_dim) #attention maskè¿›è¡Œä¾æ¬¡è°ƒæ•´ï¼šå˜æˆcausal_attention if attention_mask is not None: #å˜æˆä¸‹ä¸‰è§’çŸ©é˜µ attention_mask = attention_mask.tril() #æå–ä¸‹ä¸‰è§’éƒ¨åˆ†å•å‘æ³¨æ„åŠ› att_weight = att_weight.masked_fill(attention_mask == 0, float(&quot;-1e20&quot;)) #å¯¹0éƒ¨åˆ†å‚ä¸ä¸€ä¸ªæå°å€¼ï¼Œåœ¨softmaxåä¸º0 else: #äººå·¥æ„é€ ä¸‹ä¸‰è§’Attention mask attention_mask = torch.ones_like(att_weight).tril() att_weight = att_weight.masked_fill(attention_mask == 0, float(&quot;-1e20&quot;)) att_weight = torch.softmax(att_weight, dim = -1) # print(att_weight) # Dropoutè®ºæ–‡æœ€æ—©æå‡ºçš„æ—¶å€™è¢«æ‹’ç¨¿ï¼Œè®¤ä¸ºåªæœ‰å·¥ç¨‹è€Œæ— ç†è®ºã€‚å†çœ‹çœ‹ä»Šå¤©Dropoutæ— å¤„ä¸åœ¨ï¼Œæœç„¶æ˜¯â€œå…¥å…³åè‡ªæœ‰å¤§å„’ä¸ºæˆ‘è¾©è¯â€ att_weight = self.drop_att(att_weight) mid_output = att_weight @ V # shape [batch_size, n_heads, seq_len, head_dim] #contiguous()å‡½æ•°åœ¨PyTorchä¸­ç”¨äºç¡®ä¿å¼ é‡ï¼ˆtensorï¼‰åœ¨å†…å­˜ä¸­çš„æ•°æ®æ˜¯è¿ç»­å­˜å‚¨çš„ã€‚å½“æ‰§è¡Œtranspose()ã€permute()ç­‰æ”¹å˜ç»´åº¦é¡ºåºçš„æ“ä½œåï¼Œå¼ é‡å¯èƒ½ä¼šå˜å¾—â€œéè¿ç»­â€ï¼ˆä¸æŒ‰è¡Œä¼˜å…ˆæˆ–åˆ—ä¼˜å…ˆå­˜å‚¨ï¼‰ã€‚contiguous()çš„ä½œç”¨æ˜¯è¿”å›ä¸€ä¸ªæ–°çš„ã€å†…å­˜è¿ç»­çš„å¼ é‡å‰¯æœ¬ï¼Œè¿™æ ·view()ï¼ˆé‡å¡‘å½¢çŠ¶ï¼‰ç­‰éœ€è¦è¿ç»­å†…å­˜çš„æ“ä½œæ‰èƒ½æ­£ç¡®æ‰§è¡Œï¼Œæå‡è®¡ç®—æ•ˆç‡ã€‚ mid_output = mid_output.transpose(1,2).contiguous() batch_size, seq_len, _, _ = mid_output.size() mid_output = mid_output.view(batch_size, seq_len, -1) # OçŸ©é˜µåªæ˜¯æƒé‡çŸ©é˜µï¼Œå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œå¯¹æ‹¼æ¥åçš„çŸ©é˜µå¤„ç† output = self.o_proj(mid_output) return output def attention_block(self, X, attention_mask = None): batch_size, seq_len, _ = X.size() Q = self.q_proj(X).view(batch_size, seq_len, self.n_heads, -1).transpose(1,2) K = self.k_proj(X).view(batch_size, seq_len, self.n_heads, -1).transpose(1,2) V = self.v_proj(X).view(batch_size, seq_len, self.n_heads, -1).transpose(1,2) output = self.attention_output(Q,K,V,attention_mask = attention_mask) return self.layernorm_att(X + output) def ffn_block(self, X): # act_fn = ReLU up = self.act_fn(self.up_proj(X)) down = self.down_proj(up) #dropout down = self.drop_ffn(down) # norm return self.layernorm_ffn(X + down) def forward(self, X, attention_mask = None): # X ä¸€èˆ¬å‡è®¾æ˜¯å·²ç»ç»è¿‡embeddingçš„è¾“å…¥ï¼Œ ï¼ˆbatch_size, seq_len, hidden_dimï¼‰ # attention_mask ä¸€èˆ¬æŒ‡çš„æ˜¯ tokenizer åè¿”å›çš„ mask ç»“æœï¼Œè¡¨ç¤ºå“ªäº›æ ·æœ¬éœ€è¦å¿½ç•¥ # shape ä¸€èˆ¬æ˜¯ï¼š (batch_size, n_heads, seq_len) att_output = self.attention_block(X, attention_mask = attention_mask) ffn_output = self.ffn_block(att_output) return ffn_output# æµ‹è¯•x = torch.rand(3, 4, 64) # [batch_size, seq_len, hidden_dim]net = SimpleDecoder(64, 8) # hidden_dim = 64,n_heads = 8mask = ( torch.tensor([[1, 1, 1, 1], [1, 1, 0, 0], [1, 1, 1, 0]]) .unsqueeze(1) .unsqueeze(2) .repeat(1, 8, 4, 1))net(x, mask).shape å…³äºæ¨ç†è¿‡ç¨‹å¦‚æœä½ è®¤çœŸçœ‹äº†ä¸Šé¢çš„ä»£ç éƒ¨åˆ†ï¼Œä¸çŸ¥é“ä½ ä¼šä¸ä¼šäº§ç”Ÿè¿™æ ·çš„ç–‘é—®ï¼šå¤§è¯­è¨€æ¨¡å‹ä¸æ˜¯æ ¹æ®å½“å‰çš„å­—é¢„æµ‹ä¸‹ä¸€ä¸ªå­—æ˜¯ä»€ä¹ˆå—ï¼Ÿè¯·é—®ä¸€ä¸‹æ¨ç†æ—¶prefillé˜¶æ®µä¸ºä»€ä¹ˆéœ€è¦è®¡ç®—æ‰€æœ‰tokençš„qã€‚é¢„æµ‹next tokençš„è¯ä¸æ˜¯åªéœ€è¦è®¡ç®—ä¸Šä¸€ä¸ªtokençš„qå°±å¥½äº†ä¹ˆï¼Ÿå› æ­¤åªéœ€è¦ç®—prefillæœ€åä¸€ä¸ªtokençš„qå°±è¡Œäº†ï¼Ÿ è¿™æ˜¯ä¸€ä¸ªéå¸¸éå¸¸éå¸¸å¥½çš„é—®é¢˜ğŸ‘ æ ¸å¿ƒåŸå› æ˜¯ Decoder ä¸­æœ‰å¾ˆå¤šå±‚ï¼Œå‡è®¾åªæœ‰ä¸€å±‚ï¼Œä½ çš„æ€è€ƒæ˜¯æ²¡æœ‰é—®é¢˜ã€‚ä½†æ˜¯ decoder æœ‰å¾ˆå¤š blockï¼Œä¸Šä¸€å±‚ï¼ˆå‡è®¾ç¬¬2å±‚ï¼‰çš„è¾“å…¥æ˜¯ä¸‹ä¸€å±‚çš„è¾“å‡ºï¼Œä¸‹ä¸€å±‚ï¼ˆç¬¬ä¸€å±‚ï¼‰çš„è¾“å‡ºä¾èµ–äº qkv çš„è®¡ç®—ï¼Œé‚£ä¹ˆè®¡ç®—ç¬¬äºŒå±‚çš„ q k vçš„æ—¶å€™ï¼Œå¦‚æœä¸çŸ¥é“ç¬¬ä¸€å±‚çš„ q k vï¼ˆæˆ–è€…outputï¼‰ï¼Œé‚£ä¹ˆç¬¬äºŒå±‚çš„ k v å°±æ— æ³•è®¡ç®—å‡ºæ¥äº†ã€‚å› ä¸º prefill ä¹‹å‰æ²¡æœ‰ç¼“å­˜å¥½æ¯ä¸€å±‚çš„ k v æ˜¯ä»€ä¹ˆã€‚ é‚£ä¹ˆæ¢å¥è¯è¯´ï¼Œå¦‚æœèƒ½ç¼“å­˜å¥½æ¯ä¸€å±‚çš„Kï¼ŒVçŸ©é˜µï¼Œæ¨ç†é€Ÿåº¦å°†ä¼šå¤§å¤§å¢åŠ ï¼ å”¯ä¸€éœ€è¦è€ƒè™‘çš„é—®é¢˜æ˜¯ï¼šå¦‚ä½•é«˜æ•ˆçš„å®ç°KVçŸ©é˜µç¼“å­˜ã€‚æˆ‘ä»¬å°†æ¥åˆ°KV Cache â€”â€”ç™¾å€æ¨ç†åŠ é€ŸæŠ€æœ¯","link":"/2025/12/11/TransformerDecoder/"},{"title":"ä¸€äº›æœ‰æ„æ€çš„ç®—æ³•é¢˜","text":"ä¸æƒ³åˆ·LeetCodeå•Šï¼ï¼ï¼ è¨€å½’æ­£ä¼ ï¼Œå†™ä»£ç è¿˜æ˜¯å¾ˆæœ‰æ„æ€çš„äº‹æƒ…ã€‚æˆ‘å¤§å­¦è¯»çš„æ˜¯æ•°å­¦ä¸“ä¸šï¼Œä¸€å¼€å§‹åªè§‰å¾—å†™ä»£ç æ˜¯è®¡ç®—æœºä¸“ä¸šè¯¥å¹²çš„äº‹ï¼Œåæ¥æƒŠå¥‡åœ°å‘ç°ï¼Œæœ‰äº›ç®—æ³•è«åä¸é‚£äº›ææ€–çš„æ•°å­¦çŸ¥è¯†ç›¸å…³ï¼Œè€Œè¿™äº›ç®—æ³•éƒ½æ˜¯ä¸ºäº†è§£å†³ä¸€ä¸ªå…·ä½“çš„æœ‰è¶£é—®é¢˜ã€‚è¿™é‡Œæ•´ç†ä¸€äº›LeetCodeä¸­æœ‰è¶£çš„é¢˜ç›®ï¼Œæ›´å¤šè¿˜æ˜¯ç»“åˆæ•°å­¦çš„å†…å®¹ã€‚ Q1: çˆ¬æ¥¼æ¢¯è¿™ä¸ªé—®é¢˜æˆ‘åº”è¯¥æ˜¯æœ€æ—©åœ¨ç”µå½±ã€Šå°‘å¹´ç­ã€‹é‡Œçœ‹åˆ°çš„ï¼Œé‚£ä¸ªç¥ç«¥æ‘‡ç€é¾Ÿå£³ï¼Œæ ¹æ®æ‰è½é“œé’±æ¥å›ç­”çš„åœºæ™¯å¤ªç¥æ£ï¼Œå°è±¡å¾ˆæ·±ã€‚ å‡è®¾ä½ æ­£åœ¨çˆ¬æ¥¼æ¢¯ã€‚éœ€è¦ n é˜¶ä½ æ‰èƒ½åˆ°è¾¾æ¥¼é¡¶ã€‚æ¯æ¬¡ä½ å¯ä»¥çˆ¬ 1 æˆ– 2 ä¸ªå°é˜¶ã€‚ä½ æœ‰å¤šå°‘ç§ä¸åŒçš„æ–¹æ³•å¯ä»¥çˆ¬åˆ°æ¥¼é¡¶å‘¢ï¼Ÿæ¯”å¦‚2çº§å°é˜¶æœ‰ä¸¤ç§æ–¹æ³•ï¼š1+1ã€2ï¼›3çº§å°é˜¶æœ‰ä¸‰ç§æ–¹æ³•ï¼š1+1+1ã€1+2ã€2+1ï¼› ä»é€’å½’çš„è§’åº¦å®¹æ˜“è€ƒè™‘åˆ°ï¼Œè®°ä¸Š$n$çº§å°é˜¶çš„æ–¹æ³•æ•°é‡ä¸º$f(n)$ï¼Œé‚£ä¹ˆå‰ä¸€æ­¥ä¸€å®šåœ¨$n-1$çº§å°é˜¶æˆ–è€…$n-2$çº§å°é˜¶ï¼Œæœ‰$f(n)=f(n-1)+f(n-2)$ç­‰å¼æˆç«‹ã€‚è¿™å°±æ˜¯æ–æ³¢é‚£å¥‘æ•°åˆ—çš„é€šé¡¹$1,2,3,5,8,13,21â€¦.$ï¼Œè‡ªç„¶å¾ˆå®¹æ˜“çš„å†™å‡ºè¿™æ ·çš„ç®—æ³•ã€‚ 12345678910def climbStairs(self, n: int) -&gt; int: if n &lt;= 2: return n # Fibonacci-like sequence first, second = 1, 2 for i in range(3, n + 1): first, second = second, first + second return second äº‹å®ä¸Šå¯¹äº$f(n)=f(n-1)+f(n-2)$çš„é½æ¬¡çº¿æ€§é€’æ¨å¼ï¼Œé™¤äº†ç”¨é«˜ä¸­åœ¨æ•°åˆ—ä¸­ç”¨åˆ°çš„ç‰¹å¾æ–¹ç¨‹æ–¹æ³•æ¥æ±‚å‡ºæ–æ³¢é‚£å¥‘æ•°åˆ—çš„é€šé¡¹å…¬å¼å¤–ï¼Œè¿˜å¯ä»¥å°†é½æ¬¡çº¿æ€§é€’æ¨å¼å†™ä¸ºçŸ©é˜µï¼Œä½¿ç”¨çŸ©é˜µå¿«é€Ÿå¹‚æ–¹æ³•æ¥å®ç°ã€‚åè€…çš„ä¼˜åŠ¿åœ¨äºï¼Œå¯¹äºæŸäº›ç‰¹æ®Šçš„éé½æ¬¡çº¿æ€§é€’æ¨å¼ï¼Œä¹Ÿå¯ä»¥å¤„ç†ã€‚ Q2: åªå‡ºç°ä¸€æ¬¡çš„æ•° ç»™ä½ ä¸€ä¸ª éç©º æ•´æ•°æ•°ç»„ nums ï¼Œé™¤äº†æŸä¸ªå…ƒç´ åªå‡ºç°ä¸€æ¬¡ä»¥å¤–ï¼Œå…¶ä½™æ¯ä¸ªå…ƒç´ å‡å‡ºç°ä¸¤æ¬¡ã€‚æ‰¾å‡ºé‚£ä¸ªåªå‡ºç°äº†ä¸€æ¬¡çš„å…ƒç´ ã€‚ä½ å¿…é¡»è®¾è®¡å¹¶å®ç°çº¿æ€§æ—¶é—´å¤æ‚åº¦çš„ç®—æ³•æ¥è§£å†³æ­¤é—®é¢˜ï¼Œä¸”è¯¥ç®—æ³•åªä½¿ç”¨å¸¸é‡é¢å¤–ç©ºé—´ã€‚ è¿™é“é¢˜éš¾åº¦easyï¼Œä½†æ˜¯å…¶è§£æ³•ä¸­çš„ä½è¿ç®—æ–¹æ³•å®åœ¨å¤ªä¼˜é›…ï¼ˆä¾èµ–äºå…¶ä½™æ•°åªå‡ºç°å¶æ•°æ¬¡ï¼‰ï¼Œè®©æˆ‘æƒ³èµ·å¾ˆå¤šå¹´å‰è§åˆ°è¿‡ä¸€ä¸ªä½è¿ç®—æ±‚Nçš‡åé—®é¢˜çš„ç‰›é€¼è§£æ³•ã€‚è¿™æ­£å¥½å€Ÿè¿™ä¸ªæœºä¼šå±•ç¤ºäºŒè¿›åˆ¶çš„é­…åŠ›ã€‚ 1234567891011def singleNumber(self, nums: List[int]) -&gt; int: &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; n = len(nums) index = 0 for i in range(n): # å¼‚æˆ–æœ‰ç»“åˆå¾‹å’Œäº¤æ¢å¾‹ index ^= nums[i] return index","link":"/2025/12/11/%E4%B8%80%E4%BA%9B%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E7%AE%97%E6%B3%95%E9%A2%98/"}],"tags":[{"name":"å¤§è¯­è¨€æ¨¡å‹","slug":"å¤§è¯­è¨€æ¨¡å‹","link":"/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"name":"ç®—æ³•","slug":"ç®—æ³•","link":"/tags/%E7%AE%97%E6%B3%95/"}],"categories":[{"name":"LLMå­¦ä¹ ç³»åˆ—","slug":"LLMå­¦ä¹ ç³»åˆ—","link":"/categories/LLM%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97/"}],"pages":[]}