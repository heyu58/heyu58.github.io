{"posts":[{"title":"当代孔乙己-Attention架构的四种写法","text":"大语言模型（LLM）发展方兴未艾，从ChatGPT横空出世横扫英文互联网，到DeepSeek惊讶世人宣告低成本LLM可行性，2025年短短一年便造就了人工智能领域如此震撼的估值繁荣（or bubble），不得不让人感慨《三体》中 “技术爆炸” 一词提出的先见性。 一切从《Attention is All You Need》开始。这篇论文彻底抛弃了长序列预测的LSTM和RNN模型，在学术界有着划时代意义；更在传播学上展示了一个好的名字到底有多重要。 以下内容大量参考chaofa用代码打点酱油的博客 在AI相关的面试中，经常会要求对Transformer架构有足够的理解，通常通过手写self-attention机制来验证。这里给出了Attention机制从简到繁的四种写法 什么是$Self-Attention$？$Attention$ 机制通常在已实现 $Embedding$ + 位置编码后进行。引入 $Attention$ 机制主要是由于 $Embedding$ 和位置编码依然不够表示所有的信息。 举个例子 “你不要打人” 和 “你快去打水” 中 “打” 字含义截然不同，但仅靠 $Embedding$ 和位置编码两者并不能区别，这就需要结合上下文来增加更多信息，$Attention$ 就是这种结合的方式。 $$SelfAttention(X)=softmax(\\frac{Q\\cdot K}{\\sqrt{d}})\\cdot V$$ $Q = K = V = W_{Q/K/V} \\cdot X$，其中$Q，K，V$分别对应不同（形状也不同）的权重矩阵W matmul和@符号是一样的作用，矩阵乘法 为什么要除以$\\sqrt{d}$：一是防止梯度消失（参考Kaiming Initialization参数初始化）；二是为了让QK的内积分布保持和输入一样 爱因斯坦方程表达式用法：torch.einsum(“bqd,bkd-&gt;bqk”,X,X).shape 为什么使用softmax?注意力机制，本质上是想找与查询向量最相关的隐向量序列，实际上是想找，onehot(argmax)，比如的任务是中视野中找苹果，直接依据苹果的特征查找苹果。但是这个求解是无法计算梯度，所以才用onehot(argmax)的光滑版本softmax，以获得良好的梯度特性。是一种数学上的妥协 X.repeat(1,1,3)表示在不同维度进行repeat操作，也可以用tensor.expand操作 12345import mathimport warningsimport torchimport torch.nn as nnwarnings.filterwarnings(action='ignore') 第一种写法：简洁123456789101112131415161718192021222324class SelfAttention_1(nn.Module): # 父类是pytorch中的核心Module def __init__(self, hidden_dim): super(SelfAttention_1,self).__init__()# 调用父类的构造方法，并传递所有参数 self.hidden_dim = hidden_dim self.Q = nn.Linear(hidden_dim, hidden_dim) self.K = nn.Linear(hidden_dim, hidden_dim) self.V = nn.Linear(hidden_dim, hidden_dim) def forward(self,X): # X.shape = [batch_size, seq_len, hidden_dim] # 根据输入X的形状创建Q,K,V Q = self.Q(X) K = self.K(X) V = self.V(X) # K需要改为shape = [batch_size, seq_len, hidden_dim] QK = Q @ K.transpose(-1,-2)# K.transpose(1,2) softmax_QK = torch.softmax( QK / math.sqrt(self.hidden_dim), dim = -1) # 在最后一维上计算softmax output = softmax_QK @ V return output X = torch.rand(3,2,4)net = SelfAttention_1(4)net(X) 第二种写法：效率优化12345678910111213141516171819class SelfAttention_2(nn.Module): def __init__(self,hidden_dim): super().__init__() self.hidden_dim = hidden_dim self.proj = nn.Linear(hidden_dim, hidden_dim*3) #直接将Q，K，V网络合并 self.output_proj = nn.Linear(hidden_dim,hidden_dim) def forward(self,X): # X.shape = [batch_size, seq_len, hidden_dim] QKV = self.proj(X) # [batch_size, seq_len, dim*3] # torch.split(x, chunks_size, dim) Q,K,V = torch.split(QKV , self.hidden_dim , dim =-1) QK = torch.softmax(Q @ K.transpose(-1,-2) / math.sqrt(self.hidden_dim), dim = -1) output = QK @ V return self.output_proj(output) X = torch.rand(3,2,4)net = SelfAttention_2(4)net(X) 第三种写法：加上细节attention_mask，更完整 Attention计算时有dropout层，设置的位置很有趣 Attention计算时会加入attention_mask，因为样本会进行padding操作（不同长度的序列输入，需要相同的维度Attention_map，这要求在QKV以外进行mask处理） MultiHeadAttention过程中，除QKV三个矩阵外，还有output对应的投影矩阵 12345678910111213141516171819202122232425262728293031323334353637383940414243class SelfAttention_3(nn.Module): def __init__(self, hidden_dim): super().__init__() self.hidden_dim = hidden_dim self.proj = nn.Linear(hidden_dim, hidden_dim*3) self.att_drop = nn.Dropout(0.1) self.output_proj = nn.Linear(hidden_dim, hidden_dim) def forward(self, X,attention_mask = None): # attention_mask = [batch_size, seq_len] # X.shape = [batch_size, seq_len, hidden_dim] QKV = self.proj(X) Q, K, V = torch.split(QKV,self.hidden_dim, dim=-1) QK = Q @ K.transpose(-1,-2) / math.sqrt(self.hidden_dim) if attention_mask is not None: # mask的目的：屏蔽掉不该参与注意力计算的位置（无效padding、未来token屏蔽、无效区域） # masked_fill(condition, value) PyTorch 的方法：将 condition 为 True 的位置替换成 value # value = float(&quot;-1e20&quot;)是因为在softmax后值接近于0 QK = QK.masked_fill(attention_mask == 0, float(&quot;-1e20&quot;)) QK = torch.softmax(QK,dim = -1) # 这里在 BERT中的官方代码也说很奇怪，但是原文中这么用了，所以继承了下来 # （用于 output 后面会更符合直觉？） QK= self.att_drop(QK) output = QK @ V return self.output_proj(output) X = torch.rand(3,4,2)b = torch.tensor( [ [1, 1, 1, 0], [1, 1, 0, 0], [1, 0, 0, 0], ])print(f&quot;mask的初始形状{b.shape}&quot;)# unsqueeze(dim)表示在指定位置增加维度mask = b.unsqueeze(dim=1).repeat(1, 4, 1)print(f&quot;mask：{mask}&quot;)net = SelfAttention_3(2)net(X,mask) 第四种写法：从单头到多头一般在实际上的训练过程中都会使用 Multi Head, 多头注意力的实现核心是torch.permute()函数（支持高维转置操作）, 而且其实也仅仅是每个 Head 做完 Self-Attention 得到结果之后，进行拼接，然后过一个 output 投影矩阵。本质上是一种集成学习，与券商研报里的因子打分相比还是更高明一点。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667class MultiHeadAttention(nn.Module): def __init__(self,hidden_dim, n_head): super().__init__() self.n_head = n_head self.hidden_dim = hidden_dim self.head_dim = hidden_dim // n_head # 注意hidden_dim= head_dim * n_head # 将最后一个维度进行拆分，一个诸葛亮拆成n_head个臭皮匠 self.Q = nn.Linear(hidden_dim,hidden_dim) self.K = nn.Linear(hidden_dim,hidden_dim) self.V = nn.Linear(hidden_dim,hidden_dim) self.att_dropout = nn.Dropout(0.1) self.proj = nn.Linear(hidden_dim, hidden_dim) def forward(self,X,attention_mask=None): # attention_mask = [batch_size, seq_len] # X.shape = [batch_size, seq_len, hidden_dim] batch_size, seq_len, _ = X.size() Q = self.Q(X) K = self.Q(X) V = self.Q(X) # 下面reshape改变Q，K，V的shape为[batch_size,seq_len, n_head, head_dim] # 多头注意力训练建立在认知：训练多个小模型难度低于训练一个大模型 Q_state = Q.reshape(batch_size, seq_len, self.n_head, self.head_dim).permute(0,2,1,3) K_state = K.reshape(batch_size, seq_len, self.n_head, self.head_dim).permute(0,2,1,3) V_state = V.reshape(batch_size, seq_len, self.n_head, self.head_dim).permute(0,2,1,3) QK = Q_state @ K_state.transpose(-1,-2) / math.sqrt(self.head_dim) if attention_mask is not None: QK = QK.masked_fill(attention_mask == 0, float(&quot;-1e20&quot;)) # 或者有些实现是 attention_mask == 0 时 mask，看具体代码 # 对最后一个维度head_dim进行softmax平滑化 QK = torch.softmax(QK, dim = 3) QK = self.att_dropout(QK) print(QK.shape) print(V_state.shape) output = QK @ V_state print(output.shape) output = output.reshape(batch_size,seq_len, -1) return self.proj(output) #在 PyTorch 的 MultiHeadAttention（或自己实现的）里，attention_mask 的形状通常是：(batch_size, num_heads, query_len, key_len) # 所以我们要把这个 (batch_size, seq_len) 的 mask 扩展成 (batch_size, num_heads, seq_len, seq_len)，并且要广播到每个 head 上#最终得到的 attention_mask[batch, head, i, j] 表示：#第 batch 个样本的第 head 个头中，第 i 个 query 能不能看到第 j 个 keyattention_mask = ( torch.tensor( [ [0, 1], [0, 0], [1, 0], ] )# shape: (3, 2) 原始mask，0=可见，1=不可见（或反过来，取决于实现） .unsqueeze(1)# → (3, 1, 2) .unsqueeze(2)# → (3, 1, 1, 2) .expand(3, 8, 2, 2)# → (3, 8, 2, 2))x = torch.rand(3, 2, 128)# 八头注意力net = MultiHeadAttention(128, 8)net(x, attention_mask) 这里再次解释一下，为什么现在现在的代码实现都是 q k v 的投影矩阵都是分开写的，这是因为现在的模型很大，本身可能会做 张量并行，流水线并行等方式，所以分开写问题也不大（分开写很清晰），可能是加速效果并不明显。","link":"/2025/12/10/Attention/"},{"title":"小球带动大球-LoRA","text":"大语言模型竞争激烈，众多的基座模型在评分榜上你追我赶，只为实现人工智能的梦想–通用AGI。但是在实际业务场景中，我们并不需要一个全职全能的语言模型。作为客服机器人，不需要对政治历史和军事或者科技有什么了解，只要搞清楚商品就好了；作为问诊机器人，不需要对艺术史或者音乐有什么了解，只要搞的清楚基础病即可。大语言模型在这种场景显得牛刀杀鸡了。 这时候，微调技术应运而生。","link":"/2025/12/10/LoRA/"},{"title":"TransformerDecoder","text":"今年是2025年，我相信现在读研究生的你，在组会上，一定会遇到某位师兄或师姐说，上周读了《Transformer is All You Need》论文，为大家作汇报。这是就会展示出这张经典的图片并说，大语言模型由Encoder和Decoder组成，巴拉巴拉，然后你的导师似懂非懂点了点头，而这个时间点就是你丧失热情，开始玩手机的开始。 面试过程中让写 transformers Decoder 一定要沟通清楚是写一个 CausalLM decoder 还是原版的，原版的比较复杂，一般也不会让写。这里的 Decoder 一般指的是 CausalLM，具体变化是少了 encoder 部分的输入，所以也就没有了 encoder and decoder cross attention。 因为重点希望写 CausalLM，所以没有 Cross attention 和 也省略了 token embedding 这一步。","link":"/2025/12/11/TransformerDecoder/"},{"title":"我是谁","text":"很久以前看B站《我的音乐你听吗?》，算是这么些年罕见的高质量原创音乐舞台，也记住了很多优秀作品。印象最深的是《我是谁》这首歌。那时还是无忧无虑的大二时光，没有意识到无论是自己的人生还是国家都在一个关键点。但我还是如歌词里唱着一样，被张佩服用石头砸了一下头，完事醒了之后，忘了我是谁。 时代变化如此之快，身处其中的时候总是感觉不到，等停下来才后知后觉，怎么没买英伟达的股票，还有黄金。数据的爆炸已经让人脑无法荷载，这个时代的数据已经成为了“房间里的大象”，愈来愈烈的舆论争吵与人群对立不过是反反复复在说明一件事：所有人都是盲人摸象，只能看到自己眼中的世界，然后通过“触摸大象”来加深这种刻板。 每个人都应该被宣判死刑，看似还活着的人不过是数据意义上的行尸走肉。","link":"/2025/12/06/%E6%88%91%E6%98%AF%E8%B0%81/"},{"title":"一些有意思的算法题","text":"不想刷LeetCode啊！！！ 言归正传，写代码还是很有意思的事情。我大学读的是数学专业，一开始只觉得写代码是计算机专业该干的事，后来惊奇地发现，有些算法莫名与那些恐怖的数学知识相关，而这些算法都是为了解决一个具体的有趣问题。这里整理一些LeetCode中有趣的题目，更多还是结合数学的内容。 Q1:爬楼梯这个问题我应该是最早在电影《少年班》里看到的，那个神童摇着龟壳，根据掉落铜钱来回答的场景太神棍，印象很深。 假设你正在爬楼梯。需要 n 阶你才能到达楼顶。每次你可以爬 1 或 2 个台阶。你有多少种不同的方法可以爬到楼顶呢？比如2级台阶有两种方法：1+1、2；3级台阶有三种方法：1+1+1、1+2、2+1； 从递归的角度容易考虑到，记上$n$级台阶的方法数量为$f(n)$，那么前一步一定在$n-1$级台阶或者$n-2$级台阶，有$f(n)=f(n-1)+f(n-2)$等式成立。这就是斐波那契数列的通项$1,2,3,5,8,13,21….$，自然很容易的写出这样的算法。 12345678910def climbStairs(self, n: int) -&gt; int: if n &lt;= 2: return n # Fibonacci-like sequence first, second = 1, 2 for i in range(3, n + 1): first, second = second, first + second return second 学过线性代数的你一定会想到， Q2：只出现一次的数给你一个 非空 整数数组 nums ，除了某个元素只出现一次以外，其余每个元素均出现两次。找出那个只出现了一次的元素。你必须设计并实现线性时间复杂度的算法来解决此问题，且该算法只使用常量额外空间。 这道题难度easy，但是其解法中的位运算方法实在太优雅（依赖于其余数只出现偶数次），让我想起很多年前见到过一个位运算求N皇后问题的牛逼解法。这正好借这个机会展示二进制的魅力。 1234567891011def singleNumber(self, nums: List[int]) -&gt; int: &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; n = len(nums) index = 0 for i in range(n): # 异或有结合律和交换律 index ^= nums[i] return index","link":"/2025/12/11/%E4%B8%80%E4%BA%9B%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E7%AE%97%E6%B3%95%E9%A2%98/"}],"tags":[{"name":"大语言模型","slug":"大语言模型","link":"/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"name":"生活","slug":"生活","link":"/tags/%E7%94%9F%E6%B4%BB/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"}],"categories":[{"name":"LLM学习系列","slug":"LLM学习系列","link":"/categories/LLM%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97/"}],"pages":[]}